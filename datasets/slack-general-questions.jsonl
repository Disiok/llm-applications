{"question": "Hi, I'm trying to verify scenario in which i run some function, call it spanning function, on ray cluster which contains of many remote functions that are time consuming and also run on the cluster. I'd like to return objectRefId of a spanning function from, for instance, web endpoint and than be able to verify completness of the function and its result. I tried implementing this scneario like this\n\n```@app.post(\"/jobs\")\ndef new_job():\n    hello_ref = hello.remote()\n    ref_dump = ray.cloudpickle.dumps(hello_ref)\n    return {\n        \"id\": str(base64.urlsafe_b64encode(ref_dump), 'ascii')\n    }\n\n\n@app.get(\"/jobs/{id}\")\ndef find_job(id):\n    decoded_refdump = base64.urlsafe_b64decode(id)\n    decoded_ref = ray.cloudpickle.loads(decoded_refdump)\n    res = ray.wait([decoded_ref], timeout=1)\n    status = \"done\" if decoded_ref in res[0] else \"waiting\"\n    return {\n        \"status\": status\n    }\n\n@ray.remote(resources={'custom': 1})\ndef hello():\n    import time\n    time.sleep(1)\n    return \"ok\"```\nbut lookup fails on checking with job id (method find_job). Is this a usecase for ray ? If yes how can it be implemented"}
{"question": "I was using ray in my computer with 16 GB memory and received the following error shown below.\n\n&gt; ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node adminThinkCentre is used (15.28 / 15.59 GB). The top 10 memory consumers are:\n&gt; \n&gt; PID        MEM       COMMAND\n&gt; 13331   7.64GiB   ray::train()\n&gt; 13334   7.08GiB   ray::IDLE\n&gt; .....\n&gt; .....\n&gt; .....\n&gt; \n&gt; In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store. You can set the object store size with the\u00a0`object_store_memory`\u00a0parameter when starting Ray.\n&gt; ---\n&gt; --- Tip: Use the\u00a0`ray memory`\u00a0command to list active objects in the cluster.\n&gt; ---\nQuestions:\n1. How do I set the\u00a0`object_store_memory`\u00a0parameter to solve the above issue?\n2. ray::train() and ray::IDLE is taking up most space. By right, each ray::IDLE process should only take about 210MB, but the ray::IDLE took up 7.08GB. Why is this the case and what can I do about it?\n3. Can we make ray to run on machines with limited memory? For example, it should still run on machines with less memory by setting some memory limits for it."}
{"question": "I was trying to join a remote EC2 instance in AWS to my laptop(routing is configured correctly) using `ray start --address=`, but getting this error below in the log. Can\u2019t figured out what it means? I can see the node from dashboard though. I should say that I\u2019m connecting the nodes via VPN.\n```F1027 08:57:32.912818 44926 327486912 <http://service_based_gcs_client.cc:207]|service_based_gcs_client.cc:207]> Couldn't reconnect to GCS server. The last attempted GCS server address was 172.31.11.196:46565\n*** Check failure stack trace: ***\n    @        0x10c5f2512  google::LogMessage::~LogMessage()\n    @        0x10c58fca5  ray::RayLog::~RayLog()\n    @        0x10c28861d  ray::gcs::ServiceBasedGcsClient::ReconnectGcsServer()\n    @        0x10c287cac  ray::gcs::ServiceBasedGcsClient::PeriodicallyCheckGcsServerAddress()\n    @        0x10c287418  ray::gcs::ServiceBasedGcsClient::Connect()\n    @        0x10c095fd4  main\n    @     0x7fff6da16cc9  start\n    @               0x18  (unknown)```"}
{"question": "Hi everyone, I have a question about ray actors. Is there a way to know which node the actor is scheduled on given the actor handle? Thanks!"}
{"question": "<@UVD989S0K> could you point me out where to look for the instructions to compile the ray library? I'm not used to it. thanks for the patience"}
{"question": "We came across a scaling question for applying ray within a multi simulation use case:\nWhat is the preferred way of applying ray core\u00a0\u00a0*actors vs tasks*\u00a0in terms of performance and lower memory footprint?\n\n*Actor use case:*\n\n`@ray.remote`\n`class SingleSimulation(object):`\n    `@ray.method()`\n    `def run_simulation(self):`\n        `for i in range(simulation_iteration):`\n            `do stuff and change state per iteration`\n            `e.g. self.result +=1`\n        `return self.result`\n\n`class MultiSimulation(object):`\n    `def run_multi_simulations(self, number):`\n\t\t`for i in range(number):`\n\t\t\t`simulation = SingleSimulation.remote()`\n\t\t\t`simulation.run.remote()`\n        `... collect all_results`\n\t\t`return all_results`\n\n*Task use case:*\n\n`class SingleSimulation(object):`\n    `@ray.method()`\n    `def run_simulation(self):`\n        `for i in range(simulation_iteration):`\n            `...do stuff and change state per iteration`\n            `e.g. self.result +=1`\n        `return self.result`\n\n`class MultiSimulation(object):`\n    `def run_simulation(self, number):`\n\t\t`for i in range(number):`\n\t\t\t`simulation = SingleSimulation()`\n\t\t\t`simulation.run.remote()`\n       `... collect all_results`\n\t\t`return all_results`\n\n\nWhich approach is the more scalable one in terms of a several thousand parallel simulation tasks here?"}
{"question": "hi, if I have two python functions that have conflicting dependencies and need to be in their own virtual environments. How can I deploy a workflow using both these deps in Ray? I.e., does Ray provide isolation for individual functions?"}
{"question": "Is it possible for a node to be associated with a Queue and wait for the arrival of something in the queue while in an idle state?  Also, the system I\u2019ve been working on is a connected graph of nodes as actors where the nodes only \u2018communicate\u2019 with their immediate neighbors.  I use a messaging scheme, where each node builds  messages and sends them to their immediate neighbors by invoking a neighbor actor\u2019s recvMessage() remote function.  Once the receiving actor decodes the message, it starts its calculation process and then sends its own set of messages, ending with a return, which places the node back into an idle state. Is it possible for another neighbor to \u2018preempt\u2019 the first calculation/messaging process by sending a message to the same node?  I would think not, that they would queue and the dispatcher would wait until the first sequence goes idle, but I\u2019m seeing things that make me question that assumption.  It\u2019s kinda related to thread safety without the use of threads, and kinda related to preemption."}
{"question": "Is Fedora 33 supported? I am not able to install using pip install ray. I was also not able to install ray in Anaconda"}
{"question": "Hi folks,\n\nI have a question regarding hyperparameter tuning on ray[tune] via hugging face.\nWhen I try some hyperparameter tuning, the memory footprint blows up. Is there a way for me to limit the # of parallel trials I can run?"}
{"question": "hi folks, I recently upgraded from 0.8.5 to 1.0.0 and I see that one of my tune based training runs has increased in time from about 24 minutes to about 30 minutes. Is this due to an expected change? In our code nothing has changed apart from the needed parameter changes needed for the upgrade (parameter names, default port specifications etc)"}
{"question": "Is it posible to pass multiple parameters into an actorpool ?"}
{"question": "Hi, has anyone faced this error after installing Ray on Python 3.6?\n`ModuleNotFoundError: No module named 'dataclasses'`"}
{"question": "Does anyone have thought of applying *Detached Actors* as an *In-Memory Cache*? Simply ray can be thought of a *High level* *API* for an in memory cache baked up by *Plasma in memory object store* from arrow project. Would be interested in hearing your thoughts on that. Ray cluster probably is easier managed as a self setup hardware underlying a self managed Plasma store. :slightly_smiling_face:\n\n\n```@ray.remote\nclass sharedmemActorCache:\n    def __init__(self):\n        pass\n    \n    def set_data(self):\n        self.data = np.zeros((6000000,10))\n    \n    @ray.method(num_returns=1)\n    def get_data(self):\n        return self.data\n\n### start in memory cache ###\n\nactor = sharedmemActor.options(name=\"sharedmemActor\", lifetime=\"detached\").remote()\n\n## put some example data into it\n\nactor.set_data.remote()\n\n## extract data with fast serialization and deserialization from shared in memory actor cache\n\nactor = ray.get_actor(\"sharedmemActor\")\ndata = ray.get(actor.get_data.remote())```\nDefinetly faster than loading data from disk"}
{"question": "I am trying to use Wasmer Python with Ray (actor framework), here's a minimal example:\n```from wasmer import engine, wat2wasm, Store, Instance, Module\nfrom wasmer_compiler_cranelift import Compiler\nimport ray\nimport wasmer\n\n\n@ray.remote\nclass WasmActor:\n    def __init__(self, wasm_bytes):\n        self.engine = engine.JIT(Compiler)\n        self.store = Store(self.engine)\n        self.module = Module(self.store, wasm_bytes)\n        self.instance = Instance(self.module)\n\n    def execute(self):\n        results = self.instance.exports.sum(1, 2)\n        return results\n\n\nWASM_BYTES = wat2wasm(\"\"\"(module\n  (type $sum_t (func (param i32 i32) (result i32)))\n  (func $sum_f (type $sum_t) (param $x i32) (param $y i32) (result i32)\n      local.get $x\n      local.get $y\n      i32.add)\n  (export \"sum\" (func $sum_f)))\"\"\")\n\n\ndef cli() -&gt; None:\n    ray.init()\n    wasm_actor = WasmActor.remote(WASM_BYTES)\n    results = ray.get(wasm_actor.execute.remote())\n    print(f\"{results}\")\n    ray.shutdown()\n\n\nif __name__ == \"__main__\":\n    cli()```\nHowever I'm running into an exception whose cause I do not grok yet, something to do with pickling the actor state?\n```2020-11-04 17:27:36,840\tINFO services.py:1164 -- View the Ray dashboard at [1m[32m<http://127.0.0.1:8265>[39m[22m\n2020-11-04 17:27:37,877\tWARNING worker.py:1072 -- Failed to unpickle actor class 'WasmActor' for actor ID df5a1a8201000000. Traceback:\nTraceback (most recent call last):\n  File \"/home/krzysztof/Work/wasmer-ray/python/.direnv/python-3.8.5/lib/python3.8/site-packages/ray/function_manager.py\", line 493, in _load_actor_class_from_gcs\n    actor_class = pickle.loads(pickled_class)\nAttributeError: type object 'Instance' has no attribute 'exports'\n\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\n  File \"/home/krzysztof/Work/wasmer-ray/python/src/wasmer-ray/main.py\", line 32, in cli\n    results = ray.get(wasm_actor.execute.remote())\n  File \"/home/krzysztof/Work/wasmer-ray/python/.direnv/python-3.8.5/lib/python3.8/site-packages/ray/worker.py\", line 1428, in get\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(RuntimeError): [36mray::WasmActor.execute()[39m (pid=1487257, ip=192.168.1.57)\n  File \"python/ray/_raylet.pyx\", line 445, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 479, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 483, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\nRuntimeError: The actor with name WasmActor failed to be imported, and so cannot execute this method.\n[2m[36m(pid=1487257)[0m 2020-11-04 17:27:37,873\tERROR function_manager.py:495 -- Failed to load actor class WasmActor.\n[2m[36m(pid=1487257)[0m Traceback (most recent call last):\n[2m[36m(pid=1487257)[0m   File \"/home/krzysztof/Work/wasmer-ray/python/.direnv/python-3.8.5/lib/python3.8/site-packages/ray/function_manager.py\", line 493, in _load_actor_class_from_gcs\n[2m[36m(pid=1487257)[0m     actor_class = pickle.loads(pickled_class)\n[2m[36m(pid=1487257)[0m AttributeError: type object 'Instance' has no attribute 'exports'```\n`wasmer-python` is written in Rust with Python bindings so I wonder if this hits some sort of inherent limitation in Ray actors. Is there a way around this somehow?"}
{"question": "I'm trying to use ray on a cluster with a SLURM scheduler.\nwhen running on the login node (i.e. without SLURM) the RLLIB code scales correctly. Instead when using the scheduler to allocate a certain number of cores/nodes, RAY does not find any resource.\nAny clue? is there a simple test program i can run to see if RAY cluster is correctly initializated?"}
{"question": "Hello, what's the expected release date for the next Ray version (1.1.0)?"}
{"question": "I am running a Django server on an AWS EC2 instance and want to trigger workloads on a Ray cluster that does not include the Django instance. What is the recommended setup to trigger code on a Ray head node from a machine not in the cluster?"}
{"question": "Hi all! In Ray cluster launcher for AWS, is it possible to stop ray from automatically specify `SubnetId` ? I was trying to specify network interfaces for the instance and got this error: `create_instances: Attempt failed with An error occurred (InvalidParameterCombination) when calling the RunInstances operation: Network interfaces and an instance-level subnet ID may not be specified on the same request, retrying.`"}
{"question": "There is intellj IDEA to read scala source code for Spark, what tool do we usually suggest to read Ray's source code ?"}
{"question": "Not sure how familiar people may be with Numba or how common it is to use them both, but I am building a project that utilizes both Numba and Ray and the two separately are great but paired together are even better in terms of performance improvements.\n\nExcept for one area: caching of Numba functions.\n\nYou can cache the compiled-machine code of a numba function so that it doesn't need to be compiled on multiple uses, even if you are working in a totally fresh instance of python interpreter (or thats how I understand it). You can store the cached functions in a custom folder and you can even indicate the \"CPU_NAME\" that the cache is stored on.\n\nWhen used with Ray, however, each new worker still builds each function from scratch at instantiation (or at least at the first execution of the function within the worker), which results in a performance lag early in the worker's process.\n\nEach worker must store a cache of each function for its own local processes however, because subsequent calls to the Numba functions execute much faster.\n\nThis kind of network coms between workers and the main thread is bit beyond my skill set at present. I am wondering ... how to get the workers to use the shared cache?"}
{"question": "Hello, I am wondering if there is any docs about the grpcs setup among various components ?"}
{"question": "is there a way to prioritise tasks scheduled to cluster ?"}
{"question": "Anyway to get `ray up` to interface with kubernetes python client rather than kubectl?"}
{"question": "Hey all! I\u2019m trying to call a pytorch C++ extension in a Ray actor and I\u2019m getting the error below. Has anybody experienced this / does anybody have any insight here?\n\n```Traceback (most recent call last):\n\u00a0File \"train.py\", line 324, in &lt;module&gt;\n\u00a0\u00a0args.use_cluster, args.use_pp, args.diag_lambda)\n\u00a0File \"train.py\", line 263, in main\n\u00a0\u00a0param_server = ParameterServer.remote(model_master, args)\n\u00a0File \"/global/homes/a/alokt/.conda/envs/gnn-ray/lib/python3.5/site-packages/ray/actor.py\", line 379, in remote\n\u00a0\u00a0return self._remote(args=args, kwargs=kwargs)\n\u00a0File \"/global/homes/a/alokt/.conda/envs/gnn-ray/lib/python3.5/site-packages/ray/actor.py\", line 538, in _remote\n\u00a0\u00a0meta.method_meta.methods.keys())\n\u00a0File \"/global/homes/a/alokt/.conda/envs/gnn-ray/lib/python3.5/site-packages/ray/function_manager.py\", line 358, in export_actor_class\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\"class\": pickle.dumps(Class),\n\u00a0File \"/global/homes/a/alokt/.conda/envs/gnn-ray/lib/python3.5/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 70, in dumps\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0cp.dump(obj)\n\u00a0File \"/global/homes/a/alokt/.conda/envs/gnn-ray/lib/python3.5/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 656, in dump\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0return Pickler.dump(self, obj)\nTypeError: can't pickle PyCapsule objects```\nThe extension works when I call it outside of a Ray actor. The minute I call it from any function within a Ray actor I\u2019m using, I get the error above once I call a `.remote()` to instantiate the actor. All the extension does right now is print hello world fwiw."}
{"question": "Hello, trying to understand this:\n```Small tasks: Are your tasks very small? Ray introduces some overhead for each task (the amount of overhead depends on the arguments that are passed in). You will be unlikely to see speedups if your tasks take less than ten milliseconds. For many workloads, you can easily increase the sizes of your tasks by batching them together.```\nWe are testing our data pipeline built on Ray. It works well with large dataset where each actor has a long lifetime. However, when testing small datasets, the speed is extremely slow, and most of the time is spent on actors spinning up and shutting down. The data pipeline with small datasets repeatedly launch and shut down 200 actors every a few seconds as the duration of task is tiny.\n\n\u2022 I am trying to understand what is the overhead  in terms of managing actors ?\n\u2022 Is the overhead different for large task and small task ?\n\u2022 Is there any lock or centralized components affect the scalability when comes to a huge number of small tasks ?\n\u2022 Is it possible the slowdown is caused by Ray launch a bunch of actors but these actors do nothing useful but just spinning ? "}
{"question": "I wanted to pass `logging.WARNING` to `ray.init` as some core worker logs are quite verbose (the log file size gets significantly large). `class RayParams` has `logging_level` parameter but it's actually ignored. How can I control the level of logging?"}
{"question": "another one is for the placement group/scheduling. The log below looks like a bug, there is enough required resource but the task is not getting scheduled. a timing issue?\n```2020-11-13 14:29:49,986 WARNING worker.py:1114 -- The actor or task with ID afa8e3be4eca8f33ffffffff01000000 is pending and cannot currently be scheduled. It requires {CPU_group_320b399d7d9f836afa30f5eecd1d3d7b: 1.000000}, {CPU_group_2_320b399d7d9f836afa30f5eecd1d3d7b: 1.000000} for \nexecution and {CPU_group_320b399d7d9f836afa30f5eecd1d3d7b: 1.000000}, {CPU_group_2_320b399d7d9f836afa30f5eecd1d3d7b: 1.000000} for placement, \nbut this node only has remaining {CPU_group_2_320b399d7d9f836afa30f5eecd1d3d7b: 48.000000}, {CPU_group_320b399d7d9f836afa30f5eecd1d3d7b: 48.000000}, {node:172.31.17.78: 1.000000}, {memory: 45.019531 GiB}, {object_store_memory: 95.898438 GiB}. In total there are 10 pending tasks and 0 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.```\n"}
{"question": "Is a Ray head node able to send jobs to workers on different nodes in SLURM? Or does it depend on the configuration of the cluster? Running trainer.py from <https://docs.ray.io/en/master/cluster/slurm.html> works fine for me, and when I start a server on one node and connect to it from another, the initial printout shows both the local and other node.\n`[{'NodeID': '5f646e8bc59a422f273238b901203ce46cf73b7f', 'Alive': True, 'NodeManagerAddress': '172.17.26.101', 'NodeManagerHostname': '<http://node301.ionic.cs.princeton.edu|node301.ionic.cs.princeton.edu>', 'NodeManagerPort': 57367, 'ObjectManagerPort': 39181, 'ObjectStoreSocketName': '/tmp/ray/session_2020-11-15_20-28-47_175665_118662/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2020-11-15_20-28-47_175665_118662/sockets/raylet', 'MetricsExportPort': 50426, 'alive': True, 'Resources': {'memory': 1781.0, 'CPU': 32.0, 'object_store_memory': 526.0, 'node:172.17.26.101': 1.0}}, {'NodeID': '49199714267bc65ac160453011d576cb60727660', 'Alive': True, 'NodeManagerAddress': '172.17.26.83', 'NodeManagerHostname': '<http://node803.ionic.cs.princeton.edu|node803.ionic.cs.princeton.edu>', 'NodeManagerPort': 59131, 'ObjectManagerPort': 39628, 'ObjectStoreSocketName': '/tmp/ray/session_2020-11-15_20-28-47_175665_118662/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2020-11-15_20-28-47_175665_118662/sockets/raylet', 'MetricsExportPort': 54819, 'alive': True, 'Resources': {'GPU': 1.0, 'object_store_memory': 1016.0, 'CPU': 40.0, 'node:172.17.26.83': 1.0, 'accelerator_type:GTX': 1.0, 'memory': 3246.0}}]`\nHowever, the ip counter consistently shows   Counter({'172.17.26.83': 10}), indicating that it is not sending jobs to the other node. I think that this is related to <https://github.com/ray-project/ray/issues/10986|this issue>, but I couldn't figure out a solution from reading it. From my reading, it looked like SLURM has some difficulties communicating between nodes in different partitions, so maybe it is a problem with SLURM and not Ray."}
{"question": "Hello team, we have ran into a similar issue like this one: <https://github.com/ray-project/ray/issues/8610>.\n```File \"/app/dlav/common/py_image.binary.runfiles/pip_deps__ray_1_0_0/lib/ray/worker.py\", line 751, in init\n2020-11-16T20:43:05Z e     job_config=job_config)\n2020-11-16T20:43:05Z e   File \"/app/dlav/common/py_image.binary.runfiles/pip_deps__ray_1_0_0/lib/ray/worker.py\", line 1118, in connect\n2020-11-16T20:43:05Z e     faulthandler.enable(all_threads=False)\n2020-11-16T20:43:05Z e AttributeError: 'WrappingIO' object has no attribute 'fileno'\n2020-11-16T20:43:05Z e ```\nQuestions:\n\n1. I don't know which dependencies in our code doesn't work with faulthandler, Is there a way to debug ?\n   2.  Wondering if it makes sense to change this line ? <https://github.com/ray-project/ray/blob/master/python/ray/worker.py#L1097>\n`except io.UnsupportedOperation:` -&gt; `except Exception as e:`  maybe then print a warning message."}
{"question": "Any ideas why I am getting these in my logs?\n`[2020-11-16 23:25:08 +0100] [40786] [CRITICAL] WORKER TIMEOUT (pid:40791)`\n`*** Aborted at 1605565508 (unix time) try \"date -d @1605565508\" if you are using GNU date ***`\n`PC: @                0x0 (unknown)`\n`*** SIGABRT (@0x9f52) received by PID 40791 (TID 0x7fd91749f740) from PID 40786; stack trace: ***`\n    `@     0x7fd917080680 (unknown)`\n    `@     0x7fd91707fb3b __libc_recv`\n    `@     0x7fd90f88c1c7 sock_recv_impl`\n    `@     0x7fd90f88ca6a sock_call_ex`\n    `@     0x7fd90f88cb90 sock_recv_into`\n    `@     0x555926b44914 _PyMethodDef_RawFastCallKeywords`\n    `@     0x555926b4b7af _PyMethodDescr_FastCallKeywords`\n    `@     0x555926bb0c7c _PyEval_EvalFrameDefault`\n    `@     0x555926af485b _PyFunction_FastCallDict`\n    `@     0x555926b134d3 _PyObject_Call_Prepend`\n    `@     0x555926af4d2f _PyObject_FastCallDict`\n    `@     0x555926b137ce object_vacall`\n    `@     0x555926b6bddd PyObject_CallMethodObjArgs`\n    `@     0x555926b83fa6 _bufferedreader_raw_read`\n    `@     0x555926b8427b _bufferedreader_fill_buffer`\n    `@     0x555926c03926 _buffered_readline`\n    `@     0x555926c03d1b _io__Buffered_readline`\n    `@     0x555926b44789 _PyMethodDef_RawFastCallKeywords`\n    `@     0x555926b4b7af _PyMethodDescr_FastCallKeywords`\n    `@     0x555926bb0c7c _PyEval_EvalFrameDefault`\n    `@     0x555926b43e7b _PyFunction_FastCallKeywords`\n    `@     0x555926bac740 _PyEval_EvalFrameDefault`\n    `@     0x555926b43e7b _PyFunction_FastCallKeywords`\n    `@     0x555926bac740 _PyEval_EvalFrameDefault`\n    `@     0x555926b43e7b _PyFunction_FastCallKeywords`\n    `@     0x555926bac740 _PyEval_EvalFrameDefault`\n    `@     0x555926af3829 _PyEval_EvalCodeWithName`\n    `@     0x555926b44107 _PyFunction_FastCallKeywords`\n    `@     0x555926bad585 _PyEval_EvalFrameDefault`\n    `@     0x555926af3829 _PyEval_EvalCodeWithName`\n    `@     0x555926b44107 _PyFunction_FastCallKeywords`\n    `@     0x555926bad585 _PyEval_EvalFrameDefault`"}
{"question": "Hi, I'm running `ray start --address=&lt;head_node_address&gt;:6379` on 2 c5.4xlarge instances (16 vCPUs each) to join an existing ray cluster (head node p3.2xlarge with 8vCPUs and 1GPU).\nThe CLI seems to work fine by checking `process.communicate()` . However, a subsequent ray.tune experiment still complains about insufficient resources and indicates the nodes are not present in the cluster:\n```ray.tune.error.TuneError: Insufficient cluster resources to launch trial: trial requested 40 CPUs, 1 GPUs, but the cluster has only 8 CPUs, 1 GPUs, 33.89 GiB heap, 11.67 GiB objects (1.0 node:172.31.23.38, 1.0 accelerator_type:V100). ```\nIs there a recommended way to verify? Unfortunately I cannot use Dashboard due to ssh issue."}
{"question": "Hi all,\nI am trying to use ray as a worker for the web server and use the workers to distribute the load on the server. Point being that I want to make it easier to scale the application and demonstrate ray's capabilities within the company.\nAfter running the server for an hour or so, I get timeout from one of the 2 API endpoints. This causes the server to be unregistered for this particular API, which should not happen in the production environment.\nThus, the question here being, what is the best/suggested way to use ray's tasks to handle load on the server in a time bound manner?"}
{"question": "Hello, I'm new to ray. I'm trying to unpack an .npz file (numpy arrays compressed and stored in binary format) on a cloud notebook with limited RAM and it's causing an out of memory error due to shortage of RAM. Is there any way i can use ray to optimise RAM usage for the file loading?"}
{"question": "```import ray\n\n@ray.remote\nclass Hello:\n    def __init__(self, actor):\n        pass\n\nray.init(include_dashboard=False)\nhello = Hello.remote(actor=10)```\nThis results in error `actor_method_executor() got multiple values for argument 'actor'`. Looks like we can't have a class that takes `actor` as keyword arg? I discovered this due to a reinforcement learning algorithm. `actor` is a common variable name in RL context.\nIf I change the last line to `Hello.remote(10)` without explicit keyword, then it's fine."}
{"question": "I'm getting this error message: `KeyError: 'Error occurs in get_execution_info: job_id ...` . It seems to be thrown by <https://github.com/ray-project/ray/blob/master/python/ray/function_manager.py#L250|this line>. The worker then dies with `SystemExit` (not raised by my code) and no other info. It's very difficult for me to make a minimal reproducible script. Could someone please let me know what's typically cause of this `KeyError` msg?"}
{"question": "Hi, I am new to Ray and I am wondering what's a good approach if I want to run a simulation on one cluster node and do some ML training on another. Is it possible to specify on which device each part shall be run?\nShould I go in this case with Ray cluster and RLlib or is it possible to use RaySGD for such a scenario?"}
{"question": "Hello everyone! Can someone help me with the following question? Seems to be very simple but I can't figure out how to do that.\n<https://github.com/ray-project/ray/discussions/12204|How to find the commit hash to download a wheel from it>?"}
{"question": "Is there any\u00a0Reinforcement Learning paper talked actions has continuous components in [0, 1] and sum to 1. How does the reward or loss look like?  when the actions space is `rllib.util.spaces.Simplex`? eg.  an action"}
{"question": "Hi, can anyone tell me what is `--redis-shard-ports` for ? and how many ports do we need to assign on it?"}
{"question": "Does any one have any familiarity with what a heartbeat is?"}
{"question": "Hi all, I am using FastAPI with Ray. For the processing after receiving the POST requests, I am using `backgroundtasks` which execute the task after the response has been sent. I have it working in my tests. But I am not sure if the `ray tasks` which run in background are async or not. I am using the following way to define the background task.\n`background_tasks.add_task(handle_adel_doc_event.remote, src_lis, adel_type, content, file_name)`\n Looking into the source for tasks in (<https://github.com/encode/starlette/blob/bfa61ad92b63b6929e485f2189fd299f94189b09/starlette/background.py>), I want to know what would be the output of `self.is_async = asyncio.iscoroutinefunction(func)` at line 14 given a `handle_adel_doc_event.remote` call to execute the function on a ray cluster?"}
{"question": "I am currently trying out actors by testing a huggingface model that uses a GPU. I am having an issue while assigning device to the model instantiated inside the init of the actor.\nWhen the self.model is instantiated and the device id is being assigned to the model, torch.nn.modules.module inside convert function is returning an error\n\n```File \"/opt/conda/envs/tsuyaku_env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 652, in convert\nreturn <http://t.to|t.to>(device, dtype if t.is_floating_point() else None, non_blocking)\nRuntimeError: CUDA error: invalid device ordinal```\nAm I doing something wrong?\nRay 1.0.1\nPython 3.7\nnum_gpus= 4\ncode below:\n```from transformers import pipeline\nimport logging\n\n@ray.remote(num_cpus=1, num_gpus=1)\nclass Classify:\n    def __init__(self, id):\n        import torch\n\n        self.id = int(id) # ranges from [0-4)\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" # I tried putting only one ID \n        self.gpu_id = ray.get_gpu_ids()[0]\n        self.log = logging.getLogger(__name__)\n        self.log.warning(\n            f\"GPU IDs: {self.gpu_ids} \"\n        )\n        self.model = pipeline(\"zero-shot-classification\", device=self.id) \n\n    def predict(self, text, topics):\n        import torch\n\n        self.log.warning(\n            f\"Ray: In {self.id}, set gpu: {self.gpu_ids} and now {ray.get_gpu_ids()}\"\n        )\n        self.log.warning(f\"model in: {self.model.device}\")\n        return self.model(text, topics multi_class=True)\n\nif __name__ == \"__main__\":\n    test_sample = [\"test\" + str(i) for i in range(1, 10_000)]\n    models = [Classify.remote(i) for i in range(4)] # 0-3 \n\n    results = []\n    for i in range(4):\n        results.append(models[i].predict.remote(f\"How {i}\", [\"topic1\", \"topic2\"]))\n    new_results = ray.get(results)```"}
{"question": "hmm, strange thing\n\ngiven function\n\n```\n@ray.remote()\ndef test():\n    print(\"starting test\")\n    return 42\n\nray.get(test.remote())```\nI can see 'starting test' outputing two times\n\n```(pid=29841) starting test\n(pid=29841) starting test```\nis this because of method serialization ?"}
{"question": "Question , consider a situtation that I have trained an agent to a certain checkpoint. How would i collect more data without training from this checkpoint\u00a0to save more offline_data ? is there a config\u00a0to disable gradient calculation but still continue data collection\n\n(rl-lib)"}
{"question": "When using ray on a cluster with more than one node, do i have to initialize it in a particular way?"}
{"question": "Hi guys, i know spark, it would be great to have some comparison between spark and ray. e.g. how High Avaibility works in Ray; whether Ray is also SPOF system,... - do you have something like this?"}
{"question": "guys, I found <https://github.com/ray-project/ray/issues/5314> (High Availability feature);  questions:\n1. is *Global Control Store* the only Single Point Of Failure? From <https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview#> it looks like it is the only one\n2. my understanding is: the issue has been closed - it means nobody works on it (it is there about 1 year). HA is not so important feature for developers (anyscale) and won't be available any soon. Correct?"}
{"question": "When launching 2 services using docker-compose. First one being ray head, and they other service trying to connect to ray-head service seems to fail with\n\n```worker_1   |     self.session_name = ray.utils.decode(\nworker_1   |   File \"/opt/conda/envs/worker_env/lib/python3.8/site-packages/ray/utils.py\", line 221, in decode\nworker_1   |     raise ValueError(f\"The argument {byte_str} must be a bytes object.\")\nworker_1   | ValueError: The argument None must be a bytes object.\nworker_1   | worker_worker_1 exited with code 1```\nIf I do not run the command, and exec into the second service and then run ray start --address='ray-head:6379' --redis-password='12345') I can then connect to the ray-head service.\n\nIt seems like the ray-head service cannot launch redis server quick enough.\n\nIs there a workaround, or a fix for this? maybe to ping the ray-head service until it is up?\n\nray version 1.0.1.post1"}
{"question": "Trying to have telegraf ingest the Prometheus metrics. I am getting the error mentioned here. <https://github.com/influxdata/telegraf/issues/8098>\n\nThe issue is for telegraf to update their dependencies. Until then, has anyone had the same problem, and have they found any workarounds to get the prometheus data into telegraf (ultimately going to InfluxDB)?"}
{"question": "Question also posted at <https://discuss.ray.io/t/multiagent-to-batch-learn-and-update-a-single-policy/105>\n\n*RL problem*\nI have a single agent with custom reward function, custom actions, custom environment, custom observation, and uses the generic SAC policy with a neural network. The agent iterates through 1e6 episodes serially to update the SAC policy. This is time consuming.\n\n*Want to parallelize*\nTo speed up learning, we have multiple agents (e.g., say 10 agents) stepping in parallel. At the end of each episode, we batch update the same single SAC policy. The updated SAC policy shall be used by all the agents in the next episode. This will reduce the learning time. Alternatively, any other ray methods to speed up the RL training problem stated above is appreciated.\n\nI am new to reinforcement learning and also new to ray. Could you provide a simple step-by-step complete working ray code example with explanation to achieve the above idea? I have already scoured the entire Ray documentation website, but still am unsure on how to exactly achieve the above. Hence, any help specific to my problem is much appreciated.\n\nThank you."}
{"question": "Any way to add task names to the timeline? For example, I would like to see which tasks here have so much deserialization going on.\nFrom JSON I suspect the answer is no, but maybe anyone has a patch for this?"}
{"question": "How hard would it be to make a ray version of this? Would anyone be interested? I assume it would be a simple fork?"}
{"question": "Not sure how to ask this but, what would be the best way to keep in context a db connection? Is the best practice  `ray.put(connection_string)` and pick it up in every task or an Actor `def\u00a0__init__(self,\u00a0engine): self._db\u00a0=\u00a0models.create_session(engine)`?"}
{"question": "```W1203 17:19:59.959439 17991 <http://reference_count.cc:196]|reference_count.cc:196]> Tried to decrease ref count for nonexistent object ID: 131df227bce8619990bcad3fadbc000800000000```\nBeen seeing a lot of messages like these in the logs. What does this mean? More importantly, is this something major or just normal noise from the Ray cluster?"}
{"question": "Hi How do I connect to a remote cluster locally?"}
{"question": "Hey guys, would appreciate some advice,\n\nWhat would be considered best practice for reporting tune metrics from inside a raysgd torch TrainingOperator class?\nAside from using tune.report where relevant, would it be possible/make sense to have the TrainingOperator sublcass tune.Trainable as an alternative?\nSetup wouldn't change and the step method in the tune.Trainable is essentially equivalent to TrainingOperator.train_batch so this should be possible?"}
{"question": "Have there been discussions or there even an appetite to output the logs in `/tmp/ray/session_latest` into a json format? Mainly that it is easier for services Loggly and <http://Logz.io|Logz.io> to analyze and surface the data."}
{"question": "Hi all, I'm having a simple little question, ray core is completely python right?"}
{"question": "Do anybody remember from which version the GSC errors due to uncorrect resolution of hostname in SLURM manager was solved?\ni'm currently on v1.0.0 and i cannot understood what is the error...\nSince the installation of other versions requires me the HOC assistence, I should point them to the correct version, so as to avoid looping"}
{"question": "Hello :wave: \nI\u2019m very interested in Ray but I would very much like to run my hyper parameter optimization on AWS Lambda \n(Especially now that there are 10GB RAM 6 vCPU Lambas, billed at the millisecond)\n\nI\u2019m willing to put some elbow grease into it but could you give me any hints about how I might go about doing that ?"}
{"question": "<#C01BLVB8RAQ|slurm>\nI try to \"re-open\" this issue <https://github.com/ray-project/ray/issues/11295>\nbasically GCS fails to find valid local host.\nDid it get fixed? in what version?"}
{"question": "Hey Guys! I want to use Ray in C++ and spawn some workers running my cpp code. Is there a tutorial I can look into to get a head-start?"}
{"question": "```\n(pid=72826) E1205 00:51:27.495088 72852 <http://core_worker.cc:379|core_worker.cc:379>] Raylet failed. Shutting down.                                                                                                         \nF1205 00:51:28.751152 45493 <http://direct_task_transport.cc:167|direct_task_transport.cc:167>] IOError: 14: Connection reset by peer                                                                                                   \n*** Check failure stack trace: ***                                                                                                                                                                \n    @     0x7f03ab9b537d  google::LogMessage::Fail()                                                                                                                                              \n    @     0x7f03ab9b67ec  google::LogMessage::SendToLog()                                                                                                                                         \n(pid=72838) E1205 00:51:28.126472 72865 <http://core_worker.cc:379|core_worker.cc:379>] Raylet failed. Shutting down.                                                                                                         \n(pid=72853) E1205 00:51:27.569664 72879 <http://core_worker.cc:379|core_worker.cc:379>] Raylet failed. Shutting down.                                                                                                         \n(pid=72864) F1205 00:51:28.067273 72897 <http://direct_task_transport.cc:167|direct_task_transport.cc:167>] IOError: 14: Socket closed                                                                                                  \n(pid=72864) *** Check failure stack trace: ***                                                                                                                                                    \n(pid=72864)     @     0x7fc457c8037d  google::LogMessage::Fail()                                                                                                                                  \n(pid=72864)     @     0x7fc457c817ec  google::LogMessage::SendToLog()                                                                                                                             \n(pid=72864)     @     0x7fc457c80059  google::LogMessage::Flush()                                                                                                                                 \n(pid=72864)     @     0x7fc457c80271  google::LogMessage::~LogMessage() \n\n```\nMy cluster died after seeing this error. What causes this?"}
{"question": "Hi,\nI have a question regarding resources allocation in ray.\nsay I have a node with 16 cpus, my understanding is that ray will start automatically 16 worker process on this node.\nnow, say I run a 8 tasks in parallel each one with 2 cpus:\n```@ray.remote(num_cpus=2)\ndef f():\n    some_logic..```\nthen 8 workers will be used each one using 2 cpus, right?\n\nis there a mechanism which prevents each worker from using more than 2 cpus? or is this information used only by the schedular to note that 2 cpus are now busy?\nif so, is there a way to prevent a task from using all of the cpus?\n\nIn addition, when using fractional number of cpus, if I don't have autoscaling does it have any benefit?\nThanks!"}
{"question": "Hey all, I'm working on a small environment server that allows me to control windows-only gym environments from a linux system, and I'm interested in rebuilding it with ray. Can anyone tell me about how ray transfers data, specifically what kind of security/encryption I can expect?"}
{"question": "I use remote functions for tasks that I don't need state, and would benefit from more flexible scheduling. It seems that ray frequently starts new processes/workers for new remote function calls. I'm logging to driver and I see new pids all the time. When does ray start a new worker rather than reusing a previous worker? the time gap between two tasks wasn't long like only a few seconds. I'm using the latest wheel. It seems 'intentional' exit.\n```[2020-12-08 02:05:02,209 I 29102 29102] <http://gcs_actor_manager.cc:696|gcs_actor_manager.cc:696>: Worker 8a7701cfbe6328464148ecc2bdaf705a881da7c2 on node 21e823e1d9880e0d92eb859a3ff8e3ba03c8207c intentional exit.```"}
{"question": "Is there any technical documents on how the GC works in Ray? We are seeing memory and cpu utilization steadily increase on our head node as our cluster is up and running. Eventually, both will max out at 100% and then the box will become a zombie doing nothing until we redeploy.\n\nI actually just saw both shoot up to 100% and then I think the GC kicked in and things are back down to normal.\n\nI am also spending my day profiling our personal code to see if there are memory leaks and places to optimize. But knowing more about Ray's GC would help. And if there are nobs to tune on the Ray side, it would be happy to know that is possible. (I am thinking about how one tunes a JVM GC.)"}
{"question": "Hi, folks, I have been trying to run this performance test script \"<https://github.com/ray-project/ray/tree/master/ci/performance_tests>\".  My main goal is to run a local cluster.   Now, I sucessfully made everything build and used setup.py installs new code. but still got error :\n```Traceback (most recent call last):\n  File \"../../ci/performance_tests/test_performance.py\", line 7, in &lt;module&gt;\n    from ray.tests.cluster_utils import Cluster\nModuleNotFoundError: No module named 'ray.tests'```\nWhat did I miss?  Is this performance deprecated? if so, what the alternative we should use?   Thanks a lot!"}
{"question": "Hi there, just want to know if there is any VM's for Ray?"}
{"question": "Hi! I have a question about Actor state. If an actor is restarted (for example, I\u2019m running in Kubernetes and Kubernetes decides to restart the node that the actor is running on), is the state lost, or is it persisted somewhere?"}
{"question": "When we specify `ray.remote(num_cpus=4)`, does Ray do anything special to limit the CPU usage, or is it only a nominal number for resource management?"}
{"question": "I was taking a look at trying out huge_pages support but noticed it was removed in 1.0.  Did it prove to be too much trouble to configure or not worth the benefit?"}
{"question": "Hello, is there a limitation on the number of actors that could have been created on a node ? Looks like in our system, there could only be 80 actors since we have 80 cpu cores on the machine.\n\nI couldn't remember if there is this limitation prior to Ray 1.0, but we could launch more than that prior our upgrading to Ray 1.0."}
{"question": "I remember seeing some discussions of leveraging Ray with Yarn and other components of the Big Data system. There was a message from Ant Financial I believe about adding in the features they had developed around that to Ray but I don't think there was any further developments on that front?"}
{"question": "silly q but anyone know how to set an environment variable in the setup? That will propagate to all worker nodes/"}
{"question": "Hi guys - what is the protocol for distributing several small tasks?"}
{"question": "e.g if I have a function that goes\n```def f(x):\n  time.sleep(0.001)\n  return x```\nand want to run that 1 million times, what is the best way?"}
{"question": "How does one distribute actor methods? Seems like it\u2019s just going on 1 worker"}
{"question": "How does one put severall (e.g 1000) actors on different ec2 isntances?"}
{"question": "Hi folks, I've just encountered a scenario where I have to divide the ray cluster into several sub-clusters, and they have to satisfy the following requirements:\n1. When I start a new cluster using ray start --head, I can specify the number of sub-clusters and the maximum number of nodes each sub-cluster can have;\n2. The head node belongs to all sub-clusters, and when a new node joins the cluster, unless I specify which specific sub-cluster it joins (e.g. div_2), it joins a random sub-cluster (make sure it's not full);\n3. The task submission and resource allocation within each sub-cluster obeys the local-first rule, that is, a node prefers the nodes within the same sub-cluster when it wants to schedule a new task/actor;\n4. I have to be able to decide whether each sub-cluster can communicate with each other, that is, if the resources are full within one sub-cluster, can they use resources from other sub-clusters.\nI feel that this is a hard feature to implement and I'm not sure if these utilities can be realized with the current structure of Ray (without big modification). Currently I only have a naive thought: each node will have a new attribute--which sub-cluster it belongs to, but I find that this will require modifications almost everywhere. Could anyone give me a hint as to where to start and how to achieve this? Thanks!\nP.S. I just realized that there might be another way to do this: I don't need to change anything within a single ray cluster, rather I make it possible for several ray clusters to communicate (share resources and information) with each other. Is this utility currently available? If not, where should I start?"}
{"question": "Hi all! I am trying to use Ray to improve the throughput of a real-time ML inference pipeline (processing an infinite stream of images running on an edge device). Most of the examples I see are geared towards scaling up ML training. Would you all recommend using Ray for such workloads where each node in the pipeline is operating at a low latency (10-20 ms) ? Are there any examples/tutorials handling such use cases?"}
{"question": "Hey everyone!\n\nI work at a company which has built a Django application which makes use of Pandas library to built and analyse data frames.\n\nI have hosted the application on Kubernetes but it is taking a bit of time to handle the computations. On visiting their code, I noticed that they are making use of `multiprocessing.Pool` to spread the load across multiple cores. However, in k8s I don't think this is the right way to go and instead I will like to spread the load to different containers. Can I achieve this via `Ray`? If yes, than how? (Or is there a better way?)"}
{"question": "Hey,\nHope this is the place to ask\n\n<https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/gcp/example-full.yaml>\nI am a bit confused about the difference between the `image` under docker\nand the `sourceImage`  used in `head_node` and `worker_node`\n\nwhat is the difference?"}
{"question": "Does the versions of python have to match for distributed ray? I tried to connect to a head in a cluster and got the following error:\n```RuntimeError: Version mismatch: The cluster was started with:\n    Ray: 1.0.1.post1\n    Python: 3.8.5\nThis process on node 192.168.1.159 was started with:\n    Ray: 1.0.1.post1\n    Python: 3.6.9```"}
{"question": "where is the cluster_config_file?"}
{"question": "Hi guys! Does Ray Support simultaneously running CPP and python workers in tandem? If yes, can someone point to me a documentation for the same?"}
{"question": "Hi there :slightly_smiling_face: I am trying to combine Ray with the `shared_memory_()` feature of Pytorch. However, I am afraid it requires `torch.multiprocessing` instead of Python  `multiprocessing` to work properly. Do you know how I can make Ray use `torch.multiprocessing` ? Thanks!"}
{"question": "Hey there,\nI am currently trying to launch Ray/Rllib with a gpu. Even though I am launching the command snippet below, no gpus are used by ray. The GPUs get detected by tensorflow and GPUtils. The output of tensorflow list_local_devices is appended below. Is there anything I am missing?\n```    ray.init(num_cpus=2, num_gpus=1)\n    ModelCatalog.register_custom_model(\n        \"keras_model\", MyKerasModel)\n    run_policy = \"PPO\"\n    extra_config = {\"learning_starts\": 0}\n\n\n    config = {\n        \"env\": FlatlandEnvironment,\n        \"num_gpus\": 1,\n        \"model\": {\n            \"custom_model\": \"keras_model\",\n        },\n        # \"vf_share_layers\": True,\n        # \"lr\": grid_search([1e-2, 1e-4, 1e-6]),  # try different lrs\n        \"num_workers\": 1,  # parallelism\n        \"framework\": \"tf\",\n        \"horizon\": 500\n    }\n\n    stop = {\n        \"episode_reward_mean\": 100,\n    }\n\n    results = tune.run(\n            run_policy,\n            config=config,\n            stop=stop,\n            checkpoint_at_end = True,\n            checkpoint_freq = 50,\n            resume = True,\n            )```\nNo GPUs used by worker:\n```...\nMemory usage on this node: 3.8/251.5 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 2/2 CPUs, 0/1 GPUs, 0.0/165.67 GiB heap, 0.0/51.71 GiB objects (0/1.0 accelerator_type:GTX)\nResult logdir: /home/sebastian/ray_results/PPO\nNumber of trials: 1 (1 RUNNING)\n...```\ntensorflow list_local_devices output\n```&gt;&gt;&gt; import tensorflow\n&gt;&gt;&gt; from tensorflow.python.client import device_lib\n&gt;&gt;&gt; print(device_lib.list_local_devices())\n2020-12-17 15:25:47.704070: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n2020-12-17 15:25:47.748051: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2000000000 Hz\n2020-12-17 15:25:47.755456: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56236e493c00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-12-17 15:25:47.755495: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2020-12-17 15:25:47.757433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n2020-12-17 15:25:48.055855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \npciBusID: 0000:5e:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\ncoreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n2020-12-17 15:25:48.057079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \npciBusID: 0000:af:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n...\n[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\n...\n, name: \"/device:XLA_CPU:0\"\ndevice_type: \"XLA_CPU\"\nmemory_limit: 17179869184\nlocality {\n}\n...\nphysical_device_desc: \"device: XLA_CPU device\"\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\n...\nphysical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:5e:00.0, compute capability: 6.1\"\n, name: \"/device:GPU:1\"\ndevice_type: \"GPU\"\n...\nphysical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:af:00.0, compute capability: 6.1\"\n, name: \"/device:XLA_GPU:0\"\ndevice_type: \"XLA_GPU\"\nmemory_limit: 17179869184\nlocality {\n}\n...\nphysical_device_desc: \"device: XLA_GPU device\"\n, name: \"/device:XLA_GPU:1\"\ndevice_type: \"XLA_GPU\"\n...\n]```\nI would be really thankful for your help!"}
{"question": "I have a gym.Env that creates a folder for each step that includes files to reproduce and visualize a simulation. What's the best way to log this folder such that it is saved into the local logging directory and will thus be propagated back to the head with relevant meta data? \"render\" does not seem like a good option for multiple files, although I could hack it to deal..."}
{"question": "I was looking for a way to not flood the system with workers when I have a lot of potential job to do.  One idea was I implemented a `count_ready(ids)` that could take a list of ids and return the ready vs waiting ids.  and only submit more jobs when waiting is below a chosen threshold.    Does this sound like a good idea or maybe there are better patterns?"}
{"question": "Hello guys!\nI am trying to install Ray for the first time. Looks like Ray is not supported on mac 11.1. True?\nCan someone point me to the installation requirements?"}
{"question": "What is the effect of setting num_cpus (or num_gpus) to a number less than one?  Is this documented anywhere?  In terms of actors, if I set num_cpus to, say, 0.25, does that mean that I can effectively handle 4 times as many actors on the same machine (assuming that the actors don\u2019t become compute-bound)?  What would be a practical minimum on num-cpus (thinking of light-weight actors, here)?"}
{"question": "Hey m8s,\n\nI have started to use ray, Ive read most of the documentations including the architecture design doc but some things are still are not clear to me,\n\n1. When I spin up a cluster in k8s, I have one head node that is started with `ray start --head ...`  and deployment of worker nodes, In addition I execute a k8s job which runs `ray start ... &amp;&amp; python myapp.py` , myapp has a `ray.init` statement in it which connects to the cluster head node, it works like charm.  In the docs it says that the driver is on the same node as the the head node, am I doing something wrong? should I run the head node with the app?\n2. Our plan is to implement a pipeline which spins up ray cluster in k8s and then uses it using the `Pool()` interface, our k8s is configured to scale up the nodes according to the workload, which means that it takes some time until all worker nodes will be available for the cluster. From what I have seen in the code the number of workers that will available for the pool is determined when the Pool is initialised, does it mean that I will only have the amount of workers as I had when the Pool was initialised? Is the only option for me to make sure the cluster is ready to accept tasks will be to implement some logic around the `ray.cluster_resources` ?\n3. Currently I use the same docker image for all the worker node, an image that contains the code that is used by the pool target function, is it necessary? do I need to have all code dependencies on each worker node? Is there more documentation on how python handle module imports, etc. ? essentially I want to have the fastest \"warmup\" for each worker\nThanks !!! happy holidays"}
{"question": "Is there a way in the Ray Dashboard or via a 3rd party plugin to visually see the Ray task graph and understand the dependencies between them? Something like this graph from the paper..."}
{"question": "Hi All, I'm a newbie in Ray trying to run\u00a0: scipy.spatial.distance.cdist test\n\nimport numpy as np\nimport scipy.spatial.distance\nimport ray\n\n@ray.remote\ndef createArray():\n\u00a0\u00a0np.random.randn(10000,30)\n\n@ray.remote\ndef calcDis(A,B):\n\u00a0\u00a0return scipy.spatial.distance.cdist(A, B)\n\nif __name__ == '__main__':\n\u00a0\u00a0ray.init()\n\u00a0\u00a0A = createArray.remote()\n\u00a0\u00a0B = createArray.remote()\n\n\u00a0\u00a0AB = calcDis.remote(A,B)\n\u00a0\u00a0future = ray.get(AB)\ngetting\u00a0\nValueError: XA must be a 2-dimensional although\u00a0np.random,randn(10000,30) is a two dimension array\u00a0\nAny Ideas?"}
{"question": "If I have a remote function that calls out to other remote functions what is the right number of CPU\u2019s to give the first function? Should it requires sum(dependent functions) + self.\n\n\nIe \n<@UUWBZMU76>.remote(num_cpus=1)\ndef bar(arg):\n    Print(\u201cfoo\u201d + arg)\n    Return \u201cbar\u201d\n\n<@UUWBZMU76>.Remote(num_cpus=x)\ndef foo():\n    bars = ray.get([bar(i) for i in range(100)])\n     Return bars \n\n\nWhat should I set x to\n101? Or 1?"}
{"question": "Hello I'm finding the following issue when updating to the latest master on my local dev environment, I suspect it's related to changes in the dashboard init? Would appreciate some help\n\n```(pid=10336) INFO:     Started server process [10336]\n2020-12-23 10:29:35,359 WARNING worker.py:1030 -- The agent on node DESKTOP-IQPGIRH failed with the following error:\nTraceback (most recent call last):\n  File \"/home/raed/anaconda3/envs/ray/lib/python3.7/site-packages/ray/new_dashboard/agent.py\", line 303, in &lt;module&gt;\n    raylet_name=args.raylet_name)\n  File \"/home/raed/anaconda3/envs/ray/lib/python3.7/site-packages/ray/new_dashboard/agent.py\", line 65, in __init__\n    self.ppid = int(os.environ[\"RAY_RAYLET_PID\"])\n  File \"/home/raed/anaconda3/envs/ray/lib/python3.7/os.py\", line 681, in __getitem__\n    raise KeyError(key) from None\nKeyError: 'RAY_RAYLET_PID'```\n\nYou can run the following script to try and replciate\n```import sys\n\nimport ray\nfrom ray import serve\nimport requests\n\n\nray.init(_memory=1024 * 1024 * 1024, object_store_memory=1024 * 1024 * 1024)\nprint(ray.nodes())\nclient = serve.start()\n\n\ndef echo(flask_request):\n    return \"hello \" + flask_request.args.get(\"name\", \"serve!\")\n\n\nclient.create_backend(\n    \"hello\", echo, \n    ray_actor_options={\"object_store_memory\": 128 * 1024 * 1024}\n)\n# &gt; Traceback A (see below this code block)\n\nclient.create_endpoint(\"hello\", backend=\"hello\", route=\"/hello\")\n\nprint(client.list_backends())\nprint(requests.get(\"<http://127.0.0.1:8000/hello>\").text)```"}
{"question": "any idea why master docs show *args and **kwargs instead of actual parameters for `ray.init`? the docstring isn\u2019t appearing either"}
{"question": "Hello. I'm a software developer in Fairfax, VA, USA. I just started looking at Ray. The first program at <https://ray.io/>, in the tasks tab,  displays this error when I run it:\n```AttributeError: partially initialized module 'ray' has no attribute 'init' (most likely due to a circular import)```\nAny idea how to resolve this?\n\n*RESOLVED - don't use \"ray.py\" as your file name. Sigh.*"}
{"question": "I am trying to run ray on Windows in a conda environment. I am receiving\n`psutil.AccessDenied: psutil.AccessDenied (pid=104)`  at times. I have been successful in running the same statement, but fails other times.\nDo I need to run the code as Administrator?"}
{"question": "Hey,\nIn addition to my previous questions, I am struggling to understand if its possible to share python objects between workers on the same node when using multiprocessing pool, is there a way to keep desarilzed copy of an object so that all workers on the same node could access it?"}
{"question": "In Ray 1.1.0, when I place an object in memory larger than the available free memory (for example: `big_array3 = ray.put(np.zeros(1000 * 1024 * 1024))`), I get an odd system error:\n\n```RaySystemError: System error: No such file or directory\n2020-12-28 21:52:03,711\tWARNING worker.py:1034 -- The node with node id b251ba1a7a1c4ce84d1ce82d883cc5bf4a9ecf38 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a raylet crashes unexpectedly or has lagging heartbeats.```\nWill upgrading to 1.2 w/ object spilling fix this?"}
{"question": "When I run `ray.cluster_resources()` , what's the unit of measurement for the memory and object_store_memory k/v pairs?\n\nThey don't seem to be measured in MB/GB (or match up to what I'm seeing in the Dashboard) and it isn't clarified in the Memory Management section of the Ray docs:\n\n```{'memory': 86.0,\n 'object_store_memory': 29.0,\n 'CPU': 8.0,\n 'node:10.138.0.4': 1.0}```"}
{"question": "Hi everyone! Is there any restrictions on running ray with Python=3.8?"}
{"question": "Hi everyone,\nI used ray to increase the speed of my numpy operations. So in traditional way, I am calling that numpy.solve function in a for loop which does some operations on the matrix. The for loop is taking a lot of time. Now I wanted to remove the for loop and introduce ray for distributed computations and faster results. But it seems that I got more slower results than the normal for loop. Any idea why? I just used simple ray code for implementation."}
{"question": "Hello, I am trying to build behavior similar to a 'named objectref':\n  * each name has some associated data with it\n  * some actors periodically update that data\n  * the data should be retrievable on request given the name in other remote functions\n\nShould I be making a supervisor actor that knows which actor holds the named data to proxy the dataref request for the name? Or is there a simpler way?"}
{"question": "Hi there,\n\nThis is my first message in this community. Happy to have joined it!\n\n*Context:* I am using Pytorch-Lightning + Hydra (for config) and need to launch many experiments in parallel. Following <https://docs.ray.io/en/master/tune/tutorials/tune-pytorch-lightning.html|this tutorial>, I managed to implement a minimal example using Ray tune, however I\u2019d like to leverage Hydra to manage more complex configs.\n\n*Question:* Has anyone in this community used Hydra to launch experiments with Ray as \u201cbackend\u201d? Do you have pointers to resources I can look at to get started?\n\nThanks a lot in advance for your attention. And, Happy new year!!\n\nPietro"}
{"question": "How do I check how many Nvidia GPUs are being used when using RAY + AWS?"}
{"question": "The ray HEAD should always be on the same NODE that the python program, but the workers can be in other nodes, is that correct? Or ray head can also be in another node than the calling python program? thanks!"}
{"question": "I noticed that in the official Ray docs, in the <https://docs.ray.io/en/master/index.html|example code> for creating an actor, the Counter class has \"object\" where the base class name would go for inheritance. Generally classes without inheritance don't have anything in the parentheses. I was wondering if there was a technical reason for this?\n```@ray.remote\nclass Counter(object):\n    def __init__(self):\n        self.n = 0```\n"}
{"question": "I'm using rllib to play a game that I wrote a gym for. After training for a while, I'd like to watch the agent play my game. I've read a long issue that's very close to this ( <https://github.com/ray-project/ray/issues/9123> ), but I've got another snag. The way I observe the game is reading logs. When I load a checkpoint into a trainer, it seems the trainer takes some initiative and starts calling my environment on its own.\n\nCan I turn this off? Is there a better way to do this? I end up with several games in one log, and it's hard to read. I've experimented with calling the policy directly, but it seems the trainer calls the environment anyway. Thanks!"}
{"question": "HI, I am trying to send a long running function inside a flask application to ray cluster to run several of those in parallel. While running in `local_mode` it remote function works but when is set like `local_mode=False` , I start getting RuntimeError about Applicaiton Context, Request Context. Does anyone have experience with such a situation?"}
{"question": "Hi, I am curious about what happens when a GPU task returns a GPU object (e.g. a CuPy array). Does Ray serialize/deserialize this GPU object and put it into the CPU memory (object store)? And which files in the Ray repo should I look into? Thanks!"}
{"question": "Is there a python api that returns the number of the live actors in the cluster  ?"}
{"question": "Happy new year, folks!\n\nQuick question \u2014 is there anyone currently using <https://horovod.readthedocs.io/en/stable/ray_include.html|Horovod on Ray>?\n\nReact :+1: if you are using this.\nReact :no_mouth: if you do not know about this integration!"}
{"question": "Hi, I am a mechanical engineer and using rllib for RL control of rocket engines for my phd research. I am using rllib locally on my windows machine just with ray.init() and go. We further have a workstation with 2 CPUs and on this machine ray is only using 1 CPU and the second CPU is idling. Is there an easy way to use both CPUs?"}
{"question": "Hi there! I'm experimenting around with Ray for some big geodata analysis and was just stumbling over an odd thing. I couldn't find in the Docs a correct way to parallelize methods of a Ray actor. Something like the below:\n```@ray.remote\nclass B:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n    @ray.remote\n    def run_a(self):\n        print(\"Hello\")\n    def run_b(self):\n        print(ray.get([self.run_a.remote(self) for i in range(5)]))```\nSeems a bit weird to me. Especially the last line where I have to pass self as an attribute to `remote`  is there a best practice or a correct way to do this perhaps that I'm not aware of?"}
{"question": "Hello, is there a python API that returns the numbers of drivers connecting the Ray cluster ?"}
{"question": "Hi everyone how do I transfer requirements that i pip install on the master node to worker nodes?"}
{"question": "Hi, i am studying autonomous driving car using FLOW (rllib and sumo bridge programm).\nI have no idea why 'Critic loss' sometimes is peaked during training. But i reckon that others index is stabilized; reward mean, actor loss etc;. I've attached a figure below.\nhow could i eliminate peak of critic loss?\nUsing Algorithm: DDPG\nUsing parameter: { n step: 1, lr : 1e-5, actor lr : 1e-5, critic lr : 1e-5, tau : 0.001, train batch size : 64, buffer size: 3e5, actor hiddens: [64,64], critic hiddens: [64,64], gamma: 0.99, l2_reg :1e-6 }"}
{"question": "Hey, does the vf_share_layers parameter from the PPO Algorithm have any impact when using a custom model?"}
{"question": "Hello,  I am wondering if there is an API to voluntarily disconnect to the Ray instead of waiting until the python process exits ?"}
{"question": "Where could one find the `requirements` files that go into building the ray docker images?"}
{"question": "If I want to protect some objects (via their objectIDs) from being garbage collected what are the steps?"}
{"question": "In `setup_commands`, how do I persist an env variable? `export A=B` doesn\u2019t work"}
{"question": "Has anyone seen the following error when using ray?\n```ValueError: buffer source array is read-only```\nI have seen a few issues like this one: <https://github.com/ray-project/ray/issues/10124> that indicate it might have to do with some cython code and that pandas could be the culprit though I\u2019m not using pandas directly in my project. Does Ray use pandas directly to strore data in it object store?"}
{"question": "<@U017EFEKZRQ> I just wanted to give the new ray client a shot and I was wondering how I would connect to a ray kubernetes cluster and send tasks there? Is that somewhere in your provisional docs PR, if not, would be really cool to know how to do it\u00a0:slightly_smiling_face:\u00a0thanks a lot in advance!"}
{"question": "Sorry for the multiple posts; I keep hitting return instead of shift-return&gt;\nThis question is how to start this with the arguments.  I\u2019m missing the syntax.\nSomething like:\nRay.actor(Node::new).remote(1,2); ???"}
{"question": "Hi there! I'm sure this question has been asked before but I couldn't find any thread discussing this.\nOn kubernetes (k8s), how would I install python packages (pip install) on worker and/or head nodes? Do I need to fork the original dockerfile (`rayproject/ray:1.1.0`) and add my installs there or is there a way through the helm charts or directly through the head-node to install python packages needed for the stand-alone scripts in kubernetes? I feel like this should also be discussed in the docs here:\n<https://docs.ray.io/en/latest/cluster/kubernetes.html>"}
{"question": "I\u2019ve been running my model tuning with `ray.tune` and its been working great but when I try to schedule training using a `ray.remote` decorated function nothing happens. Is there a way to follow a `ray.remote` function similar to how `tune.run` follows the tuning process? Additionally, `tune.run` has a `queue_trials` param so I can wait for my cluster to scale up. Is there any equivalent when calling a `ray.remote` function?"}
{"question": "```Insufficient cluster resources to launch trial: trial requested 3 CPUs, 1 GPUs, but the cluster has only 2 CPUs, 1 GPUs, 7.23 GiB heap, 2.49 GiB objects (1.0 accelerator_type:T4, 1.0 node:172.28.0.2).```\nI am getting this error in Google colab, I tried using num_workers and num_gpu's still I get this error. What possibly could the reason be for this error..anyone?  I am trying to do use Tune with Open AI gym environment."}
{"question": "`2021-01-11 15:21:39,344 WARNING worker.py:1091 -- This worker was asked to execute a function that it does not have registered. You may have to restart Ray.`\n\nWhat does this warning really mean?"}
{"question": "Is there a way to export a policy as a tf2 model? I stumbled upon `export_model`  in `tf_policy` , but it exports the model using a tf1 graph."}
{"question": "```Ray worker pid: 43710\nWARNING: Logging before InitGoogleLogging() is written to STDERR\nF0111 16:43:28.280293 43710 43822 <http://core_worker.cc:165]|core_worker.cc:165]>  Check failed: instance_ The core worker process is not initialized yet or already shutdown.\n*** Check failure stack trace: ***\n    @     0x7fa72398fb5d  google::LogMessage::Fail()\n    @     0x7fa723990fcc  google::LogMessage::SendToLog()\n    @     0x7fa72398f839  google::LogMessage::Flush()\n    @     0x7fa72398fa51  google::LogMessage::~LogMessage()\n    @     0x7fa723942ec9  ray::RayLog::~RayLog()\n    @     0x7fa7235f17c8  ray::CoreWorkerProcess::EnsureInitialized()\n    @     0x7fa7235f8ab2  ray::CoreWorkerProcess::GetCoreWorker()\n    @     0x7fa723565cc5  __pyx_pw_3ray_7_raylet_10CoreWorker_59profile_event()\n    @           0x503a29  (unknown)\n    @           0x56b399  _PyEval_EvalFrameDefault\n    @           0x56955a  _PyEval_EvalCodeWithName\n    @           0x5f7323  _PyFunction_Vectorcall\n    @           0x570286  _PyEval_EvalFrameDefault\n    @           0x5f7146  _PyFunction_Vectorcall\n    @           0x56b399  _PyEval_EvalFrameDefault\n    @           0x5f7146  _PyFunction_Vectorcall\n    @           0x50a24c  (unknown)\n    @           0x5f3d42  PyObject_Call\n    @           0x56ca92  _PyEval_EvalFrameDefault\n    @           0x5f7146  _PyFunction_Vectorcall\n    @           0x56b399  _PyEval_EvalFrameDefault\n    @           0x5f7146  _PyFunction_Vectorcall\n    @           0x56b399  _PyEval_EvalFrameDefault\n    @           0x5f7146  _PyFunction_Vectorcall\n    @           0x50a24c  (unknown)\n    @           0x5f3d42  PyObject_Call\n    @           0x65561c  (unknown)\n    @           0x6750c8  (unknown)\n    @     0x7fa724909609  start_thread\n    @     0x7fa724a45293  clone```\nI started seeing these in my ray application. What is this usually associated with?"}
{"question": "Is there any way to set the minimum worker port number in a ray config file rather than via command line?"}
{"question": "Hello, is there a way to set the CPU affinity of actor process ?"}
{"question": "Hello, I want to use a custom Ray version. So basically i cloned the repo and made some changes to some files. How can i now, build and install Ray using this modified version ?  can anyone help me with this ?"}
{"question": "Hi, how can retrieve redis password?"}
{"question": "What python version(s) does Ray require?"}
{"question": "I'm trying to understand the difference between using actor  options(max_concurrency=N)  i.e., \"threaded actors\" versus creating and starting N async actor instances.\n\nI think the latter means multiple processes which are by definition concurrent (can run simultaneously with a multi-core cpu, and top cmd shows multiple processes named for my actor) and max_concurrency=N only provides extra threads to a single actor process which is unlikely to help concurrency at all because of GIL (top cmd shows only one process named for my actor, despite N=2).\n\nIf that is correct, then is max_concurrency=N is only useful for coroutines?  It provides multiple execution stacks as a convenient way to think about the computation but no actual concurrent execution."}
{"question": "Hi, I'm trying to apply Epsilon Greedy among exploration type on DDPG algorithm.\nBut i got a runtime error. How to solve it?\n```2021-01-12 12:21:29,140\tERROR trial_runner.py:523 -- Trial DDPG_WaveAttenuationPOEnv-v0_2dd21_00000: Error processing event.\nTraceback (most recent call last):\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 471, in _process_trial\n    result = self.trial_executor.fetch_result(trial)\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 430, in fetch_result\n    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 1538, in get\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(RuntimeError): ray::DDPG.train() (pid=21972, ip=220.149.82.56)\n  File \"python/ray/_raylet.pyx\", line 479, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 522, in train\n    raise e\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 508, in train\n    result = Trainable.train(self)\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/tune/trainable.py\", line 332, in train\n    result = self.step()\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 110, in step\n    res = next(self.train_exec_impl)\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 758, in __next__\n    return next(self.built_iterator)\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 785, in apply_foreach\n    for item in it:\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 845, in apply_filter\n    for item in it:\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 845, in apply_filter\n    for item in it:\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 785, in apply_foreach\n    for item in it:\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 845, in apply_filter\n    for item in it:\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 1078, in build_union\n    item = next(it)\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 758, in __next__\n    return next(self.built_iterator)\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 785, in apply_foreach\n    for item in it:\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 785, in apply_foreach\n    for item in it:\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 785, in apply_foreach\n    for item in it:\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 793, in apply_foreach\n    result = fn(item)\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/execution/train_ops.py\", line 69, in __call__\n    info = self.workers.local_worker().learn_on_batch(batch)\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 752, in learn_on_batch\n    info_out[pid] = policy.learn_on_batch(batch)\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/policy/torch_policy.py\", line 328, in learn_on_batch\n    self._loss(self, self.model, self.dist_class, train_batch))\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/agents/ddpg/ddpg_torch_policy.py\", line 89, in ddpg_actor_critic_loss\n    q_t = model.get_q_values(model_out_t, train_batch[SampleBatch.ACTIONS])\n  File \"/home/bmil/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/agents/ddpg/ddpg_torch_model.py\", line 147, in get_q_values\n    return self.q_model(<http://torch.cat|torch.cat>([model_out, actions], -1))\nRuntimeError: Expected object of scalar type Float but got scalar type Long for sequence element 1 in sequence argument at position #1 'tensors'\n2021-01-12 12:21:29,143\tINFO trial_runner.py:638 -- Trial DDPG_WaveAttenuationPOEnv-v0_2dd21_00000: Attempting to restore trial state from last checkpoint.```"}
{"question": "How can I install ray on AWS ARM instance(c6g instance)?"}
{"question": "Can you guys please help me how to use ray library for multiprocessing in kubernetes containers?"}
{"question": "Hi there! I'm trying to figure out how to best stream a big chunk of data from a ray worker node to a ray head node. As I would like to stream the return of a ray remote actor (on a k8s worker node) directly to a file on the receiving end, without cluttering up the RAM of the receiver, I kind of need a streaming facility. I thought of having a method of the remote actor be a generator type and yield the data chunk-wise, allowing to store the blobs in a file as they come in, using always the same amount of RAM. (<@U019GS3J4CA> do you maybe have somewhere to point me to?).\n\nI also looked into Ray streaming but it seems to be a bit off from what I'm trying to achieve, or at least there's no clear example of data-related streaming from a worker to the source. I also looked into the MapReduce example but it doesn't seem that it's really discussing this specific use-case either."}
{"question": "Hi Guys, we are using ray library in python script running on kuberenets pod. We are not using ray cluster i.e. head &amp; worker. Simply running ray in single pod. We are getting below issue after 3-4 hours of execution.\n```2021-01-13 06:46:44,151 ERROR worker.py:981 -- print_logs: Connection closed by server.\n2021-01-13 06:46:44,152 ERROR import_thread.py:89 -- ImportThread: Connection closed by server.\n2021-01-13 06:46:44,162 ERROR worker.py:1074 -- listen_error_messages_raylet: Connection closed by server.\nKilled```\nWe are using ray.init(num_cpus=psutil.cpu_count(logical=True), _memory=4096 * 1024 * 1024, logging_level=\"debug\")\nCan someone please help me?"}
{"question": "Hi, seeing this error:\n```\n(pid=243785, ip=10.40.11.12)   File \"python/ray/_raylet.pyx\", line 444, in ray._raylet.execute_task\n(pid=243785, ip=10.40.11.12)   File \"/venv/lib/python3.8/site-packages/ray/memory_monitor.py\", line 138, in raise_if_low_memory\n(pid=243785, ip=10.40.11.12)     raise RayOutOfMemoryError(\n(pid=243785, ip=10.40.11.12) ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node ray-worker-59464c78c9-sp6fj is used (28.82 / 30.0 GB). The top 10 memory consumers are:\n(pid=243785, ip=10.40.11.12) \n(pid=243785, ip=10.40.11.12) PID        MEM     COMMAND\n(pid=243785, ip=10.40.11.12) 36008      0.7GiB  ray::IDLE\n(pid=243785, ip=10.40.11.12) 64637      0.68GiB ray::IDLE\n(pid=243785, ip=10.40.11.12) 24156      0.67GiB ray::IDLE\n(pid=243785, ip=10.40.11.12) 64641      0.64GiB ray::IDLE\n(pid=243785, ip=10.40.11.12) 35795      0.64GiB ray::IDLE\n(pid=243785, ip=10.40.11.12) 24864      0.64GiB ray::IDLE\n(pid=243785, ip=10.40.11.12) 64640      0.64GiB ray::IDLE\n(pid=243785, ip=10.40.11.12) 25197      0.61GiB ray::IDLE\n(pid=243785, ip=10.40.11.12) 115607     0.59GiB ray::IDLE\n(pid=243785, ip=10.40.11.12) 65266      0.57GiB ray::IDLE\n(pid=243785, ip=10.40.11.12) \n(pid=243785, ip=10.40.11.12) In addition, up to 0.83 GiB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray.```\na few questions\n\u2022 what are the ray::IDLE processes?  \n\u2022 Are there logs where I can see what might be taking up the memory?  was the worker restarted?\nthanks!"}
{"question": "Is there any documentation on the set of AWS permissions that are needed to be able to use the cluster launch with the AWS provider? The use cases being that our security policies won\u2019t allow users the  `iam:CreateInstanceProfile` permission. So we would need to create the instance profile before hand and put in the cluster launcher yaml file."}
{"question": "Is ray monitor log changed on version 1.1.0? I cannot see pretty log."}
{"question": "Anyone have a nice optimized base ray image the `raypropject/ray` image seems pretty bloated to me and is some what hard to work with as a base image if you use poetry for managing your dependencies?"}
{"question": "would like to create a Ray cluster with private IPs only on AWS EC2, accessible via VPN - not sure if this is possible?  Didn't see anything in docs"}
{"question": "also, does anyone have tips for Googling material on Ray?  I get a lot of hits on other stuff such as AWS X-Ray"}
{"question": "Has anyone else had this issue? I can\u2019t get the cluster launcher with thee AWS provider to launch the head node. It keep failing on the last `docker exec` in the command runner step:\n```  [5/7] Initalizing command runner\n    Running `command -v docker || echo 'NoExist'`\nShared connection to 34.204.10.37 closed.\n    Running `docker pull rayproject/ray:latest-cpu`\nlatest-cpu: Pulling from rayproject/ray\nDigest: sha256:0e2638722e0d602c0fd41673599917b4adaeb44e23aac708f6829fabf9d984c5\nStatus: Image is up to date for rayproject/ray:latest-cpu\n<http://docker.io/rayproject/ray:latest-cpu|docker.io/rayproject/ray:latest-cpu>\nShared connection to 34.204.10.37 closed.\n    Running `docker inspect -f '{{.State.Running}}' content_recommend || true`\nShared connection to 34.204.10.37 closed.\n    Running `docker inspect -f '{{json .Config.Env}}' rayproject/ray:latest-cpu`\nShared connection to 34.204.10.37 closed.\n    Running `docker info -f '{{.Runtimes}}' `\nShared connection to 34.204.10.37 closed.\n    Running `cat /proc/meminfo || true`\nShared connection to 34.204.10.37 closed.\n    Running `docker run --rm --name content_recommend -d -it  -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 --shm-size='1202281512.96b' --net=host rayproject/ray:latest-cpu bash`\nd3b628b908c0b8650432cca45b11feeb1d98e2181ddea6191fee2bdc03f50dc4\nShared connection to 34.204.10.37 closed.\n    Running `docker exec content_recommend printenv HOME`\nShared connection to 34.204.10.37 closed.\n  New status: update-failed\n  !!!\n  {'message': 'SSH command failed.'}\n  SSH command failed.\n  !!!\n\n  Failed to setup head node. ```"}
{"question": "I used to use for i, x in enumerate(postprocessed_batch[\u2018infos\u2019]): in on_postprocess_trajectory(). Can someone tell me how to do this now in the latest Ray?"}
{"question": "Hello everyone! I\u2019m trying to use ray for the first time, but i met the following problem at the very beginning\u00a0:cry:. What should I do to get ray init successfully? I\u2019m using anaconda\u00a0on MacOS Big Sur.\n```(fllearn) \u279c ~ python3\nPython 3.8.5 (default, Sep  4 2020, 02:22:02)\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import ray\n&gt;&gt;&gt; ray.init()\n[1]    2469 segmentation fault  python3```"}
{"question": "Any idea how to fix this?\n\n```(pid=raylet, ip=&lt;IP-HERE&gt;) *** Aborted at 1610929557 (unix time) try \"date -d @1610929557\" if you are using GNU date ***\n(pid=raylet, ip=&lt;IP-HERE&gt;) PC: @                0x0 (unknown)\n(pid=raylet, ip=&lt;IP-HERE&gt;) *** SIGTERM (@0x229) received by PID 536 (TID 0x7f4f9fdbd7c0) from PID 553; stack trace: ***\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f4f9ffef3c0 (unknown)\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f4f9fea2361 clock_nanosleep\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f4f9fea7eb7 nanosleep\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x5642fdeecef5 ray::gcs::ServiceBasedGcsClient::ReconnectGcsServer()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x5642fdeed1ce ray::gcs::ServiceBasedGcsClient::GcsServiceFailureDetected()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x5642fdeed371 ray::gcs::ServiceBasedGcsClient::PeriodicallyCheckGcsServerAddress()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x5642fdeefaf7 ray::gcs::ServiceBasedGcsClient::Connect()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x5642fddb4fd5 main\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f4f9fde90b3 __libc_start_main\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x5642fddc76b1 (unknown)\n(pid=raylet, ip=&lt;IP-HERE&gt;) *** Aborted at 1610929516 (unix time) try \"date -d @1610929516\" if you are using GNU date ***\n(pid=raylet, ip=&lt;IP-HERE&gt;) PC: @                0x0 (unknown)\n(pid=raylet, ip=&lt;IP-HERE&gt;) *** SIGTERM (@0x207) received by PID 502 (TID 0x7f63b58247c0) from PID 519; stack trace: ***\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f63b5a563c0 (unknown)\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f63b5909361 clock_nanosleep\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f63b590eeb7 nanosleep\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55c7e6823ef5 ray::gcs::ServiceBasedGcsClient::ReconnectGcsServer()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55c7e68241ce ray::gcs::ServiceBasedGcsClient::GcsServiceFailureDetected()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55c7e6824371 ray::gcs::ServiceBasedGcsClient::PeriodicallyCheckGcsServerAddress()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55c7e6826af7 ray::gcs::ServiceBasedGcsClient::Connect()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55c7e66ebfd5 main\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f63b58500b3 __libc_start_main\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55c7e66fe6b1 (unknown)\n(pid=raylet, ip=&lt;IP-HERE&gt;) *** Aborted at 1610929476 (unix time) try \"date -d @1610929476\" if you are using GNU date ***\n(pid=raylet, ip=&lt;IP-HERE&gt;) PC: @                0x0 (unknown)\n(pid=raylet, ip=&lt;IP-HERE&gt;) *** SIGTERM (@0x1e4) received by PID 467 (TID 0x7f2afccf07c0) from PID 484; stack trace: ***\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f2afcf223c0 (unknown)\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f2afce1743e epoll_wait\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac25cc6b2d boost::asio::detail::epoll_reactor::run()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac25cc6d6e boost::asio::detail::scheduler::do_run_one()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac25cc879b boost::asio::detail::scheduler::run_one()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac25cc9602 boost::asio::io_context::run_one()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac25991af1 AsyncClient::Connect()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac25992533 Ping()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac25774e9a ray::gcs::ServiceBasedGcsClient::ReconnectGcsServer()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac257751ce ray::gcs::ServiceBasedGcsClient::GcsServiceFailureDetected()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac25775371 ray::gcs::ServiceBasedGcsClient::PeriodicallyCheckGcsServerAddress()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac25777af7 ray::gcs::ServiceBasedGcsClient::Connect()\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac2563cfd5 main\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x7f2afcd1c0b3 __libc_start_main\n(pid=raylet, ip=&lt;IP-HERE&gt;)     @     0x55ac2564f6b1 (unknown)```"}
{"question": "Trying my luck in this channel : Hello, is it possible to run ray.tune on a web server (fastapi/uvicorn) ? I get errors when i try since I guess you are not supposed to use ray.tune on top of uvicorn."}
{"question": "Hi! I\u2019m trying to use ray serve for a streaming video processing task that has some heavy CPU preprocessing (decoding and resizing 4K frames), followed by a few light weight CNNs running on GPUs. 2 questions:\n1. What\u2019s the best way to parallelize the preprocessing steps and then pass them for batch inference to the actor based backends? Based on the examples in the docs best way to do that is to have an endpoint that takes batches of requests, farms them out to remotes for preprocessing then passes a batch to the model, but that introduces pipeline stalls that I\u2019d like to avoid. \n2. is there a way to pass CUDA tensors between actors / serve backends without copying them back to CPU and serializing them?  "}
{"question": "Is there any support in ray for sharing big data sources with distributed machines? (So that they can be processed by the distributed nodes?) Or what would be the recommended way."}
{"question": "Hi! I\u2019ve been playing around with a small K8s ray cluster which i\u2019ve configured based on the manifest files in <https://github.com/ray-project/ray/tree/master/doc/kubernetes>. With my experiments i had some bits of code that loaded like 10 actors when ran and this was working fine in ray 1.0.0. Now, i\u2019ve been trying to update this environment to ray 1.1.0 and even 1.0.1 and i\u2019m facing a weird problem. Most of those 10 actors are kept in a pending resources state despite both worker nodes i\u2019m using in my setup have no actors and the actors themselves are not asking for any resources. Is there anything on the latest versions that could have caused this behaviour?"}
{"question": "Hi! I have one question, do you have an access with some internal Python API to the head node table of shared objects (located in GCS, as I understand) ? I mean could I locate the node address (or some id) of the specific object by ObjectRef?"}
{"question": "What is new with the new scheduler?  Is there a design doc or something talking about the differences?  Just curious."}
{"question": "Hey all, I am trying to understand how you implemented the chain replication on top of the redis cluster? Could you please point me the code on the github?"}
{"question": "so for placement groups just trying to get the syntax correct.  If I want a function spread across all my nodes evenly but not needing strictly 1 worker per node running the task I would use the SPREAD strategy but what would be the bundle? CPU= total number cpus per node? and if I want no more than 1 task per node running I would use STRICT SPREAD with bundle CPU = 1 or what?, maybe make use of the node custom resource?  So for an explicit example, if I have 10 nodes in my cluster each with 50 cpus and I have a remote function that is called with ray.remote say 1000 times and I want no more than 15 concurrent workers running the task per node I would use what as a bundle?"}
{"question": "Hello! I have a little question about critic loss and TD error.\nI've trained using TD3 algorithm. I'm satisfied with everything else, but there's only one problem on critic loss(related with TD error).\nIs this result moderate? I reckon that behavior policy is exploring because TD3 algorithm is used to off-policy method, so Q-func and TD error aren't converging.\nwho can teach me?"}
{"question": "Hi all! Is there any chance for the `ray.put` API to become asynchronous? Or can I emulate the async behavior with some internal API (get an ObjectRef first and then start sending object)?  <@UMQSXB6TD> <@UNCRYAV9N>?"}
{"question": "Hey everyone! I\u2019ve setup a ray cluster by following the instructions in the <https://docs.ray.io/en/master/cluster/kubernetes.html|docs>. I was wondering whether there\u2019s a way to connect to this cluster from a jupyter notebook on my local machine? I have tried `kubectl port-forward/ray-head 6379`  and then running `ray.init(address=\"localhost:6379\")` but unfortunately that did not work.\n\nAny help or advice would be greatly appreciated!"}
{"question": "Hi all! We are a team from an AI company and want to optimize the Object Store in Ray for large-scale RL problem (StarCraftII).  Is there any detailed doc of the design of Object Store we could look at? And is there any unit test for Object Store only?"}
{"question": "Hi all! Can Ray save small objects returned from remote call in the in-process object store of the driver process?"}
{"question": "Hi <@U01FDP1KJSH> and <@U01K9UF1A6P>, these are great questions. We're moving questions to <https://discuss.ray.io/> so that more users can find them (e.g. via google search) and participate in the discussion. Could you post your question there? You can sign up with your GitHub or Google account"}
{"question": "Hi all! Anyscale PM here.\n\nDoes anyone use `file mounts` as a part of their development or deployment?\n\nLooking to talk to some developers and learn more about how you're using them, and what could be made better."}
{"question": "Hello everybody, I'm pretty new to ray, just found out what is about and considering to use it where I work for some ML projects built by the data scientists. I've got so a few questions for you\n1. First of all I see that it is possible to create a ray cluster in K8s which, I guess, would span over the nodes already in the k8s cluster so to make use of all the CPUs and GPUs. I guess ray will create a pod or set of pods  in each machine? that said, I'm wondering, as I did not get it, how can I deploy a specific app to the ray cluster? I mean I might have a microservice or a script which needs to run  in the ray cluster, and I would generally deploy on k86 as a service with say n pods. Do we have to  do the same also with ray? The code in the deployed service will call ray.init(address of the cluster master) and the code will run in the ray nodes PLUS the pod with the service in it?\n2. Ray seems built for ML but do you think it would be a good fit also for any other project, even say a basic web API, where I could scale up the requests so that each request would have an actor or a remote function producing the result?\n3. I see there is also a Java and an experimental c++ API. I see example of code where the same script is built in each of these languages, but what if I want one script made with python to call a remote which is only implemented in Java or C++, is that possible? Was browsing the documentation but could not get that\nThanks everybody in advance"}
{"question": "Ah, one more, if I have say 10 projects I would like to use ray with, if I deploy all of them in my cluster, they will all share the same runtime, but would this influence the performance somehow? I mean I'm planning to use ray, if it fits my needs, for quite a lot of stuff"}
{"question": "Hello everyone,\n\nI have a use case to implement a REST API that passes a given query through a directed graph of machine learning models. Some of the models in the graph are parallel nodes, while others depend on the output of the previous node.\n\nI would like to implement this API to work on a single machine(all nodes deployed on the same machine) as well as in a distributed setting.\n\nAre there any abstractions provided by Ray that might potentially fit my use case? :slightly_smiling_face:"}
{"question": "Anyone has experience successfully setting ulimits for open file descriptors for ray start when running on EC2? In my cluster yaml, `head_start_ray_commands` looks like this:\n\n```head_start_ray_commands:\n    - ray stop\n    - ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml```\nBut I'm not sure the 65536 limit is actually honored because the default limit in EC2 instances is 8192, and trying to run `ulimit -n 65536` gives this error: `bash: ulimit: open files: cannot modify limit: Operation not permitted`. To verify that ray workers indeed have a ulimit of 8192 (instead of 65536), I ran this snippet:\n\n```import resource\nimport ray\n\ndef get_limit():\n    return resource.getrlimit(resource.RLIMIT_NOFILE)\n\nray.init(address='auto')\nf = ray.remote(get_limit)\nresult = ray.get(f.remote())\nprint(result)   # Soft, hard limit\n# Result was (8192, 8192) on a r5.2xlarge instance```\nTo increase the ulimit, I tried running `sudo bash -c \"echo $USER hard nofile 65536 &gt;&gt; /etc/security/limits.conf\"` as recommended in the <https://docs.ray.io/en/master/troubleshooting.html|docs>. This increases the limit, but only after a `sudo reboot` of the ec2 instance (log-out log-in does not update the limit). I'm afraid adding the limit update to my setup scripts is not useful since the instance wont be restarted before the head_start_ray_commands are run.\n\nSo, is there a way to reliably set the ulimit on EC2 instances in the autoscaler yaml?"}
{"question": "Hi, is anyone running Ray in docker? I met trouble when accessing the Ray dashboard in the container.\nThe Dockerfile looks like this:\n```FROM python:3.8\nWORKDIR /app\nCOPY . .\nRUN pip install --upgrade pip &amp;&amp; pip install -r requirements.txt # including ray\nENV PYTHONPATH=.\nEXPOSE 8265\nENTRYPOINT [ \"python\", \"-c\", \"import ray; ray.init(); # do some tasks...\" ]```\nRun:\n```docker run -p 8265:8265 --shm-size=1G rayindocker ```\nWhen visiting `<http://localhost:8265>` , get `ERR_EMPTY_RESPONSE` ."}
{"question": "How can I copy the worker logs from the cluster nodes to the head node easily? (from `/tmp/ray/session_latest/logs/`)"}
{"question": "Hey, I am logging custom metrics from my ray tune run to tensorboard by overriding the `on_episode_end` function from `DefaultCallbacks`. For each value I have currently value_mean, value_max and value_min, but I only want to save the value_mean field. Is there a way to configure this?"}
{"question": "Hi guys,\nI have a question on storing objects to plasma. If I have multiple remote calls which return large objects, will these objects be stored to plasma sequently or in parallel?"}
{"question": "Is there a working version of Ray for the Apple M1 either through Rosetta or natively? Has anyone tried?"}
{"question": "There is a network connection between the notebook instance and the cluster node since it gave me password mismatch error when I intentionally provided bad Redis password. Can someone please help me debug this issue?"}
{"question": ":wave: is there anyone using *distributed tracing* in production? we are thinking about adding support for it in Ray and I would love to learn more about your experience and use case! Please DM/comment in thread."}
{"question": "is there a way so specify the maximum amount of memory available to a ray worker, same as you do with the *`--num-cpus`* option?"}
{"question": "Hi <@UMQSXB6TD> I had some quick questions about the difference between running ray through separate processes with remote functions and running it through the multiprocessing pool function. Why does multiprocessing pool not use remote functions? Also, I was wondering why multiprocessing pool only allows me to run as many processes as the number of total cpus across allocated cluster nodes while I can spin up hundreds of processes in a for loop with calls to a remote function?"}
{"question": "Is it possible to configure custom logging in order to push ray logs via a handler and not store them inside the temp_dir ?"}
{"question": "Anyone have insight into choosing the size of you worker pods when using the k8 provider? I want each of my hp tuning runs to have access to 4 cpu to help parallelize the model fitting. Additionally, I\u2019m running c5.2xlarge (8 cpus) instances in my k8s cluster so I can essentially fit two models per instance. Is it better or preferred to have less larger worker pod or have a pod per model fitting? For example, if I want to run a max of 20 samples is it better to have 10 worker pods (one for each instances) or 20 worker pods (one for each tuning trial), or some other configuration that I haven\u2019t mentioned?"}
{"question": "Is there a ray-ml:tag-gpu for which the base is a cuda-devel rather than runtime? I'd prefer an image with nvcc such that I can compile projects."}
{"question": "possibly silly question: why does ray use `absl::MutexLock` over `std::scoped_lock`?"}
{"question": "I\u2019m trying to post on discuss, but it says \u201caccount temporarily on hold\u201d. Could a mod help me?"}
{"question": "Is it possible to share the ray cluster to many users? After reading the Ray design paper, it seems the security boundary is at the EC2 machine. How could I share the same Ray cluster with many users? One solution I could think of is to launch the worker in a container and share the raylet with different users. I haven't had experience to operate such cluster but by reading the design paper, the cluster's utilization would be low if the app couldn't fully utilize the machine. (link to discuss forum: <https://discuss.ray.io/t/share-the-ray-cluster/755>)"}
{"question": "How would one serve a reinforcement learning model?"}
{"question": "In ray.remote(num_cpus=X), is this an upperbound on the number of cpus this function can access?\n\n`ray.remote(num_cpus=X)`\n`def f(i):`\n    `return i`\n\nso if I set X = 10 and run the following:\n\n`results_refs = [for ray.rremote(i) for i in range(100)]` \n`results = ray.get(result_refs)` \n\nray should technically not use more than 10 cpus at a time for this task. Am I correct?"}
{"question": "Hi Guys!\n\nI m new to ray community.\nI was wondering if someone could share example code for ray tune using tensorflow 1.15?\nI am facing issues to get it running."}
{"question": "Hi All, just got started off ray less than an hour ago. I am first going through the crash course.\n\nI am currently in \"01-Ray-Tasks.ipynb\"\nI cloned the repo and following the youtube tutorial. The notebook returns this, what noob mistake am I making?\n\n```AttributeError: module 'ray' has no attribute 'get_webui_url'```"}
{"question": "Hello guys, I just encountered this error... I am wondering if there is a way to check which object cause the issue. Any idea?"}
{"question": "Are these tutorials complimentary or anyscale tutorial superseeds the other?\n\n1- <https://github.com/ray-project/tutorial>\n2- <https://github.com/anyscale/academy>"}
{"question": "When using nested remote functions (calling a remote function from a remote function), I am encountering `This worker was asked to execute a function that it does not have registered. You may have to restart Ray.` constantly when my number of workers increases. I saw an issue referencing this, but there was no resolution. Do you have any advice for me?"}
{"question": "Hello, I received this error when I tried to parallelize few functions. Can anyone help? (pid=14760) Windows fatal exception: access violation\n(pid=14760)\n(pid=14760) Stack (most recent call first):\n(pid=14760)   File \"C:\\Users\\avg\\anaconda3\\lib\\site-packages\\scipy\\signal\\signaltools.py\", line 2874 in resample\n(pid=14760)   File \"ray_test.py\", line 84 in downsample\n(pid=14760)   File \"C:\\Users\\avg\\anaconda3\\lib\\site-packages\\ray\\worker.py\", line 390 in main_loop\n(pid=14760)   File \"C:\\Users\\avg\\anaconda3\\lib\\site-packages\\ray\\workers/default_worker.py\", line 183 in &lt;module&gt;"}
{"question": "A question regarding using tensorflow with ray. If I trained my model using the usual way in tensorflow with `model.train`. Can I use the saved model with ray directly? Or must I re-train the model with ray first so I can use it with ray later for predicting?"}
{"question": "Hi everyone!, I'm trying to use an AWS EC2 remote cluster (with a single instance) and I'm facing troubles to get the connection work.  Using `ray start --address='54.208.87.127:6379' --redis-password='5241590000000000'` works fine, even though when I try to use `ray.init(address='54.208.87.127:6379')` the terminal throws me the following error: `12:36:29.564093 11737 408686080 <http://redis_context.cc:298]|redis_context.cc:298]> Could not establish connection to redis 172.31.52.86:31876 (context.err = 1)`. I was reading that maybe it could be related to firewall problems, but I have opened the ports 6379 and 8000 on AWS. Could you help me with and insight or comment to solve this please?, I'm using macOs Big Sur, python 3.8.0 and ray{serve]==1.0.1"}
{"question": "Hello,\nI'm trying to use `ray.get(classifier.remote(i, x,)`  where `i`  and `x`  are file paths (string variables). For some reason, the `Classifier` function isn't picking up the string variables to open the files. Is it normal?\nPlease help!"}
{"question": "Hi, I'm having issues resuming trials after they have been interrupted. I'm receiving `KeyError: '517bae37'` when using non default SearchAlgorithms.\n`\n```Traceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3.2\\envs\\default\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3427, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-2-1219507c04b7&gt;\", line 1, in &lt;module&gt;\n    runfile('.../parameter-search.py', wdir='...')\n  File \"C:\\Program Files\\JetBrains\\PyCharm 2020.3.2B\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\n  File \"C:\\Program Files\\JetBrains\\PyCharm 2020.3.2B\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\n  File \".../parameter-search.py\", line 519, in &lt;module&gt;\n    raise e\n  File \".../parameter-search.py\", line 465, in &lt;module&gt;\n    analysis = tune.run(ImpalaTrainer,\n  File \"C:\\ProgramData\\Anaconda3.2\\envs\\default\\lib\\site-packages\\ray\\tune\\tune.py\", line 520, in run\n    runner.step()\n  File \"C:\\ProgramData\\Anaconda3.2\\envs\\default\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 517, in step\n    self._process_events(timeout=timeout)  # blocking\n  File \"C:\\ProgramData\\Anaconda3.2\\envs\\default\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 670, in _process_events\n    self._process_trial(trial)\n  File \"C:\\ProgramData\\Anaconda3.2\\envs\\default\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 728, in _process_trial\n    self._process_trial_failure(trial, traceback.format_exc())\n  File \"C:\\ProgramData\\Anaconda3.2\\envs\\default\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 938, in _process_trial_failure\n    self._search_alg.on_trial_complete(trial.trial_id, error=True)\n  File \"C:\\ProgramData\\Anaconda3.2\\envs\\default\\lib\\site-packages\\ray\\tune\\suggest\\search_generator.py\", line 129, in on_trial_complete\n    self.searcher.on_trial_complete(\n  File \"C:\\ProgramData\\Anaconda3.2\\envs\\default\\lib\\site-packages\\ray\\tune\\suggest\\hebo.py\", line 266, in on_trial_complete\n    self._live_trial_mapping.pop(trial_id)\nKeyError: '517bae37'```\nShould the resume work for these algorithms (Optuna, Nevergrad, HEBO, etc.)?"}
{"question": "I saw that random points to here for questions.\n\nWith the knowledge that rllib exists and probably has an answer to this question that I haven't fully grasped yet, I have a question about how to be working with Ray + PyTorch (I did see the recent blog post).\n\nIn the case where I am running an evolutionary algorithm (or anything gradient-free), the method of distributing the population evaluation makes sense to me. I can do something like below.\n```@ray.remote\ndef remote_evaluate(game, builder_network_weights, agent_network_weights):\n    env = gym.make(game)\n    builder = NN1(state_space_info).update_weights(builder_network_weights)\n    level = builder.create_level()\n    state = env.reset(level_string=level)\n    agent = NN2(action_space_info, state_space_info)\n    agent.update_weights(network_weights)\n\n    # run rollout and capture info\n\n    # close env\n    return rewards.sum()```\nOn the other hand, with gradient-based methods the way that I could think to do this would be to fully encapsulate the training step (which is one of the requirements that `rllib` asks for if I'm not mistaken). But that still runs into the problem of if I am using PBT in a distributed manner and get back 5 different sets of weights for each network, how are you supposed to combine them all?\n```@ray.remote\ndef remote_train(game, builder_network_weights, agent_network_weights):\n    env = gym.make(game)\n    builder = NN1(state_space_info).update_weights(builder_network_weights)\n    level = builder.create_level()\n    state = env.reset(level_string=level)\n    agent = NN2(action_space_info, state_space_info)\n    agent.update_weights(network_weights)\n\n    # run rollout and capture info\n\n    # calculate loss and do backward passes\n\n    # close env\n    return new_bulider_weights, new_agent_weights```\nIntuition says that I shouldn't place each of these networks into ray-actor classes because then I have giant NNs just hanging out in the shared-memory-store. But if I don't run the train step together with the evaluate, then I cannot use the gradients that got calculated in the distributed step since they'll be local to the remote call's instantiation of the networks.\n\nPart of the draw for me using Ray right now is that it will allow me to easily swap in and out different optimization algorithms into my setup, but the cases for each optimization type just seem very different."}
{"question": "Hey does anyone know of a work around or fix for this in ray tune? <https://github.com/ray-project/ray/issues/14320>\n\nThe memory leak with evaluations that successfully complete means that I can't actually start any meaningful jobs"}
{"question": "Hi all! We are using RaySGD + PyTorch + GCP to train our neural networks faster using more GPUs. Scaling is amazing, we are very satisfied with training speed. But sometimes cluster fails because of unknown reasons, usually it is happening after about 5 hours of training. The most common error: The actor died unexpectedly before finishing this task. Could you help to understand the reason of such an error or explain me how can I debug it?"}
{"question": "Hi Ray users and devs, can someone please help me understand get_actor? Looks like it works fine as long as both the definition ('options' call) and get_actor are within the same process - everything works. But if I run a parallel python interpreter and connect to the same ray instance I am getting \"RuntimeError: Lost reference to actor\". This wouldn't be such a big problem itself,  but I am getting the same issue while creating backends with string imports."}
{"question": "Hello, I am wondering where I can download the prebuilt wheel(linux + python3.6) for ray1.2.0 ?"}
{"question": "Hello, what is the minimum gcc/g++ version required to build Ray ? g++-4.9 gives me some compiling error."}
{"question": "Is there a way to profile the time span from the moment the actor is created at driver to the moment actor starts running (executing the first command of constructor) ?"}
{"question": "Hi, I have a quick question for serialization, why you choose `msgpack` for ray-c++ and `protobuf` for ray-python? why not unify the serialization tools? thanks!"}
{"question": "Is this where I can post my issue on running a cluster with ray?"}
{"question": "Hello, I have a question regarding dependency management: if a user spawns a task on ray remote cluster, how are the binaries required to run the task copied from local laptop to remote ray cluster? is there any documentation from dependency management perspective ?"}
{"question": "Hi guys! I am developing new features for ray but have prob with cython. How do you debug the cython codes in ray? Can I debug cython in vscode?"}
{"question": "Hi everyone, I\u2019m trying to run gym environments in parallel to speed up RL training, is this possible with ray? If so, how can this be achieved?"}
{"question": "For PBT tune result, I don't see 'experiment_tag' in analysis.dataframe(), instead all config values are the same in there, not perturbed. is that a version issue? my version is 1.1.0"}
{"question": "For PBT, is checkpoint required? <https://docs.ray.io/en/master/tune/tutorials/tune-advanced-tutorial.html#function-api-with-population-based-training|Ray doc> says it's needed, but academy <https://github.com/anyscale/academy/blob/9317775c393aff06cff06ae58c88f85ce201940d/ray-tune/solutions/03-Search-Algos-and-Schedulers-Solutions.ipynb|pbt example> given doesn't have it.  <@UMQSXB6TD>"}
{"question": "Hello - Ray on K8s\n\nAfter deploying a cluster on k8s using <https://docs.ray.io/en/master/cluster/kubernetes.html|this tutorial >\n\nI\u2019m trying to test connectivity to this cluster by doing:\n\n```kubectl -n ray port-forward service/ray-cluster-ray-head 10001:10001```\nand on another shell\n```import ray\n\nray.util.connect(\"127.0.0.1:10001\")```\nI am getting an error `ConnectionError: ray client connection timeout`\n\nfrom the client error logs we see:\n\n```ERROR:grpc._server:Exception iterating responses: \nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/grpc/_server.py\", line 453, in _take_response_from_response_iterator\n    return next(response_iterator), True\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/client/server/dataservicer.py\", line 49, in Datapath\n    assert accepted_connection\nAssertionError```\nAny idea?"}
{"question": "Hello, I have a question regarding the memory management in Ray. I keep getting the error `INFO (unknown file):0 -- gc.collect() freed 33 refs in 0.08558361400355352 seconds` after a few minutes during a run. From what I have read online, it seems that the issue is that it runs out of resource memory before the garbage collector is able to release memory. Will setting the `object_store_memory` and `memory` parameters like `ray.init(object_store_memory=X, memory=Y)` help overcome the issue - is the default already setting it to the maximum possible value? Or is there a way to set a threshold memory to activate the garbage collector before this happens? I would be grateful for any guidance/advice. Thanks in advance!"}
{"question": "Need advice for setting up a docker container for Ray and Horovod for NVidia GPUs.  I tried starting with an NVidia NGC container that has a lot of optimizations in it, and uninstall/reinstall Horovod with Ray but I can't get past compilation errors.  Is there any easier way to do this?  Part of the problem is I don't have access to the Dockerfile for the NGC container, so I don't know how to replicate what is in it if I start from scratch..."}
{"question": "Hi, the instructions at <https://docs.ray.io/en/master/installation.html#installing-from-a-specific-commit> show how to install a particular wheel for commit from master. Is there a way to do so for a particular release branch? We are specifically looking to install a wheel for some testing of the not yet released but branched 1.3.0"}
{"question": "Hello, would like to fix alpha zero contribution to take only actions from action mask... this is correctly done in mcts search but in rollout workers it fails.... the cartpol example is to easy to trigger the bug. Also I have a improvement to my reported problem (<https://github.com/ray-project/ray/issues/14912>) to pull request at the same time. My current problem is, I see other implementations like QMIX to apply a action mask with \"-inf\" or FLOAT_MIN... but I strucle with the current abstraction... there is a call from \"rollout workers\" that takes action outside from action mask... I have no clue where to add the fix... should I add a forward function to the alpha zero policy?!"}
{"question": "Hi guys!\n\nI have a question, please.\n\n*A bit of context:* I am working on an ETL like pipeline: I download a bunch of CSV files from a s3 bucket, read them, concat into 2 pandas dataframes - all of that is done with plain multiprocessing for now. For some entries in the first dataframe I want to find the best match from the second one based on certain features - KNN.\n\n*How I use ray*: I have a function (ray task) that runs KNN. This function gets references to both dataframes and some other args such as what columns to use for matching etc, index etc. Using the index provided, the function can _slice_, index the first dataframe to get only a single entry it will be searching a match for. Once the match is found, it will return another dataframe with some information such as distance, match id etc.\n\n*Some pseudo code*:\n```@ray.remote\ndef run_matching(i, df1, df0, features):\n    to_match = df1.loc[i, :]\n    # Run matching against df0 using features provided\n    # Return matching results + some other related stuff\n    return pd.DataFrame()\n\n\ndef main():\n    # Download CSV\n    # Read CSV, concat into 2 dfs\n    df1 = pd.DataFrame()\n    df0 = pd.DataFrame()\n    # Moves DFs to the object store\n    df1_ref = ray.put(df1)\n    df0_ref = ray.put(df0)\n    total_to_match = df1.shape[0]\n    # Launch tasks\n    futures = [\n        run_matching.remote(i, df1_ref, df0_ref, [\"feature1\", \"feature2\"])\n        for i in range(total_to_match)\n    ]\n    output = ray.get(futures)\n    # Postprocess output by concating dfs and uploading results\n\n\nif __name__ == \"__main__\":\n    ray.init()\n    main()\n    ray.shutdown()```\nEverything seems to be working fine, the DFs get successfully moved to the object store but then everything freezes and I get a single message - Killed. In addition, when I check the processes running on the VM after that there are a bunch of ray processes.\n![image](<https://user-images.githubusercontent.com/52342908/113816123-ad770580-97b7-11eb-935e-5daefe2e1bcf.png>)\n\n\nThe total number of tasks I might need to run can be ~5-10k (certain combinations in DF1). I tried to run with just 10-20, the same output.\n\n*Questions*:\n1) Could anyone explain what is happening? Am I doing something wrong?\n2) If I have thousands of tasks and then attempt launching them using list comprehension, does Ray limit the total number of tasks running simultaneously depending on the machine characteristics?\n3) Should I consider actors instead of tasks considering I have several thousands of them to run? Currently we use multiprocessing pool for this task. I decided to rewrite the thing using Ray to use smaller VM RAM-wise (no need to copy DFs into each process) and potentially run it on a cluster. Each matching run doesnt take a lot of time, there is just a lot of them, so probably having N actors (N = to number of cores?) is a better idea to avoid the overhead for creating a new task for each match?\n4) How to give ray unlimited resources - can use all cores and all RAM? Or does it assume it can use everything unless stated otherwise?\n\nThank you in advance!\n\nRegards,\nEugene"}
{"question": "On Ray 1.2.0, Ray is incorrectly reporting lack of resources trying to run the Horovod/Ray Mnist example (tensorflow2_mnist_ray.py) on SLURM.  It's running on a single node with 40 CPU and 3 GPU.  I launch it like this:\n\n```# HEAD\nsrun  --nodes=1  --ntasks=1 -w server1 --cpus-per-task=5 singularity run ~/horovodDocker/native_horray.sif  ray start  --head  --node-ip-address=30.30.30.30  --port=6379  --redis-password=supersecret  --num-cpus 5 --num-gpus 0  --include-dashboard False  --block &amp;\n\n# WORKER\nsrun  --nodes=1  --ntasks=1  --cpus-per-task=5 --gres=gpu:1 -w server1 singularity run ~/horovodDocker/native_horray.sif ray start --address 30.30.30.30:6379  --redis-password=supersecret --num-cpus 5 --num-gpus 0  --block &amp;\n\n# Training script\nsrun --nodes=1 --ntasks=1 singularity run ~/horovodDocker/native_horray.sif python horray_mnist.py --address 30.30.30.30:6379 --redis_password supersecret```\nI stripped down the RayExecutor to use barely any resources:\n```    executor = RayExecutor(settings, num_hosts=1, num_slots=3, use_gpu=False, cpus_per_slot=4)```\nI'm getting this:\n```2021-04-07 14:10:01,526\tWARNING worker.py:1107 -- The actor or task with ID ffffffffffffffffa01c95bc3d119e36e40b31b204000000 cannot be scheduled right now. It requires {CPU: 12.000000} for placement, however the cluster currently cannot provide the requested resources. The required resources may be added as autoscaling takes place or placement groups are scheduled. Otherwise, consider reducing the resource requirements of the task.```\nEven though there are plenty of resources available:\n```scontrol show node\nNodeName=************* Arch=x86_64 CoresPerSocket=12 \n   CPUAlloc=12 CPUTot=48 CPULoad=0.16\n   AvailableFeatures=(null)\n   ActiveFeatures=(null)\n   Gres=gpu:3(S:0-1)\n   RealMemory=90185 AllocMem=0 FreeMem=6034 Sockets=2 Boards=1\n   State=MIXED ThreadsPerCore=2 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A\n   Partitions=batch \n   BootTime=2020-12-07T20:50:20 SlurmdStartTime=2021-01-15T14:47:14\n   CfgTRES=cpu=48,mem=90185M,billing=48,gres/gpu=3\n   AllocTRES=cpu=12,gres/gpu=1\n   CapWatts=n/a\n   CurrentWatts=0 AveWatts=0\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s```\nAny ideas what is causing this?"}
{"question": "Can someone tell me how to pass the redis password in when trying to a simple CLI command like `ray status` ?\nEvery variant I can think of for `ray status --redis_password=\"blah\"` gets `Error: no such option`"}
{"question": "Seems Ray community has individual branch for each tag. If I like to submit bug fixes for 1.2.0. What would be the target branch?  releases/1.2.0?"}
{"question": "What\u2019s the recommend way to contribute some large changes to Ray? Does the community follow RFC process or there\u2019s some improvement proposal like Spark SPIP?"}
{"question": "We\u2019re getting the following error every time we start ray (by calling `ray.tune()` in Python):\n`You are using the 'pickle5' module, but the exact version is unknown (possibly carried as an internal component by another module). Please make sure you are using pickle5 &gt;= 0.0.10 because previous versions may leak memory.`\nWe do not have pickle5 installed (at least as far as I can see in `pip freeze`).  I breakpointed right where that error message is generated, and verified that the pickle5 in `sys.modules` is the one inside the ray library. Considering this, is it safe to assume that we should not expect to see a memory leak from this? We\u2019re on ray 1.2.0. We are currently seeing a memory leak from somewhere, and just want to rule out a simple issue elsewhere before we dig into our own code. Thanks!\nPossibly related to this: <https://github.com/ray-project/ray/issues/10637>"}
{"question": "In 1.2.0. Seems user has to explicitly enable client server to use `ray.util.connect` ? How to start`python -m ray.util.client.server`   in <https://github.com/ray-project/ray/blob/ray-1.2.0/python/ray/autoscaler/kubernetes/operator_configs/example_cluster.yaml>?"}
{"question": "*TL;DR:* I\u2019m attempting to install Ray clusters into K8s clusters that are heavily locked-down using `NetworkPolicy` resources and Istio\u2019s `STRICT` mTLS authentication policy.\n\nIn order to create functional clusters, I need to open up the proper network holes for cluster inter-communication and I\u2019m trying to figure out which components listen on which ports.\n\n\u2022 `node-manager-port` \u2013 workers listen for gRPC traffic here, but the does head node also need this port exposed?\n\u2022 `object-manager-port` \u2013 head, workers, both?\n\u2022 `port`, `redis-shard-ports`, `gcs-server-port`, `dashboard-port`, and `ray-client-server-port` \u2013 tmk, these are only exposed by the head node. are they all TCP based except for the dashboard (HTTP)?\n\u2022 `min-worker-port` , `max-worker-port`, `worker-port-list` \u2013 how does these work, how are they used, what is the protocol used?\n*Please let me know if I should post these questions elsewhere.*"}
{"question": "Hi guys,\n\nwhen executing `ray submit`  is it possible to override the default synced location?"}
{"question": "I'm working with the Ray client and I've noticed that if I call a function to be executed in ray, that function will fail if all packages &amp; dependencies are not on the Ray server. Is it possible that I'm doing something wrong or is this the expected behavior?"}
{"question": "Hello, any timeline for Ray 2.0 ?"}
{"question": "Hi I am using rayTune API for hyperparameter tuning on a BERT based model. My code works perfectly fine in my local machine. However, when I deploy it in a cluster computer(my professor provided) it does not work. I understand the cluster architecture is not working properly for rayTune. Should I use rayTune cluster mode for this?\n\nThis is the error message :\n*RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method*"}
{"question": "Hi team, I\u2019m working to build ray cluster with Kubernetes operator.\nI set min_workers=1, max_workers=10. How can I testing the auto scaling works?"}
{"question": "Hi Team, I am working on generative adversarial imitation learning with Rllib. I would like to understand whether it is possible to use rollout_fragment_length, horizon and soft_horizon parameters to do the following experiment. Imagine that I use PPO and that I want to build a training batch composed  of peaces of  episod trajectory where each peace has h steps but  episod trajectories could be longer : lets say T &gt; k*h . In each peace of trajectory of h steps I want to bootstrap at the last step or put reward equal 0 if done=True.  So I guess that I have to use rollout_fragment_length=h and Horizon= T and soft_horizon=True . So I expect that I don't reset at step h and that I bootstrap properly in each peace of episod trajectory. Is it correct ?"}
{"question": "Hi Team,\nDuring Ray dev setup (for dashboard), what node.js version are we using?"}
{"question": "What is the best place to find the examples of ray+optuna for serving ?"}
{"question": "Hi, I'm having a few issues with getting ray, ray-tune, horovod and TensorFlow to work. I've checked for all dependency version conflicts and the only detail I havent been able to find, is Ray 1.2.0 compatible with Tensorflow 2.4?"}
{"question": "Hi! I am new to Ray. \n\nAny idea when there will be a release for Python 3.9? Thanks! "}
{"question": "Any idea why `torch.load` might get \"stuck\" (hang indefinetly) inside a ray process (both cpu and gpu) ? No idea if the bug is ray related to be honest, but it works fine when launching in other ways, so maybe someone here has a clue..."}
{"question": "Hi, I recently upgraded to Ray 1.3; I got a warning message - FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'` ; for the installation I followed the documentation, i.e. `pip install -U ray` . I am using Python 3.8.5 and Spyder 5.0.  Any idea what could have gone wrong?"}
{"question": "I am working on a demo project with ray serve and trying to write testcases however when i have a testcase with await, i get an error RuntimeWarning:\u00a0Enable\u00a0tracemalloc\u00a0to\u00a0get\u00a0the\u00a0object\u00a0allocation.  has anyone seen this?  do you have examples of good testcases with the await functions?"}
{"question": "Hello, I am testing a huggingface code (<https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb>) that uses Ray for fine tuning, but I getting an error \"cannot pickle thread.RLock object\": /usr/local/lib/python3.8/dist-packages/ray/cloudpickle/cloudpickle_fast.py in dump(self, obj)\n    *578*     def dump(self, obj):\n    *579*         try:\n--&gt; 580             return Pickler.dump(self, obj)\n    *581*         except RuntimeError as e:\n    *582*             if \"recursion\" in e.args[0]:\n\nTypeError: cannot pickle '_thread.RLock' object\n\nI am using ray:20.0.dev0.\n\nAny idea of what the problem can be? And how to solve it?"}
{"question": "Hi all, does anyone know how to turn off the autoscaler messages when running ray? Since tune keeps on switching in and out of actors, these messages accumulate and get annoying quickly."}
{"question": "does anyone know how to speed up serialization with ray.get on a list of strings?"}
{"question": "Hey all, any recent changes that may've killed Serve on Windows? All my tasks are now getting stuck on startup (<https://discuss.ray.io/t/the-actor-or-task-with-id-cannot-be-scheduled-right-now/2107>)"}
{"question": "is anyone experienced with\n```ERROR: Could not find a version that satisfies the requirement ray[rllib]==1.2.0 (from versions: none)\nERROR: No matching distribution found for ray[rllib]==1.2.0```\n?\n\nI'm trying to get up and running on my jetson nano (ubuntu 18.04 ish). this `requirements.txt` file has worked consistently in 18.04 environments in github workflows. why is pip acting strange?"}
{"question": "A general question: how ray serialize pytorch tensor (cpu and cuda) in v1.0? Thanks!"}
{"question": "is there a way todo a `ray start` with the *lru_evict=True* option?"}
{"question": "Hi all, is huge page still supported in ray 1.3.0? I didn\u2019t find the huge page flag."}
{"question": "Hello guys. I have a database which is on spark and every entry is a seperate dataset to train on. Now my question is how can i make this spark dataframe an iterable so i can then use it with my model.\nI have seen the implementations with the PyTorch and TF APIs which deal with a lot of code but in my case i want to use a custom estimator which will just take as an input X_train, y_train etc.. Any ideas? :slightly_smiling_face:"}
{"question": "Hi! I am a PhD student, new to Ray.\nI was wondering if Ray supports AMD gpus (ROCm stack specifically)?"}
{"question": "Is there a way via the ray start CLI command to set the redis password? I see it's possible in the python API but I need to do this from the command line"}
{"question": "Is there a way to set the `MetricsExportPort` for a Ray cluster started via the cluster manager? I don\u2019t see anything in the docs about where to set this in the YAML. I\u2019m trying to collect metrics to Prometheus."}
{"question": "hi there, to config ray serve for a cluster, must I run deploy script within the head node, or I can do it elsewhere?  I have tried to run\n```ray.init(address='ray-head:6379', _redis_password='PASSWD')```\nfrom a same node (here it's a container brought up by the same docker-compose file) in the cluster, but I got an Abort error right after I run this script. I am using 1.2.0"}
{"question": "hi, does anyone have experience working with Multi-Agent RL Policy Implementations in Ray? I am looking for some guidance on moving my existing implementation with PettingZoo and Torch to Ray to speed up testing"}
{"question": "When submitting ray jobs with slurm, the documentation specifies to request multiple nodes and --tasks-per-node=1. Is there a reason why one should not request all cpus on the same node? This obviously limits the number of cores one can request, but comes with the advantage that all cores have access to the same memory, which should improve ray performance. Am I missing something?\nI want to do:\n`#SBATCH --ntasks=1`\n`#SBATCH --cpus-per-task=20`\n\nAnd in the python file I simply specify  `ray.init(num_cpus = 20)` . Is there something wrong with this?"}
{"question": "Is there any chance of adding something ray.list() that returns a list of ClientObjectRef objects to the client API?"}
{"question": "Greetings to everyone!\n\nTrying to set up Ray on private cluster, I stumbled upon a problem with `ray start` not handling ipv6 addresses properly.\nAfter spending some time trying to patch the sources I figured out that it is not so trivial, as the `address.split(':')` is all over the code.\nI am wondering, is it known issue something working on already, or am I the only one, unfortunate enough to have a cluster, which is interconnected with ipv6 interfaces only?"}
{"question": "How does Ray determine when an object should be purged from the backing object store?"}
{"question": "Hey! I\u2019ve been trying to set up an AWS cluster and connect it to an existing EFS volume. Has anyone done this before? I think the issue I\u2019m running into is that I\u2019m using docker and it doesn\u2019t play well with the EFS mount. Here\u2019s my config.yml\n```# An unique identifier for the head node and workers of this cluster.\ncluster_name: virtual_screening\n\ndocker:\n  image: rmeinl/virtual_screening\n  container_name: virtual_screening\n  # If true, pulls latest version of image. Otherwise, `docker run` will only pull the image\n  # if no cached version is present.\n  # pull_before_run: True\n\n# How Ray will authenticate with newly launched nodes.\nauth:\n  ssh_user: ubuntu\n\navailable_node_types:\n  ray.head.default:\n      node_config:\n        InstanceType: r4.large\n        ImageId: ami-090717c950a5c34d3 \n        SubnetId: subnet-49fe0c03\n        SecurityGroupIds:\n          - sg-0f2ec03b5a36cf3f9\n      resources: {\"CPU\": 2}\n      min_workers: 0\n      max_workers: 0\n  ray.worker.default:\n      node_config:\n        InstanceType: r4.large\n        ImageId: ami-090717c950a5c34d3 \n        SubnetId: subnet-49fe0c03\n        SecurityGroupIds:\n          - sg-0f2ec03b5a36cf3f9\n        InstanceMarketOptions:\n            MarketType: spot\n      resources: {\"CPU\": 2}\n      min_workers: 0\n      max_workers: 100\n\nhead_node:\n  InstanceType: r4.large\n  ImageId: ami-090717c950a5c34d3 \n  SubnetId: subnet-49fe0c03\n  SecurityGroupIds:\n    - sg-0f2ec03b5a36cf3f9\n\nworker_nodes:\n  InstanceType: r4.large  \n  ImageId: ami-090717c950a5c34d3 \n  SubnetId: subnet-49fe0c03\n  SecurityGroupIds:\n    - sg-0f2ec03b5a36cf3f9\n  # InstanceMarketOptions:\n  #   MarketType: spot\n\n# Cloud-provider specific configuration.\nprovider:\n    type: aws\n    region: us-west-2\n    availability_zone: us-west-2b\n    cache_stopped_nodes: False\n\n# Patterns for files to exclude when running rsync up or rsync down\nrsync_exclude:\n    - \"**/.git\"\n    - \"**/.git/**\"\n\n# Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for\n# in the source directory and recursively through all subdirectories. For example, if .gitignore is provided\n# as a value, the behavior will match git's behavior for finding and using .gitignore files.\nrsync_filter:\n    - \".gitignore\"\n\n# List of commands that will be run before `setup_commands`. If docker is\n# enabled, these commands will run outside the container and before docker\n# is setup.\ninitialization_commands:\n  - sudo apt-get update -y\n  - sudo kill -9 `sudo lsof /var/lib/dpkg/lock-frontend | awk '{print $2}' | tail -n 1`;\n    sudo pkill -9 apt-get;\n    sudo pkill -9 dpkg;\n    sudo dpkg --configure -a;\n    sudo apt-get -y install binutils;\n    cd $HOME;\n    git clone <https://github.com/aws/efs-utils>;\n    cd $HOME/efs-utils;\n    ./build-deb.sh;\n    sudo apt-get -y install ./build/amazon-efs-utils*deb;\n    cd $HOME;\n    mkdir efs;\n    sudo mount -t efs {{FileSystemId}}:/ efs;\n    sudo chmod 777 efs;\n  - curl -fsSL <https://get.docker.com> -o get-docker.sh\n  - sudo sh get-docker.sh\n  - sudo usermod -aG docker $USER\n  - sudo systemctl restart docker -f\n\n# List of shell commands to run to set up nodes.\nsetup_commands: []\n\n# Custom commands that will be run on the head node after common setup.\nhead_setup_commands: []\n\n# Custom commands that will be run on worker nodes after common setup.\nworker_setup_commands: []\n\n# Command to start ray on the head node. You don't need to change this.\nhead_start_ray_commands:\n    - ray stop\n    - ulimit -n 65536;\n        ray start --head --port=6379\n        --object-manager-port=8076\n        --autoscaling-config=~/ray_bootstrap_config.yaml\n\n# Command to start ray on worker nodes. You don't need to change this.\nworker_start_ray_commands:\n    - ray stop\n    - ulimit -n 65536;\n        ray start\n        --address=$RAY_HEAD_IP:6379\n        --object-manager-port=8076```"}
{"question": "Very basic, but I'm using Ray Tune to run an optimisation, and it's spun up a server for the dashboard on port 8265 of the compute nodes, which I'm finding a bit tricky to get access to. I can usually manage port forwarding to my local machine from the login nodes. Has anybody has experience with doing this? Currently the output I'm seeing is not a lot of help."}
{"question": "Hi, Can anyone help me with the issue posted here?\n<https://discuss.ray.io/t/getting-deterministic-policy-after-dqn-training/2237>"}
{"question": "update: if I use following approach, the error is gone\nray.util.connect(\"example-cluster-ray-head:10001\")\npool=Pool(ray_address=\"auto\")\n\nNow the error is\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/multiprocessing/pool.py\",\n line 147, in get\n    raise result.underlying\nModuleNotFoundError: No module named 'easyocr'\nHowever, easyocr is installed in my image (not in ray head or worker) and is under /home/ray/anaconda3/lib/python3.7/site-packages/. Is there any requirement that any custom package needs to be installed on ray worker node?\n\n\nHas anyone encountered this and is there a solution? I deployed Ray cluster on Kubernetes and submit Kubernetes job which uses\nfrom ray.util.multiprocessing import Pool\npool = Pool(ray_address=\"example-cluster-ray-head:10001\")\n\nif I ssh to ray head node, then pool=Pool(ray_address=\"auto\"), there is no error. However, when I ssh to worker node, and use \"pool = Pool(ray_address=\"example-cluster-ray-head:10001\")\", I encounter this error.\n<https://docs.ray.io/en/latest/multiprocessing.html>\n<https://docs.ray.io/en/master/cluster/kubernetes.html>"}
{"question": "Hi, is there an easy way to obtain memory profiles of a Ray task?\nE.g. from something like fil: <https://blog.dask.org/2021/03/11/dask_memory_usage>"}
{"question": "Q: Is there a way to export the model representation of a Ray program that is language agnostic?"}
{"question": "Hi.. Can anyone confirm is LSTM option is now supported in DQN ? This is with reference to the following ticket:\n<https://github.com/ray-project/ray/pull/4316>\ncc: <@UMQSXB6TD>"}
{"question": "I mean, for example a server is running out of ram memory, and this event would bring down the entire cluster. Is it possible to isolate this node, or disconnect it from the cluster?"}
{"question": "Hi : When will Ray 1.4/1.5 be released and what feature list?"}
{"question": "Hi!  Should I  see  GPU and GRAM columns in Machine View tab of Ray Dashboard  by default?  I started  ray from command line or from python  (all on local machine)  but  I never see this columns  in Ray Dashboard on my machine.   I see this columns in docs <https://docs.ray.io/en/master/ray-dashboard.html#machine-view> ,  but  never  in my test.  Do I need  to start Ray some  special way?  Also  PyTorch see CUDA device  - torch.cuda.is_available()  give me True .  But  if  I try  -  ray.get_gpu_ids() , it gives me  empty list. This happen even if I use  ray.init(num_gpus=1). I generally  want to see that my  GPU utilized during  Ray  run"}
{"question": "I'm having some issues starting a ray cluster, someone could help me in how to run a ray cluster with a head node and a worker node?"}
{"question": "Hi! I am new to use Ray. I tried a simple ray tune code in local, HPC and k8s and it worked well. However, now I am getting a error related to k8s again and again in HPC env with same code. Does anyone know how to solve it? It seems like ray has some integrations with k8s, how can I get rid of it and just run it on prem? Thanks for any advices."}
{"question": "hi.. am new to Ray and working on customEnv.. the attached code works fine.. is there a way i could access the custom class function (_save_asset_memory)_ after the training is complete for debugging purpose??"}
{"question": "hello I had asked this question on <http://discuss.ray.io|discuss.ray.io> but haven\u2019t gotten any responses so far. Would appreciate any pointers.\n\nCopy/pasted from <https://discuss.ray.io/t/how-to-install-3rd-party-python-packages-on-worker-and-head-nodes/2506>:\n\nI have a couple of 3rd party packages that my ray task depends on. I tried to install them on the worker and head nodes as follows in my cluster YML config:\n```cluster_name: snippets\n\nprovider:\n  type: aws\n  region: us-west-2\n\nsetup_commands:\n  - pip install -U avilabs-snippets```\nHowever, this errors out because it does not install the ray package before it tries to run the ray executable. My question is what is a good way to install 3rd party python packages on ray cluster?"}
{"question": "Hi, I wondering if there is a ray equivalent to a useful dask execution feature. The dask cluster API (<https://distributed.dask.org/en/latest/api.html#cluster>) exposes simple parameters for 1) number of workers, 2) number of threads per worker, and 3) Whether to use processes (true/false). This gives you an easy way to \u2018tune\u2019 your workflow for max throughput. Is there any way to get similar functionality in ray?"}
{"question": "Running into an issue with Ray 1.3.0, doing a HPO with `tune`. Ray is running in containers on Kubernetes. The containers all have access to shared NFS storage and attempting to store results there.  Based on the documentation, it seems that for a situation like this it is recommended that sync_to_driver is disabled as results are saved to the locally mounted shared storage. We are using a snippet that roughly looks as follows:\n\n```tune.run(\n    trainable,\n    config=search_space,\n    num_samples=10,\n    scheduler=ASHAScheduler(),\n    stop={\"epoch\": 10},\n    local_dir=\"/container/mounted/nfs/path\",\n    verbose=1,\n    metric=\"val_loss\",\n    mode=\"min\",\n    sync_config=tune.SyncConfig(\n        sync_to_driver=False,\n    )\n)```\nRunning into the following error:\n```(pid=190) 2021-06-16 15:42:12,907\tERROR trial_runner.py:732 -- Trial TorchTrainable_eaabe_00000: Error processing event.\n(pid=190) Traceback (most recent call last):\n(pid=190)   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 710, in _process_trial\n(pid=190)     decision = self._process_trial_result(trial, result)\n(pid=190)   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 783, in _process_trial_result\n(pid=190)     self._callbacks.on_trial_result(\n(pid=190)   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/callback.py\", line 204, in on_trial_result\n(pid=190)     callback.on_trial_result(**info)\n(pid=190)   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/syncer.py\", line 440, in on_trial_result\n(pid=190)     trial_syncer.sync_down_if_needed()\n(pid=190)   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/syncer.py\", line 262, in sync_down_if_needed\n(pid=190)     return super(NodeSyncer, self).sync_down_if_needed(NODE_SYNC_PERIOD)\n(pid=190)   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/syncer.py\", line 159, in sync_down_if_needed\n(pid=190)     self.sync_down()\n(pid=190)   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/syncer.py\", line 283, in sync_down\n(pid=190)     logger.debug(\"Syncing from %s to %s\", self._remote_path,\n(pid=190)   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/syncer.py\", line 289, in _remote_path\n(pid=190)     ssh_user = get_ssh_user()\n(pid=190)   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/cluster_info.py\", line 17, in get_ssh_user\n(pid=190)     return getpass.getuser()\n(pid=190)   File \"/home/ray/anaconda3/lib/python3.8/getpass.py\", line 169, in getuser\n(pid=190)     return pwd.getpwuid(os.getuid())[0]\n(pid=190) KeyError: 'getpwuid(): uid not found: 12574'```\nLooks like it is still attempting to sync results with SSH and the containers that we are using (not using the launcher) are as expected not enabled for SSH.\n\nAny thoughts?"}
{"question": "I'm trying to restrict resources used by Ray while running them through slurm. I can restrict CPUs through `ray.init` but a bit confused about overall memory restrictions (`_memory` vs *`object_store_memory` vs* `driver_object_store_memory`). I came across this issue but still confused.\nIs this issue solved? or still open? - <https://github.com/ray-project/ray/issues/6968>\n\nPlease help!\nThanks!"}
{"question": "Hello everyone. To use the newer collective options is necessary to install cupy ? The NCCL calls are being made by the cupy library ? Would be possible to support others libraries/frameworks like PyTorch? thanks"}
{"question": "There was some general discussion at Ray Summit about supporting a generic Beam Runner on Ray.\n\nDoes anyone else have any feedback or would like to +1 this feature request? Here\u2019s a link the the specific Github issue that we are tracking:\n\n<https://github.com/ray-project/ray/issues/16622>"}
{"question": "If I submit a ray job (i.e. run `ray submit`) on a cluster, and i don't want to stay connected to that task, just to see the results when it is done, is there a way to do it? Right now it seems to depend on my laptop remaining connected to the cluster in order to run the job, and once I disconnect the job terminates."}
{"question": "Hello everyone,\n\nWill the sessions in RaySummit be recorded?"}
{"question": "Hi everyone, I'd like to use Ray to run my optimization iteration method (not ML) on multiple GPUs or even multiple nodes.  Does Ray support some APIs/ tools to help me schedule resources and transmit data?"}
{"question": "Are the ppts to the Talks in Ray Summit available somewhere?"}
{"question": "Hey everyone!\nI have few question regrading using ray and triggering the tasks from a FastAPI server\nI want each HTTP request to trigger and forget a remote task, that its input is the output of the last running task (for that I keep a table with the current running task and their ObjectRef hex)\nBut from what I understood, the owner (FastAPI pod ?) is responsible for waiting until the input ObjectRef is finished - will it work if I want to trigger and forget?\nAnd will it work if I have multiple FastAPI pods?\nThanks!"}
{"question": "Hey guys, I'm trying to follow the docu and this issue <https://github.com/ray-project/ray/issues/10910>\nI'm running a ray tune cluster on slurm with pytorch+horovod integration, it's seems a bit unstable for now and trying to work out why...\n\nHowever, I'm wasn't able to find a proper way to resume the training, as `checkpoint_dir` seems to always be set to `None` (it does resume, but starts from epoch 0, and tune keeps incrementing `iter` from where it stopped).\nAm I doing something wrong here?\nMy code is basically as below,\nthank you already for any help and the great work on the framework!\n```import ray\nfrom ray import tune\nfrom ray.tune.suggest.hebo import HEBOSearch\nfrom ray.tune.integration.horovod import DistributedTrainableCreator\n\n# import torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom torchvision import models\nimport horovod.torch as hvd\n\nimport os\nimport glob\nfrom attrdict import AttrDict\n\n\ndef save_checkpoint(epoch, model, optimizer):\n    with tune.checkpoint_dir(epoch) as checkpoint_dir:\n        filepath = os.path.join(checkpoint_dir, 'checkpoint')\n        state = {\n            'epoch': epoch,\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }\n        torch.save(state, filepath)\n\ndef load_checkpoint(checkpoint_dir, model, optimizer):\n    filepath = os.path.join(checkpoint_dir, 'checkpoint')\n    checkpoint = torch.load(filepath)\n    model.load_state_dict(checkpoint['model'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    return checkpoint['epoch']\n\n\ndef train_model(config, checkpoint_dir):\n    args = AttrDict(**config)\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n\n    hvd.init()\n    torch.manual_seed(args.seed)\n    print(hvd.rank(), checkpoint_dir, config)\n\n    # Set up standard ResNet-50 model.\n    model = models.resnet50()\n\n    optimizer = optim.SGD(model.parameters(),\n                          lr=args.base_lr,\n                          momentum=args.momentum,\n                          weight_decay=args.wd)\n\n    # resume training from a given checkpoint_dir\n    if checkpoint_dir:\n        resume_from_epoch = load_checkpoint(checkpoint_dir, model, optimizer)\n        print(f\"LOADED CHECKPOINT WITH {checkpoint_dir}\")\n    else:\n        resume_from_epoch = 0\n\n    for epoch in range(resume_from_epoch, args.epochs):\n        # train_loss, train_accuracy = train(epoch, model, args, train_loader, optimizer)\n        # val_loss, val_accuracy = validate(epoch, model, args, val_loader)\n        # tune.report(loss=val_loss, accuracy=val_accuracy)\n        tune.report(accuracy=args.epochs/epoch)\n        save_checkpoint(epoch, model, optimizer)\n\n    return dict(accuracy=val_accuracy)\n\n\nif __name__ == '__main__':\n    ray.init(address='auto',\n            _node_ip_address=os.environ['RAY_HEAD_NODE'],\n            _redis_password=os.environ['RAY_REDIS_PASSWORD'])\n    analysis = tune.run(\n        DistributedTrainableCreator(train_model, num_hosts=32, use_gpu=True, num_slots=1,\n                                    num_cpus_per_slot=int(os.environ['SLURM_CPUS_PER_TASK'])),\n        num_samples=24,\n        resume=True,\n\t\tlocal_dir=os.environ[\"SCRATCH\"],\n\t\tsearch_alg=HEBOSearch(metric=\"accuracy\", mode=\"max\"),\n        name='hebo-search',\n        metric='accuracy',\n        mode='max',\n        config={\n            'wd': tune.loguniform(1e-5, 1e-3),\n            'base_lr': tune.loguniform(1e-5, 0.2),\n            'momentum': tune.uniform(0.75, 0.95),\n            # ...\n        }\n    )\n\n    print(analysis.best_config)```"}
{"question": "guys, is it possible to use an external store in place of plasma?"}
{"question": "Hi!\nI want to know the back-propagation implementation on ray.\nWhere is source code? Could anyone help me?"}
{"question": "```18a019e90c67af85ffffffffffffffffffffffff0100000003000000\t134.5 MiB\tLOCAL_REFERENCE\nabc5c8d75dc67420ffffffffffffffffffffffff0100000003000000\t134.5 MiB\tLOCAL_REFERENCE```\nI see these two ray objects in the memory profile. Is there any chance that they are sharing underlying memory? or two objects with different object ids must be using separate memory? (in which case I'm mistakenly making a copy). they are on same machine."}
{"question": "Does Ray run on ARM devices? I am planning to put a Ray cluster on Raspberry Pis"}
{"question": "Hiya, are there any public code examples of unit and integration tests of ray serve deployments?"}
{"question": "Hello, is there any example for dask-on-ray \"groupby apply\" operation?"}
{"question": "Hiya\n\n\u2014  Task queuing in Ray \u2014\n\nhow does ray handle backpressure? as in more tasks than resources?\ndoes ray implement some kind of queuing?\nis this queuing different between actor tasks and \u201cregular\u201d tasks ?\n\naccording to the architecture document, i understand that the queue is per-node, than what happens when an actor is being called from two different places?"}
{"question": "Hi, I am trying to use joblib with ray parallel as backend. I am using ray client to connect to a cluster of 5 remote nodes ( 4 cores each) . But unfortunately every-time I schedule about 100 processes with 20 worker nodes I get a ray system error saying that ray has not been start use ray.init() to start even when I can see all my nodes to be active on ray dashboard. Does anyone have any idea of why its happening?"}
{"question": "Hi. Is there a way to tell Ray to automatically deploy referenced local python modules to workers (containers in case of k8s) or we have to do it 'manually' ?"}
{"question": "Hello everyone how to specify as resources a node ?"}
{"question": "Hello,\nI watched this great talk by <@UN1T4KA2G>\nI was wondering how to deploy this code to a ray cluster (1.4.0) on K8s? Does this script need to be run on the head-node?"}
{"question": "Hey everyone. We are running Ray using the Kubernetes operator. We have a Dask on Ray workload we are testing out. When we run it with 100 workers everything works fine but when we run it with 800 workers Ray crashes with the following messages\n`(raylet, ip=10.0.202.24) E0630 20:40:52.155332683    5284 <http://server_chttp2.cc:49]|server_chttp2.cc:49]>        {\"created\":\"@1625110852.155244150\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":872,\"referenced_errors\":[{\"created\":\"@1625110852.155232460\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"src/core/lib/iomgr/tcp_server_posix.cc\",\"file_line\":340,\"referenced_errors\":[{\"created\":\"@1625110852.155216463\",\"description\":\"Unable to configure socket\",\"fd\":19,\"file\":\"src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1625110852.155212124\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":190,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]},{\"created\":\"@1625110852.155231412\",\"description\":\"Unable to configure socket\",\"fd\":19,\"file\":\"src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1625110852.155228306\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":190,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}`\n`(raylet, ip=10.0.202.24) Traceback (most recent call last):`\n`(raylet, ip=10.0.202.24)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/agent.py\", line 338, in &lt;module&gt;`\n`(raylet, ip=10.0.202.24)     raise e`\n`(raylet, ip=10.0.202.24)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/agent.py\", line 323, in &lt;module&gt;`\n`(raylet, ip=10.0.202.24)     raylet_name=args.raylet_name)`\n`(raylet, ip=10.0.202.24)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/agent.py\", line 75, in __init__`\n`(raylet, ip=10.0.202.24)     f\"[::]:{self.dashboard_agent_port}\")`\n`(raylet, ip=10.0.202.24)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/grpc/aio/_server.py\", line 84, in add_insecure_port`\n`(raylet, ip=10.0.202.24)     address, self._server.add_insecure_port(_common.encode(address)))`\n`(raylet, ip=10.0.202.24)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/grpc/_common.py\", line 166, in validate_port_binding_result`\n`(raylet, ip=10.0.202.24)     raise RuntimeError(_ERROR_MESSAGE_PORT_BINDING_FAILED % address)`\n`(raylet, ip=10.0.202.24) RuntimeError: Failed to bind to address [::]:42848; set GRPC_VERBOSITY=debug environment variable to see detailed error message.`\n`(raylet, ip=10.0.203.112) E0630 20:40:52.972852140     138 <http://tcp_server_posix.cc:213]|tcp_server_posix.cc:213]>    Failed accept4: Too many open files`\n`(raylet, ip=10.0.193.62) E0630 20:40:53.827097287     123 <http://tcp_server_posix.cc:213]|tcp_server_posix.cc:213]>    Failed accept4: Too many open files`\n`(raylet, ip=10.0.192.176) E0630 20:40:54.163172204     126 <http://tcp_server_posix.cc:213]|tcp_server_posix.cc:213]>    Failed accept4: Too many open files`\nIt looks like we need to increase the ulimit for open files but I don't see where that is exposed in the operator. Is this possible and does this make sense that we would be going over the 65k limit with 800 workers?"}
{"question": "The talks of the ray summit 2021 are really great! Will they be published somewhere else like YouTube maybe?\nI'm asking this because the official platform keeps crashing on my browser (Chrome or Safari, tried both) after about 15-20 minutes of video playback. It's a bit tedious to refresh the site and having to search for the last playback marker then."}
{"question": "Hi, just had a question about `ray.cancel(task)` - if my `ray serve` server is in the waiting for `ray.get(task)` in one endpoint, and I `ray.cancel(task, force=True)` from a different endpoint, shouldn\u2019t `ray.get` be interrupted as soon as the cancellation is executed?\n\nRunning into an issue where `cancel` seems to be blocked until the `ray.get` finishes, however that happens\n\nEDIT: Edward helped us fix the issue! the wait function wasn\u2019t yielding"}
{"question": "Hiya, can Ray serve be used to mirror traffic to multiple models. I'd like to run challenger models in shadow/dark mode (ie: models that are called and their prediction logged, but the response is discarded and not sent to the end user)?"}
{"question": "Hi everyone! I'm using python and windows, and ray hangs on `tune.run()`. How can I use ray in windows from python without launching the dashboard?"}
{"question": "Hello everyone. On one currently program that I am developing, sometimes I got this warning:\n```2021-07-03 18:15:08,448 WARNING worker.py:1114 -- WARNING: 24 PYTHON workers have been started on a node of the id: a74af82b9fc4175bcc55d2b9b89b39118989fc77366ff0f304915de8 and address: 10.0.2.100. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see <https://github.com/ray-project/ray/issues/3644>) for some a discussion of workarounds.```\nThe program uses nested tasks (it fit neatly for this problem). If I am not mistaken This was considered \"ok\", due to \"two level\" scheduler employed by ray. But recently there was a lot of changes to the core of the ray. This still a reasonable use case? The issue referenced is old (2018) the advise contained there still applicable?I got this warning mainly when testing on small scale experiment. I am currently controlling the number of tasks using resources (`.options(memory=memsize).remote).` . thanks :smiley:"}
{"question": "Hiya, are there any examples of integrating ray with experiment tracking tools?"}
{"question": "Hello everyone, I was running a HPO training for PPO using custom CNN network (multigpu training with 4 GPUs) and after few trials got this error:\n```tensorflow.python.framework.errors_impl.InternalError: Constraining by assigned device should not cause an error. Original root's assigned device name: /job:localhost/replica:0/task:0/device:GPU:0 node's assigned device name \"/job:localhost/replica:0/task:0/device:GPU:1. Error: Cannot merge devices with incompatible ids: '/job:localhost/replica:0/task:0/device:GPU:0' and '/job:localhost/replica:0/task:0/device:GPU:1'```\nAny idea, how to resolve this?"}
{"question": "Quick poll: does anyone have experience or know of other projects using Sentry in OSS? <https://sentry.io/for/open-source/|https://sentry.io/for/open-source/>\n\nIt seems Firefox uses it to collect crash telemetry. Something like this would be immensely useful for improving and maintaining the stability of Ray core, though there are privacy concerns for an OSS project."}
{"question": "Has anyone seen this error before?\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/dask/__init__.py\", line 25, in ray_dask_persist_mixin`\n    `return dask_persist_mixin(self, **kwargs)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/dask/base.py\", line 258, in persist`\n    `(result,) = persist(self, traverse=False, **kwargs)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/dask/base.py\", line 773, in persist`\n    `results = schedule(dsk, keys, **kwargs)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/dask/scheduler.py\", line 444, in ray_dask_get_sync`\n    `**kwargs,`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/dask/scheduler_utils.py\", line 339, in get_async`\n    `fire_task()`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/dask/scheduler_utils.py\", line 334, in fire_task`\n    `callback=queue.put,`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/dask/scheduler.py\", line 165, in wrapper`\n    `callback=callback,`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/dask/scheduler_utils.py\", line 376, in apply_sync`\n    `res = func(*args, **kwds)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/dask/scheduler.py\", line 220, in _rayify_task_wrapper`\n    `result = pack_exception(e, dumps)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/dask/scheduler.py\", line 214, in _rayify_task_wrapper`\n    `ray_posttask_cbs,`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/dask/scheduler.py\", line 298, in _rayify_task`\n    `*arg_object_refs,`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/remote_function.py\", line 175, in remote`\n    `name=name)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py\", line 292, in _invocation_remote_span`\n    `return method(self, args, kwargs, *_args, **_kwargs)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/remote_function.py\", line 217, in _remote`\n    `name=name)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 114, in client_mode_convert_function`\n    `return client_func._remote(in_args, in_kwargs, **kwargs)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/client/common.py\", line 177, in _remote`\n    `return self.options(**option_args).remote(*args, **kwargs)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/client/common.py\", line 354, in remote`\n    `return return_refs(ray.call_remote(self, *args, **kwargs))`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/client/api.py\", line 103, in call_remote`\n    `return self.worker.call_remote(instance, *args, **kwargs)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/client/worker.py\", line 305, in call_remote`\n    `return self._call_schedule_for_task(task)`\n  `File \"/usr/local/Caskroom/miniconda/base/envs/ray-test/lib/python3.7/site-packages/ray/util/client/worker.py\", line 318, in _call_schedule_for_task`\n    `raise cloudpickle.loads(ticket.error)`\n`TypeError: __cinit__() takes at least 2 positional arguments (0 given)`\nThe error is coming from the backend but I can't find any logs to explain it."}
{"question": "When I install ray, it seems to use a *very* opinionated way to specify Python for the scripts in the /bin dir, viz:\n`#!/usr/bin/python`\nShort of editing these files, is there some way I can direct it to the version of python that I need?"}
{"question": "Hi, I have a general question about Ray Serve. When using multiple replicas, in the examples I've seen the replicas load a copy of the model into memory, this ok for small models but not for large model or models using embeddings, do you typically load models as part of another Actor? (instantiated before the Ray Serve Deployment)"}
{"question": "I've got Ray running on a kubernetes cluster and it is working well except it doesn't seem to be scaling down after a job is finished. It scaled up nicely from 1 worker to 379 workers but after the job finished there are still 379 workers running ~20 minutes later. I see these logs from the autoscaler\n`ray,default:2021-07-08 14:02:00,971\tINFO autoscaler.py:208 -- StandardAutoscaler: ray-ray-worker-type-29mk4: Terminating idle node.`\n`error: unable to upgrade connection: container not found (\"ray-node\")`\n`ray,default:2021-07-08 14:02:01,068\tINFO command_runner.py:172 -- NodeUpdater: ray-ray-worker-type-7q2zq: Running kubectl -n default exec -it ray-ray-worker-type-7q2zq -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (uptime)'`\n`ray,default:2021-07-08 14:02:01,155\tINFO command_runner.py:172 -- NodeUpdater: ray-ray-worker-type-8mkg5: Running kubectl -n default exec -it ray-ray-worker-type-8mkg5 -- bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (uptime)'`\nCan someone explain the \"unable to upgrade connection\" message? Is that error causing the scale-down to not happen?"}
{"question": "Can the <https://github.com/ray-project/ray_shuffling_data_loader|ray_shuffling_data_loader> also do data augmentation or does it only do data shuffling for distributed training ?"}
{"question": "So I need to store the optimised config from a stride ( e.g. 5 days) and then apply that to the following week. Or I might optimise for 10 days and test against the following 5 days. Any advice on approach?"}
{"question": "When deploying 300 small models as a composed model in ray, is it recommended to set the requirements per CPU by load? What I mean by that is, lets say the composed models consists of only two models, A and B.\nA is huge and has about 70% of the cases, whereas B has 30% of the cases. Should this somehow be reflected in the CPU parameter? (this is how I currently do it, I just wanted to make sure I am not mis-using the ray serve API)"}
{"question": "Is it possible for Ray cluster to scale-up from 0 workers? If I start a kubernetes cluster using `minWorkers: 0` and I start a job no workers are launched. If I change that to `minWorkers: 1` then everything scales up and works fine."}
{"question": "hey, is there a way to let ray run on-demand instances from amazon in case it can't run spot instances? I have allowed different types of instances, but it keeps trying to submit the same type of instance, despite the `InsufficientInstanceCapacity` error it receives."}
{"question": "The ray.utils.queue is safe to be called from multiple producers and consumers?"}
{"question": "Does anyone know what might give rise to ID errors like \"`ValueError: ID string needs to have length 28, got 20` \"?\n\n`Traceback (most recent call last):`\n  `File \"/code/examples/mnist_pytorch_trainable.py\", line 65, in &lt;module&gt;`\n    `ray.init(address='auto', _node_ip_address=os.environ[\"ip_head\"].split(\":\")[0], _redis_password=os.environ[\"redis_password\"])`\n  `File \"/opt/conda/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 62, in wrapper`\n    `return func(*args, **kwargs)`\n  `File \"/opt/conda/lib/python3.8/site-packages/ray/worker.py\", line 839, in init`\n    `_global_node = ray.node.Node(`\n  `File \"/opt/conda/lib/python3.8/site-packages/ray/node.py\", line 186, in __init__`\n    `ray._private.services.get_address_info_from_redis(`\n  `File \"/opt/conda/lib/python3.8/site-packages/ray/_private/services.py\", line 309, in get_address_info_from_redis`\n    `return get_address_info_from_redis_helper(`\n  `File \"/opt/conda/lib/python3.8/site-packages/ray/_private/services.py\", line 266, in get_address_info_from_redis_helper`\n    `client_table = global_state.node_table()`\n  `File \"/opt/conda/lib/python3.8/site-packages/ray/state.py\", line 244, in node_table`\n    `node_info[\"Resources\"] = self.node_resource_table(`\n  `File \"/opt/conda/lib/python3.8/site-packages/ray/state.py\", line 205, in node_resource_table`\n    `node_id = ray.NodeID(hex_to_binary(node_id))`\n  `File \"python/ray/includes/unique_ids.pxi\", line 207, in ray._raylet.NodeID.__init__`\n  `File \"python/ray/includes/unique_ids.pxi\", line 33, in ray._raylet.check_id`\n`ValueError: ID string needs to have length 28, got 20`"}
{"question": "Hi , facing issues in multinode java program, running in Single mode is working fine with java but multinode is not. For the start I am using ray start --head\u00a0on one linux machine and on other linux machine creating two other nodes. To start I just created a simple maven project which has main class RayExample.\n\nGetting error : Caused by: java.lang.RuntimeException:\u00a0*The exit value of the process is 1.*\u00a0Command: python -c import ray; print(ray._private.services.get_address_info_from_redis('1.1.1.1:6379', '1.1.1.1.240', redis_password='5241590000000000', log_warning=False))\n\nIs there any working example of java application working on multinode?"}
{"question": "Hello folks. I\u2019m working on parallelizing a complex workflow in my system and I found ray.\nIt looks like an amazing library, however I\u2019m struggling with adopting it in a workflow where the results of remote tasks need to be chained.\nHere\u2019s a simplified example:\n\n```from asyncio import sleep\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass WrapperObject:\n    my_field: int\n\n\ndef execute_flow_1(my_param: WrapperObject):\n    # execute flow 1 with WrapperObject\n    sleep(5)\n    return \"fail\"\n\n\ndef execute_flow_2(my_param: WrapperObject):\n    # execute flow 1 with WrapperObject\n    sleep(5)\n    return \"success\"\n\n\nfor i in range(1, 100):\n    param = WrapperObject(i)\n    if execute_flow_1(param) == \"fail\":\n        execute_flow_2(param)```\nWhat would be the best approach to migrate this to ray, when the execution of a remote method depends on the result of the previous remote operation?\nThe only solution that comes to mind is to build a dictionary of the ray ObjectRef with the instance of the WrapperObject and  retrieve it from the dict after calling ray.get. But this gets complicated really quick if one needs to chain more then this two calls. I feel that there\u2019s a better way to do it, but I can\u2019t see it.\nAny direction would be highly appreciated."}
{"question": "Hey there!\nIf I'm not specifying `num_gpus` in `@ray.remote` decorator, my inference is crashing with the reason \"no CUDA capable device found\". Any idea why this is happening and how to resolve this?\nTIA."}
{"question": "Hey There! I have a question about how Ray Tune handles memory (specifically stuff in the object store) when using GPUs. My understanding is that when trials only have access to CPUs, each worker is a python process and they have references to stuff in the object store, which lives in shared memory. This prevents each trial from creating a copy of training data, for example, in memory. Do trials that use GPU have access to the object store as well? If not, does each trial have a copy of the data?"}
{"question": "Hello everyone ! I have a question regarding RaySGD with tensorflow. I have a working ray cluster and a script for distributed tuning which works fine. I have been trying to get <https://docs.ray.io/en/master/raysgd/raysgd_tensorflow.html#tftrainer-example|this example> working for the distributed training working (adapting ray init and parser arguments to my cluster). Regardless of the cluster\u2019s resources, every time the workers show the logs attached in the worker_logs file complaining about an unshardable dataset and no training logs, all training happens on the master node (file attached as well).\u00a0Shouldn't training logs appear on the worker node as well ?\nAlso is there any difference on how the data should be processed for distributed training? From the tensorflow documentation, for distributed data something like the following line should be added: `train_dataset = strategy.experimental_distribute_dataset(train_dataset)` which assumes we can access the distribution strategy in the dataset creation.\nAny help is appreciated, thanks !"}
{"question": "looking for some quick pointers on how I should model this problem w/ ray:\n\nI have a pytorch model that I want to map across a bunch of partitions to encode a feature.\n\neach partition is a numpy matrix w/ feature columns (npy.gz on disk)\n\nso I'll extract the partition from disk, encode the feature using the pytorch model, and then load it back to disk (npy.gz w/ new feature)\n\nI already have these routines:\n\n`def extract_transform(path, model)`\n\nand\n\n`def load(path, npy)`\n\n\u2022 I'd like to run `extract_transform` in parallel across N GPU workers\n\u2022 i'd like to run `load` in parallel across N CPU workers\nhow should I do this w/ ray?"}
{"question": "I have a very silly doubt. Should my code for remote execution be available at the workers also? Or calling the .remote method is sufficient to make sure the execution happens on the workers?"}
{"question": "Why does `ray.nodes` say \"for debugging only\"? <https://docs.ray.io/en/master/package-ref.html#ray-nodes>\n\nI'd like to use this for sizing an ActorPool (e.g., I know how much memory each actor needs, so I'll look at the available memory on each node and determine how many Actors it can fit, then sum these values up to get the final size of the pool). Is there a better way?"}
{"question": "Another question, as I scale the number of cores I am really struggling to utilize them. It seems like `ray` really struggles to get through the queue to pending tasks, and I need tasks to take longer and longer in order to leverage more cores.\nFor example, I have a simple remote function which takes one argument, and I am passing it integers. It sleeps for some time to simulate load.\n\u2022 If the tasks take around the \"rule of thumb\" minimum of 1 second (I tested tasks taking 0.9-1.9 seconds, actually) then I am only seeing around 500 cores being used in parallel (!).\n\u2022 If the tasks take 15-18 seconds, I am see a maximum of around 3000 cores used in parallel. I slowly climb to that number as the tasks get picked up and processed.\nIs there any way to speed up the scheduling, so my tasks don't need to take longer and longer as I get more cores? If not, it seems like I'll need to batch work in different sizes as I scale the cluster up, which is annoying."}
{"question": "Hello, I am using Ray to build my distributive computing application, and I want to know how to visualize the computational graph in ray? or are there any helper tools to record the computing path?"}
{"question": "I am running a ray head node on a EC2 machine, with port 6379 enabled. When I am trying to add a worker from another EC2 machine, it gives me a GCS connection error. I have posted the exact issue in this <https://discuss.ray.io/t/cannot-connect-to-head-node-on-aws/2971?u=chhaya_kumar_das|forum post> . Do I have to enable any additional ports on the head node?"}
{"question": "Ray does not seem to report any error message when the client runs ray.init, but it can't connect.  It's especially problematic when running using SLURM and all you see is a message like:\n`srun: error: compute1: task 0: Aborted`\n\nEspecially since it's hard to know who's aborting what.. did SLURM abort the job, was it something wrong with Ray?  Etc.  I had to independently run the SLURM job components and then shell into the client container and dig into the logs to find the\n`Could not connect to socket /tmp/ray/session_2021-07-23_15-48-27_960119_96589/sockets/raylet`\n\nSo the question is, can I do something to make this and other errors surface to the SLURM job output?  Or do users just have to deal with this?"}
{"question": "Hi, I'm trying to set up Ray between virtual machines within the same GCP network (the relevant port ranges are whitelisted):\n```ray start --head --port=6379 --gcs-server-port=10000 --object-manager-port=10001 --node-manager-port=10002 --min-worker-port=10003 --max-worker-port=10040\nray --logging-level debug start --address='172.16.80.42:6379' --redis-password='5241590000000000' --object-manager-port=10001 --node-manager-port=10002 --min-worker-port=10003 --max-worker-port=10040```\nOn the worker node, the \"cat raylet.out\" gives\n```[2021-07-23 19:48:30,578 W 451725 451725] <http://redis_context.cc:335|redis_context.cc:335>:  Will retry in 100 milliseconds. Each retry takes about two minutes.\n[2021-07-23 19:50:41,650 W 451725 451725] <http://redis_context.cc:335|redis_context.cc:335>:  Will retry in 100 milliseconds. Each retry takes about two minutes.\n[2021-07-23 19:52:52,722 W 451725 451725] <http://redis_context.cc:335|redis_context.cc:335>:  Will retry in 100 milliseconds. Each retry takes about two minutes.```\nAny obvious mistakes? Does redis require other ports except 6379?"}
{"question": "Hi everyone. I am trying to run some executions for a stream of messages that I am getting via a MQ. I am noticing some strange behaviour. Whenever the remote() method is called it gives me back an ObjectRef but when I ray.get() on the ObjectRef it gets stuck. I have tried removing all the heavy operations from the method(which uses the remote()) and still it gets stuck. Has anyone faced such an issue before?"}
{"question": "Can I call a ray.get on an ObjectRef that is created by another python process? I am keeping the ObjectRef in a queue( multiprocessing Queue)."}
{"question": "Does anyone currently have problems compiling the documentation locally?"}
{"question": "Hello,\n\nWe just discovered Ray and we are wondering if it could offer an experience close to AWS Lambda + GPU support for model serving.\nIdeally we would want to pay for GPU EC2 instances only when doing inferences and not 24/7.\nCan Ray (Serve + Cluster) offer that?\n\nAlso, I am not certain if I understood some parts correctly.\n\nFrom my understanding, you can host your FastAPI app on let's say a CPU EC2 instance and use Ray Serve to offload the computation. When using Ray Clusters you can have clusters of (eg: GPU) instances for the computations only, meaning it is only alive for the time of the computation and you have to handle FastAPI CPU instances with something else than Ray. Is that correct?\n\nThanks"}
{"question": "what to do if ray stays in this mode until it times out?"}
{"question": "Hi, with Ray 1.3, I was using\n```ray.util.connect(\"ADDRESS\"+\":10001\")```\nto connect to a remote Ray server. With Ray 1.4 and 1.5, I see this gives a `WRONGPASS` error. Has the API changed? Do we need to pass the password somehow?\n\nI tried following <https://docs.ray.io/en/master/cluster/ray-client.html#ray-client|this tutorial> and switched to\n```ray.init(\"<ray://ADDRESS:10001>\").connect()```\nand this is the error I get:\n```Traceback (most recent call last):\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/util/client/server/proxier.py\", line 425, in Datapath\n    if not self.proxy_manager.start_specific_server(\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/util/client/server/proxier.py\", line 200, in start_specific_server\n    output, error = self.node.get_log_file_handles(\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/util/client/server/proxier.py\", line 163, in node\n    self._node = ray.node.Node(\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/node.py\", line 181, in __init__\n    session_name = _get_with_retry(redis_client, \"session_name\")\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/ray/node.py\", line 41, in _get_with_retry\n    result = redis_client.get(key)\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/redis/client.py\", line 1606, in get\n    return self.execute_command('GET', name)\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/redis/client.py\", line 898, in execute_command\n    conn = self.connection or pool.get_connection(command_name, **options)\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/redis/connection.py\", line 1192, in get_connection\n    connection.connect()\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/redis/connection.py\", line 567, in connect\n    self.on_connect()\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/redis/connection.py\", line 643, in on_connect\n    auth_response = self.read_response()\n  File \"/lus/scratch/arigazzi/anaconda3/envs/smartsim/lib/python3.8/site-packages/redis/connection.py\", line 756, in read_response\n    raise response\nredis.exceptions.ResponseError: WRONGPASS invalid username-password pair```"}
{"question": "Hello. Is there any paper or documentation about the scheduling algorithm used by ray? I know papers like (<https://www.usenix.org/system/files/nsdi21-wang.pdf>) about the new ownership concept. However I am referring to scheduling and how the resources constraints are considered. thanks =)"}
{"question": "```Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/usr/local/lib/python3.6/dist-packages/ray/util/client_connect.py\", line 26, in connect\n    conn_str, secure=secure, metadata=metadata, connection_retries=3)\n  File \"/usr/local/lib/python3.6/dist-packages/ray/util/client/__init__.py\", line 57, in connect\n    connection_retries=connection_retries)\n  File \"/usr/local/lib/python3.6/dist-packages/ray/util/client/worker.py\", line 111, in __init__\n    raise e\n  File \"/usr/local/lib/python3.6/dist-packages/ray/util/client/worker.py\", line 89, in __init__\n    ray_ready = self.is_initialized()\n  File \"/usr/local/lib/python3.6/dist-packages/ray/util/client/worker.py\", line 357, in is_initialized\n    ray_client_pb2.ClusterInfoType.IS_INITIALIZED)\n  File \"/usr/local/lib/python3.6/dist-packages/ray/util/client/worker.py\", line 325, in get_cluster_info\n    resp = self.server.ClusterInfo(req, metadata=self.metadata)\n  File \"/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\", line 946, in __call__\n    return _end_unary_response_blocking(state, call, False, None)\n  File \"/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\", line 849, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)\ngrpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n        status = StatusCode.NOT_FOUND\n        details = \"Failed to serialize response!\"\n        debug_error_string = \"{\"created\":\"@1627510790.240898329\",\"description\":\"Error received from peer ipv4:10.0.254.218:10001\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1069,\"grpc_message\":\"Failed to serialize response!\",\"grpc_status\":5}\" ```\nAnyone seen this error? I am trying to connect from a pod to the head service of a ray cluster `ray.util.connect(\"10.0.254.218:10001\")`"}
{"question": "Hello, is there any limitation to call remote functions or actors from a torch dataloader with multiple processes ? thanks"}
{"question": "Hello, does someone happen to know the best way to reference Ray on a research paper? [ie bibtex citation]\nThere\u2019s a 2018 paper on the arxiv [Ray: A Distributed \u2026]. Would that be the way to go?\nSorry, I couldn\u2019t find info about that in Documentation."}
{"question": "Hi Guys I am trying to get ray(in one kubernetes pod) to connect to a head ray cluster in another (pod).....I also have a backend that I am trying to setup using a client. I am not sure what I am doing wrong but here is the code I currently have. The ray cluster head app is exposed using a service. Should I be using `<http://ray.int|ray.int>`  in my current pod or should it be ray.client.connect? Once I get the client to connect do I need to start ray.serve inside of the local pod or should it be started in the head pod? Confused about that......And then come the backends. Would someone be able to clarify this? My intent is to have have the function run remotely on the ray cluster that get invoked by client calls into my main web app.\n```ray.init(num_cpus=12, num_gpus=1)\nclient = serve.start(detached=True, http_host=\"0.0.0.0\", http_port=8000)\nclient.create_backend(\"composite_check\",\n                      CompositeCheck,\n                      config={\"max_concurrent_queries\": None,\n                              \"num_replicas\": 2})\nclient.create_endpoint(\"composite\",\n                       backend=\"composite_check\",\n                       route=\"/composite\",\n                       methods=[\"POST\"])```"}
{"question": "Hi, I'm trying to connect to a ray cluster using the client (default config). `ray://` on `ray.init` seems to produce an error though\n\n```ray.init(\"ray://&lt;ip&gt;:10001\")  # produces the error\nray.client(address='&lt;ip&gt;:10001').connect()  # works```\nThis is the error:\n```Traceback (most recent call last):\n  File \"test-cluster.py\", line 3, in &lt;module&gt;\n    ray.init(\"ray://&lt;ip&gt;:10001\")\n  File \"/Users/lukas/dev/virtualenvs/atriomx-3.8.5/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 62, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/lukas/dev/virtualenvs/atriomx-3.8.5/lib/python3.8/site-packages/ray/worker.py\", line 727, in init\n    redis_address, _, _ = services.validate_redis_address(address)\n  File \"/Users/lukas/dev/virtualenvs/atriomx-3.8.5/lib/python3.8/site-packages/ray/_private/services.py\", line 362, in validate_redis_address\n    redis_address = address_to_ip(address)\n  File \"/Users/lukas/dev/virtualenvs/atriomx-3.8.5/lib/python3.8/site-packages/ray/_private/services.py\", line 394, in address_to_ip\n    ip_address = socket.gethostbyname(address_parts[0])\nsocket.gaierror: [Errno 8] nodename nor servname provided, or not known```\nI don't mind using ray.client, but the docs say it will be deprecated eventually, so I'm weary. Any pointers?"}
{"question": "I'm receiving an error at shutdown (tested with latest master).\nQ. As a client, what is the proper way to disconnect/shutdown?\nQ. Do I need to explicitly discard the actor?\n```import ray\nimport sys\n\n@ray.remote\nclass FooActor:\n    pass\n\nif __name__ == \"__main__\":\n\n    # ray.init(f\"ray://{ray_host}:{ray_client_server_port}\")\n    ray.client(f\"{ray_host}:{ray_client_server_port}\").namespace(\"foo\").connect()\n\n    current_namespace = ray.get_runtime_context().namespace\n    print(f\"Current Namespace: {current_namespace}\")\n\n    actor = FooActor.remote()\n    sys.exit(0)```\n```Current Namespace: foo\nException ignored in: &lt;function ClientActorHandle.__del__ at 0x7f54fef5c670&gt;\nTraceback (most recent call last):\n  File \"/home/foo/.virtualenvs/foo/lib/python3.9/site-packages/ray/util/client/common.py\", line 219, in __del__\nAttributeError: 'NoneType' object has no attribute 'is_connected'\nAttributeError: 'NoneType' object has no attribute 'ray'\nException ignored in: 'ray._raylet.ClientActorRef.__dealloc__'\nAttributeError: 'NoneType' object has no attribute 'ray'\nError in sys.excepthook:\n\nOriginal exception was:\nError in sys.excepthook:\n\nOriginal exception was:```"}
{"question": "Hi All, I am trying to run a code remotely using below code snippet. However, it's unable to find the module at remote, which I can confirm is there. I am locating the modules using `code_search_path` . Any clue what I am missing here?\n\nSnippet:\n\n```    code_search_path=[\"/home/ray/shared/demo/project\"]\n    runtime_env={'pip':'requirements.txt'}\n    worker_env={'PYTHONPATH':'/home/ray/shared/demo/project'} \n    job_config=ray.job_config.JobConfig(code_search_path=code_search_path, runtime_env=runtime_env, worker_env=worker_env)\n    ray.util.connect(\"example-cluster-ray-head:10001\", job_config=job_config)```"}
{"question": "Can someone please tell me what am I doing wrong here in this simple function?\n\n```@ray.remote(num_returns=2)\ndef return_2_variables(x):\n    a = x + 1\n    b = x + 2\n    return a, b\n\n\nfoo = return_2_variables.remote(280)\nbar = return_2_variables.remote(290)\n\nitems_list = [foo, bar]\n\nvalue_a, value_b = ray.get([items_list[x] for x in range(2)])```\n"}
{"question": "Hello everyone,\nI think, I am missing a key concept of how to use ray.\n\nCurrent situtation:\n\u2022 I am currently still working on a Laptop but get an on-prem server in near future (unfortunately no cloud)\n\u2022 I have a machine learning project as a dvc pipeline. Means, each step is a python file which reads in some data from local filesystem, does preprocessing, data prep, etc. and saves the result back to local disk. Of course, with dvc you can upload all these files to a remote storage.\n\u2022 I want to do hyperparameter tuning for the modeling step, and my function is loading data from disk for each hyperparameter configuration.\nQuestion:\n\u2022 What would be best practice to be able to do hyperparam modeling locally and on a remote ray server with data that is result of already executed pipeline steps?\nI appreciate your help. Thank you!\nAndreas"}
{"question": "What would a reasonable size be for `mx_cocurrent_queries` and `num_replicas` for a backend? do u know if a backend will use more than 1 cpu? I notice that the pod does not use more than 1 cpu..."}
{"question": "Hi there. Not sure I\u2019m in the right place. I wanted to get some info. I\u2019m trying to connect to my ray cluster from inside a locally running docker container. When I try to connect with `ray.client('RayClusterAddr:port').connect()` I get an AssertionError mentioning a `cloudpickle.load(ticket.error)` ; however, this same code runs on my local machine outside of the container without issue (same python version and dependencies). Can anyone point me in the right direction?"}
{"question": "Hi Guys, Does ray automatically scale workers on k8s nodes? Or do we need to provide alternate means of scaling native to k8s?"}
{"question": "If i have a piece of code that I want to deploy to a ray cluster and it has certain dependencies  that the ray docker images dont have...do we need to pack those dependencies on the rauy cluster side as well?\n````In [6]: HTTPDeployment.deploy()\n(pid=297) 2021-08-04 19:34:43,110       INFO backend_state.py:869 -- Adding 1 replicas to backend 'http_deployment'.\n\n(pid=327) ERROR:    Exception in ASGI application\n(pid=327) Traceback (most recent call last):\n(pid=327)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/uvicorn/protocols/http/h11_impl.py\", line 369, in run_asgi\n(pid=327)     result = await app(self.scope, self.receive, self.send)\n(pid=327)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/uvicorn/middleware/proxy_headers.py\", line 59, in __call__\n(pid=327)     return await <http://self.app|self.app>(scope, receive, send)\n(pid=327)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/http_proxy.py\", line 319, in __call__\n(pid=327)     await self.starlette_router(scope, receive, send)\n(pid=327)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/starlette/routing.py\", line 609, in __call__\n(pid=327)     await self.default(scope, receive, send)\n(pid=327)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/http_proxy.py\", line 303, in _fallback_to_prefix_router\n(pid=327)     await _send_request_to_handle(handle, scope, receive, send)\n(pid=327)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/http_proxy.py\", line 60, in _send_request_to_handle\n(pid=327)     result = await object_ref\n(pid=327) ray.exceptions.RayTaskError(ModuleNotFoundError): ray::RayServeWrappedReplica.handle_request() (pid=167, ip=10.240.0.145)\n(pid=327)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/backend_worker.py\", line 122, in wrap_to_ray_error\n(pid=327)     raise exception\n(pid=327)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/backend_worker.py\", line 242, in invoke_single\n(pid=327)     result = await method_to_call(*args, **kwargs)\n(pid=327)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/async_compat.py\", line 29, in wrapper\n(pid=327)     return func(*args, **kwargs)\n(pid=327)   File \"&lt;ipython-input-5-a5dbb3b73ede&gt;\", line 4, in __call__\n(pid=327) ModuleNotFoundError: No module named 'riff'\n(pid=164, ip=10.240.0.104) ERROR:    Exception in ASGI application\n(pid=164, ip=10.240.0.104) Traceback (most recent call last):\n(pid=164, ip=10.240.0.104)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/uvicorn/protocols/http/h11_impl.py\", line 369, in run_asgi\n(pid=164, ip=10.240.0.104)     result = await app(self.scope, self.receive, self.send)\n(pid=164, ip=10.240.0.104)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/uvicorn/middleware/proxy_headers.py\", line 59, in __call__\n(pid=164, ip=10.240.0.104)     return await <http://self.app|self.app>(scope, receive, send)\n(pid=164, ip=10.240.0.104)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/http_proxy.py\", line 319, in __call__\n(pid=164, ip=10.240.0.104)     await self.starlette_router(scope, receive, send)\n(pid=164, ip=10.240.0.104)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/starlette/routing.py\", line 609, in __call__\n(pid=164, ip=10.240.0.104)     await self.default(scope, receive, send)\n(pid=164, ip=10.240.0.104)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/http_proxy.py\", line 303, in _fallback_to_prefix_router\n(pid=164, ip=10.240.0.104)     await _send_request_to_handle(handle, scope, receive, send)\n(pid=164, ip=10.240.0.104)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/http_proxy.py\", line 60, in _send_request_to_handle\n(pid=164, ip=10.240.0.104)     result = await object_ref\n(pid=164, ip=10.240.0.104) ray.exceptions.RayTaskError(ModuleNotFoundError): ray::RayServeWrappedReplica.handle_request() (pid=167, ip=10.240.0.145)\n(pid=164, ip=10.240.0.104)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/backend_worker.py\", line 122, in wrap_to_ray_error\n(pid=164, ip=10.240.0.104)     raise exception\n(pid=164, ip=10.240.0.104)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/backend_worker.py\", line 242, in invoke_single\n(pid=164, ip=10.240.0.104)     result = await method_to_call(*args, **kwargs)\n(pid=164, ip=10.240.0.104)   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/async_compat.py\", line 29, in wrapper\n(pid=164, ip=10.240.0.104)     return func(*args, **kwargs)\n(pid=164, ip=10.240.0.104)   File \"&lt;ipython-input-5-a5dbb3b73ede&gt;\", line 4, in __call__\n(pid=164, ip=10.240.0.104) ModuleNotFoundError: No module named 'riff'```"}
{"question": "How do we have a deployment use a specific amount of gpu? `@ray.remote(_num_cpus_=1, _num_gpus_=0.25)` is this going to run the deployment on a machine  that has that much gpu and cpu available OR is it going to use that much in actuality?"}
{"question": "<https://discuss.ray.io/t/whether-request-data-exists-in-latest-version/3147|Whether request.data exists in latest version?>"}
{"question": "Hi everyone, I'm fairly new to RLlib and I will appreciate if you could answer my question about `train_batch_size` in off-policy algorithm specially QMIX.\nI believe that the replay buffer for QMIX stores \"batches\" instead of steps from workers, and these batches are the concatenated batch from all workers at the same iteration. By default, QMIX's replay buffer stores 1000 batches, and the `train_batch_size` is 32 also by default. Does that mean in each training iteration 32 batches will be sampled from the replay buffer and will be used for training? Because the unit for QMIX's buffer is \"batches\" instead of \"steps\", therefore the default setting for `batch_mode` is `complete_episode` .\nThanks in advance."}
{"question": "Hi everyone, I have a question about RAY version. There is a available RAY v2.0.0.dev0 version? or It will be soon?"}
{"question": "Hi! I have few questions about clusters. I created a local cluster consisting of 7 nodes (84 CPUs in total). It seems to work nicely for a single job. Unfortunately, I don\u2019t understand/can\u2019t find in the docs how multiple jobs are scheduled and the resources are assigned. For instance, if I run a job (grid search with 40 trials), it occupies 40 CPUs, if I run second one (similar task), it takes next 40 CPUS, and the third starts with 4, as expected. However, when the first job finishes, the last one uses still only 4 CPUs with lot of resources unused. Is this expected behaviour? Also, why \u2018submit\u2019 is a blocking command? As an end user, I expect that I submit jobs to a queue, let the cluster do the scheduling and computing and check the results when done. Is ray cluster not intended to be used this way? Thanks!"}
{"question": "Does anyone know what are those `experiment-state.json`  and `basic-variant-state.json`  files used for? They were auto-generated outside of the log directory."}
{"question": "Hey all, is there a way to stop runtime_env from appending ray wheel from the amazonaws repository and a python3 version in the environment.yaml?  How can I add ray wheel from my own repository and use a different python3 version with runtime_env?"}
{"question": "Seems like `pip install ray==1.4.1` is the oldest available version. Does that work for you?"}
{"question": "Hi everyone,\n\nIn ray tutorial has a ray.data.read_parquet. But we tried to read data from HDFS. We got this error :\n\"ray has no attribute data\"\nHow we can solve this error?\nRay version: <https://docs.ray.io/en/master/index.html|Ray v2.0.0.dev0>\nNot: We also tried \"experimental\" method, but we got same error.\nThank you for help"}
{"question": "Hello! :slightly_smiling_face:\nI deployed a broken version of my code with ray-serve, then I wanted to clean up without having to delete the whole cluster so I went to the ray head node and called\n`serve.list_deployments()`\nto see that I only had one deployment, which I fetched\n`k = serve.list_deployments()['0']`\nand then deleted\n`k.delete()`\nhowever, I still have high CPU usage on the cluster and many dead actors. How can I clean them without having the recreate the whole ray cluster?"}
{"question": "Hi,\nI just wanted to confirm that if I use Ray Serve then I don't need to use Gunicorn?"}
{"question": "Hello, I have a question that how to get a list of head nodes' IP addresss running on a server in CLI?\nAfter starting ray, when the informations goes up in terminal, is there any way to find it in CLI?\nFor example, if we started it, we would get this. Do I need to remember the address at this moment?\n```root@fbfe31622faa:/host_server# ray start --head --dashboard-host=0.0.0.0 --num-gpus=2\n/opt/conda/envs/ray-gpu/lib/python3.9/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n  warnings.warn(\nLocal node IP: 172.17.0.5\n2021-08-13 05:46:32,506 INFO services.py:1245 -- View the Ray dashboard at <http://172.17.0.5:8265>\n\n--------------------\nRay runtime started.\n--------------------\n\nNext steps\n  To connect to this Ray runtime from another node, run\n    ray start --address='172.17.0.5:6379' --redis-password='5241590000000000'\n  \n  Alternatively, use the following Python code:\n    import ray\n    ray.init(address='auto', _redis_password='5241590000000000')\n  \n  If connection fails, check your firewall settings and network configuration.\n  \n  To terminate the Ray runtime, run\n    ray stop```"}
{"question": "Hi everyone. In my knowledge for any debugger like the ones in VSCode or Pycharm to catch breakpoints in Ray scripts, I need to first set `local_mode = True` . But this way the GPU usage is disabled, and this will cause troubles when debugging async algorithms like APPO, where inferences are ran on multi-GPU learners. So if I set `local_mode = True` , the optimizer of my policies will always get a empty list of parameters. Is there a work around?"}
{"question": "Hello all, I'm trying to commit some fixes for rllib and get the tests and all that working, but I cannot seem to get the lint testing to pass even though format.sh returns no errors/changes. It appears the linter setup in buildkite is different from that in format.sh? How can i make sure that once i run the formatter tool locally, it still will build on buildkite?"}
{"question": "Sorry to ask  such a simple question here, but I can't find any mention of it in the docs, when using the Ray Client API how does one disconnect? `ray.shutdown()` doesn't seem to do it for me"}
{"question": "Hi everyone, can I put custom configs in the `config` dict rather than putting them into its sub-dict `custom_model_config` ?"}
{"question": "Hi!\n\nWhat's the best way to update the code running in clusters using CI/CD? Ideally I'd like a new docker image being built and pulled after every push to master with as little downtime as possible. Is there any documentation for this?"}
{"question": "Hi All, I am running Ray cluster on k8s, while running a job - I keep on seeing below for new pods that comes up. Is that temporary, for the time being where ray and other libs getting installed? as I see eventually it comes up as ray node.\n\n```(raylet, ip=192.168.80.136)   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/workers/default_worker.py\", line 8, in &lt;module&gt;\n(raylet, ip=192.168.80.136)     import ray\n(raylet, ip=192.168.80.136) ModuleNotFoundError: No module named 'ray'```"}
{"question": "Do we know what is the timeline for Ray-2.0 release schedule?"}
{"question": "Hi, in my Ray Serve project, I have to download the model from S3. I initially did it like this:\n```\n@serve.deployment(route_prefix=\"/predict\", num_replicas=4)\nclass ImageModel:\n    def __init__(self):\n        self.predictor = download_and_load_model_from_s3() \n\n..```\nHowever, since `num_replicas=4`, it downloads the model 4 times probably overwriting the previous download.  I moved away from that to:\n\n```\n# Global Model\nPREDICTOR=download_and_load_model_from_s3()\n\n@serve.deployment(route_prefix=\"/predict\", num_replicas=4)\nclass ImageModel:\n    def __init__(self):\n        self.predictor = PREDICTOR \n\n..```\nIs this recommended? What would be the best practice?"}
{"question": "Dear All,\n\nI have a quick question.\nIs there a way to print the ray logs to console instead of the files?\n\nI don\u2019t see anything like this explained in the official doc\n\n<https://docs.ray.io/en/master/configure.html#logging-and-debugging>"}
{"question": "Hi all, is there a config parameter I can set in tune which will disable logging (to the log_dir)? I dont want to keep log files of my debug runs"}
{"question": "Hi there!  I'm trying to run a large CV training model using Ray SGD's wrapper for pytorch DDP.  Once the training begins after about 3% of the batches, a node dies and it seems to start training from ground zero.  Any advise on this?  I still have 20 workers alive, so I am not sure why its starting from the beginning when its supposed to distribute the training any way.  Thanks so much for your insights on this."}
{"question": "Hi <@U01Q3QAFE07> and others.  wrt to the above discussion in the thread, I find myself not using `ray.put(predictor)` so I'm wondering if I'm doing something wrong. Could you let me know what's wrong with the following code or how to use `ray.put(predictor)`  and the corresponding `ray.get`? Is the below `self.predictor` still a global variable in some way?\n\n```model_actor = \"model_actor\"\n\n@ray.remote\nclass ModelActor(object):\n    def __init__(self):\n        self.predictor = download_and_load_model_from_s3()\n\n    def predict(self, args):\n        return self.predictor.predict(args)\n\n\nmodel_actor = ModelActor.options(name=model_actor).remote()\n\n\n@serve.deployment(route_prefix=\"/predict\", num_replicas=4)\nclass ImageModel:\n    def __init__(self):\n        self.model_actor = ray.get_actor(model_actor)\n\n    async def __call__(self, starlette_request):\n            image_payload_bytes = ... \n            args = dict(image_bytes=image_payload_bytes)\n            result_ref = self.model_actor.predict.remote(args)\n            prediction_info = ray.get(result_ref)\n            ...```\n"}
{"question": "Hi, I am working on hyperparameter tuning with ray[tune] on slurm cluster\nI tried to run my ray script written with pytorch lightning on multiple nodes but it failed\nThe error message tells that pytorch lightning generates signal but it works only in main thread\nHow can I solve this problem except removing pytorch-lightning ?\n\n** error message **\n(pid=17370) 2021-08-20 10:37:06,808     ERROR function_runner.py:254 -- Runner Thread raised error.\n\n.......\n(pid=17370)   File \"/scratch/paop03/env/lightning/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/slurm_connector.py\", line 26, in register_slurm_signal_handlers\n(pid=17370)     signal.signal(signal.SIGUSR1, self.sig_handler)\n(pid=17370)   File \"/scratch/paop03/env/lightning/lib/python3.8/signal.py\", line 47, in signal\n(pid=17370)     handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n(pid=17370) ValueError: signal only works in main thread\n\n\n** scirpt **\n\n`def train_w_tune(config, num_epochs=10, num_gpus=0):`\n    `\"\"\" loader and model construction  \"\"\"`\n    `trainer = pl.Trainer(`\n                         `max_epochs=num_epochs,`\n                         `precision = config['precision'],`\n                         `# If fractional GPUs passed in, convert to int.`\n                         `gpus=math.ceil(num_gpus),`\n                         `logger=TensorBoardLogger(`\n                                                  `save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),`\n                                                  `progress_bar_refresh_rate=0,`\n                                                  `callbacks=[`\n                                                             `TuneReportCallback(`\n                                                                                `{`\n                                                                                 `\"loss(v)\": \"ts(v)\",`\n                                                                                 `\"res(v)\":  \"res(v)\",`\n                                                                                 `\"MAPE(v)\": \"MAPE(v)\",`\n                                                                                `},`\n                                                                                `on=\"validation_end\"`\n                                                                               `),`\n                                                             `TuneReportCallback(`\n                                                                                `{`\n                                                                                 `\"loss(t)\": \"ts(t)\",`\n                                                                                 `\"res(t)\":  \"res(t)\",`\n                                                                                 `\"MAPE(t)\": \"MAPE(t)\",`\n                                                                                `},`\n                                                                                `on=\"train_end\"`\n                                                                               `)`\n                                                            `]`\n                        `)`\n\n\n    `trainer.fit(model, loader1, loader2)`"}
{"question": "I am getting the `You are using the 'pickle5' module, but the exact version is unknown (possibly carried as an internal component by another module). Please make sure you are using pickle5 &gt;= 0.0.10 because previous versions may leak memory.` warning while my code runs. So, I imported pickle5 to see where it was coming from and that pointed me directly back at ray\n```pickle5\nOut[5]: &lt;module 'pickle5' from 'D:\\\\miniconda\\\\envs\\\\enigma\\\\lib\\\\site-packages\\\\ray\\\\pickle5_files\\\\pickle5\\\\__init__.py'&gt;```\nSo, if pickle is coming from you all, shouldn't there be no error?"}
{"question": "After deploying ray 1.5.2  in production we saw some of these error messages - which we currently try to tackle by adding more logging info - but has anyone else encountered them?\n```\t  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/backend_worker.py\", line 122, in wrap_to_ray_error\n    raise exception\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/backend_worker.py\", line 242, in invoke_single\n    result = await method_to_call(*args, **kwargs)\n  File \"/home/tkaymak/.local/share/virtualenvs/di-attribute-detection-ray-serve-local-MthlBLha/lib/python3.7/site-packages/ray/serve/api.py\", line 1082, in __call__\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/fastapi/applications.py\", line 208, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/starlette/middleware/errors.py\", line 172, in __call__\n    response = await self.handler(request, exc)\nTypeError: application_exception_handler() takes 1 positional argument but 2 were given```"}
{"question": "Hey, I just stumbled over runtime environments when checking your release notes:\n<https://docs.ray.io/en/latest/advanced.html?highlight=environments#runtime-environments>\nThat's an awesome feature :tada:\nHow does this work technically?\nE.g. do I need to have `ray` installed in the runtime env? If not, how does communication with the runtime work?\nAre there any restrictions on what kind of data can be transferred to the runtime env?"}
{"question": "Why ray java lib latest version is still 1.4.0? :thinking_face:"}
{"question": "hi there, from the logging related doc <https://docs.ray.io/en/releases-1.4.1/ray-logging.html#worker-logs>, it mentioned\n&gt; By default, all of the tasks/actors stdout and stderr that is redirected to worker log files are published to the driver. Drivers display logs generated from its tasks/actors to its stdout and stderr.\nmy question is that is it possible to disable the log redirect?"}
{"question": "Hi All,\n\nWe are using Ray on top of kubernetes, by default it doesn\u2019t come with any authentication it seems.\nDo anyone has any suggestion to add authentication layer? Does Ray has any library or module that we can use to include auth or OIDC layer? or something else to secure the Ray dashboard?"}
{"question": "Is there an example somewhere on how to add a node to my current ray cluster? (NodeLauncher manual use perhaps)"}
{"question": "Hi, guys. May I ask a general question on serialization? I am trying to serialize a python object with `cffi` backend (a class with python interface but some member of it was written in c as exposed with `cffi`), and I got the error `TypeError: can't pickle _cffi_backend.__CDataOwn objects` , both `pickl`  and `cloudpickle`  failed here. `inspect_serializability` confirms it with msg `!!! FAIL serialization: can't pickle _cffi_backend.__CDataOwn objects` . Could you please give me some possible ways to deal with the problem?"}
{"question": "Hi, a question about passing object_ref in actor method, looks like the the object is implicitly returned to method?"}
{"question": "I was using ray 1.5.2 and got the following error. Tried to searched online for a solution but was not able to solve it. Does anyone have similar issue before?"}
{"question": "Hey guys,\n\nI am trying to run the simple getting started tutorial from your website and after `pip install ray` when I `import ray` I am being hit with:\n\n```AttributeError: type object 'ray._raylet.Buffer' has no attribute '__reduce_cython__'```\nI am using Python version 3.8.2\n\nAny ideas?"}
{"question": "Hi, I have to expose another endpoint (`/health`) in my Ray Serve application. Currently, it's something like this:\n```@serve.deployment(route_prefix=\"/predict\", num_replicas=4)\nclass ImageModel:\n    def __init__(self):\n        self.model_actor = ray.get_actor(model_actor)\n\n    async def __call__(self, starlette_request):\n            image_payload_bytes = ... \n            args = dict(image_bytes=image_payload_bytes)\n            result_ref = self.model_actor.predict.remote(args)\n            prediction_info = ray.get(result_ref)\n\n    async def health(self, starlette_request):\n          # return 200\n          pass```\nHow do I expose the method `health` as an endpoint `/health` similar to `/predict`? Thanks for any help"}
{"question": "or any other best practice as long as it's accessible at the same port. Currently, I have another `deployment` but then I have to reduce one core from my `ImageModel` to give it here which seems wasteful?\n```@serve.deployment(route_prefix=\"/health\")\nclass HealthCheck:\n    def __init__(self):\n        <http://logger.info|logger.info>(\"HealthCheck Ready\")\n        pass\n\n    async def __call__(self, starlette_request):\n        try:\n            return JSONResponse(dict(result=\"healthy\"), 200)\n\n        except Exception as ex:\n            error_info = error_dict(ex)\n            logger.error(error_info[\"stacktrace\"])\n            return JSONResponse({\"error\": error_info}, 400)```"}
{"question": "Hi guys, I'm trying to use ray for parallel execution of a function over rows in a dataset. I've set this up manually in the EMR on AWS (on 9 nodes) one driver + 8 workers. Each node has 96 cores.\n\nTo maximize node usage, I've implemented a round robin scheduling using custom resources. The problem I've been running into is that, the remote is not getting scheduled to the fullest. If I schedule 768 tasks (1 per each cpu over the 8 worker nodes) by calling remote, only 10 end up getting scheduled, which I can see in the debug state logs on the driver and using htop on the clients. \n\nThis problem of scheduling worsens as I increase the nodes. When I start with two nodes and want to run 192 tasks, only 96 were ever found to be running. And it keeps dropping as I increased the nodes to 1+8.\n\nIs this because of the limitation on max-chunks-allowed for the pushmanager? Or is there some other issue that could be going on?\n\nReally want to use this library instead of having to write some package to share data across the nodes and use multiprocessing on each node"}
{"question": "Hi all, I am Niranda from IU Bloomington, and we are building a scalable data engineering framework <https://github.com/cylondata/cylon|Cylon>. I am thinking of extending Cylon with Ray-workflows. I just went through the Ray workflow docs section. And I have some quick questions.\n1. Are Ray-workflows are developed purely on Python (are there any C++ backends to it)?\n2. Can the resources (# of cores, memory, etc) be managed/ specified at each \"step\"?"}
{"question": "Is anybody have successful experience of creating Ray cluster on several Windows machines? I mean any way - automatically or starting each machine manually. If yes -\u00a0 what way you use:\u00a0 native Windows Ray version, use WSL and linux Ray version with docker, or use WSL and linux Ray version without docker(in conda)? I try many way, but no full success yet.:confused:\nI want to find successful case and reproduce using hints and best practices from our community. I will be glad for any help or recommendations based on experience"}
{"question": "Hi, guys. Is it possible to start ray dashboard on previous-run ray logs in /tmp/ray (just like Tensorboard)? I check the raydashboard section but there seems nothing about it."}
{"question": "Is it possible to use ray with docker-compose for testing small-scale jobs?"}
{"question": "Hello :slightly_smiling_face: .. I have a noob question(s), does ray support sharded redis? IIUC redis does support sharding in a cluster mode."}
{"question": "Hi.\n\nRunning a simple example gets me an error: <https://gist.github.com/Industrial/07053bb0445d711b099cf3d98752bb80>\n\n`AttributeError: 'NoneType' object has no attribute 'config'`, something with gpu count .. I set it to 1, but it also does it at 0\n\n1. How do I fix this error?\n2. Can I set it to use torch? Torch lists 1 gpu available."}
{"question": "Hi guys, I have a question regarding a case in Ray. I have a class A instance calling a ray remote class B instance and I want ray remote class B instance to callback a function in class A instance. Without converting class A to a ray remote class, is there any other way to achieve this? I was thinking about using a @ray.remote inner function wrapper but I am not sure if this works."}
{"question": "Hi all,\n\nI am implementing a double player(Alice and Bob) rock paper and scissor environment using MultiAgentEnv class.\n*Observation space*\u00a0contains both players\u2019 utilities(reward), and both players\u2019 action.\n*Action space*\u00a0contains 0(rock),1(paper),2(scissor)\n*Reward*\u00a0for each player: winner +20, loser -20\nIf it\u2019s a tie, one case(a) is all players will have +10, the other case(b) is all players will have 0 or -10.\nAfter 500 iteration,\nCase a) converges to Alice and Bob will always have the same action,\nwhich will maximize the total reward.\nCase b) converges to Alice always shows scissor, and Bob shows rock, which only maximize Alice reward while ignoring Bobs\u2019.\nWhat I understand is the result won\u2019t be converged to one specific action, each action will have 1/3 chance to be shown. I am wondering where is this error from?\nThanks!\n\n```# 0 Alice;1 Bob\n\nclass rpsEnv(MultiAgentEnv):\n    def __init__(self, return_agent_actions=False, part=False):\n        self.num_agents = 2\n        self.player_list = ['Alice', 'Bob']\n        self.action_space = gym.spaces.Discrete(3)\n        self.action_space_dict = {}\n        self.obs_space_dict = {}\n        for i in self.player_list:\n            self.obs_space_dict[i] = gym.spaces.Dict({\n                'Alice_utilities': gym.spaces.Box(low=np.array([-50.]), high=np.array([50.])),\n                'Bob_utilities': gym.spaces.Box(low=np.array([-50.]), high=np.array([50.])),\n                'Alice_action': gym.spaces.Discrete(3),\n                'Bob_action': gym.spaces.Discrete(3)\n            })\n        self.action_space_dict = {\n            i: gym.spaces.Discrete(3)\n            for i in self.player_list\n        }\n        self.obs = {i: {\n            'Alice_utilities': np.array([0]),\n            'Bob_utilities': np.array([0]),\n            'Alice_action': 0,\n            'Bob_action': 0\n        } for i in self.player_list}  # Observations.\n        self.rew = {i: 0. for i in self.player_list}  # Rewards.\n        self.done = {i: False for i in self.player_list}\n        self.done['__all__'] = False\n\n    def reset(self):\n        self.obs = {i: {\n            'Alice_utilities': np.array([0]),\n            'Bob_utilities': np.array([0]),\n            'Alice_action': 0,\n            'Bob_action': 0\n        } for i in self.player_list}\n        return self.obs\n\n    def cal_rewards(self, action_dict):\n        if action_dict['Alice'] == 0:\n            if action_dict['Bob'] == 2:\n                reward['Alice'] += 20\n                reward['Bob'] -= 20\n            elif action_dict['Bob'] == 1:\n                reward['Alice'] -= 20\n                reward['Bob'] += 20\n            else:\n                reward['Alice'] -= 0\n                reward['Bob'] -= 0\n        elif action_dict['Alice'] == 1:\n            if action_dict['Bob'] == 0:\n                reward['Alice'] += 20\n                reward['Bob'] -= 20\n            elif action_dict['Bob'] == 2:\n                reward['Alice'] -= 20\n                reward['Bob'] += 20\n            else:\n                reward['Alice'] -= 0\n                reward['Bob'] -= 0\n        else:\n            if action_dict['Bob'] == 1:\n                reward['Alice'] += 20\n                reward['Bob'] -= 20\n            elif action_dict['Bob'] == 0:\n                reward['Alice'] -= 20\n                reward['Bob'] += 20\n            else:\n                reward['Alice'] -= 0\n                reward['Bob'] -= 0\n        return reward\n\n    def step(self, action_dict):\n        self.obs = {i: {\n            'Alice_utilities': np.array([0]),\n            'Bob_utilities': np.array([0]),\n            'Alice_action': 0,\n            'Bob_action': 0\n        } for i in self.player_list}\n        self.rew, self.done, info = {}, {}, {}\n        reward = self.cal_rewards(action_dict)\n        for i in self.player_list:\n            self.obs[i]['Alice_action'] = action_dict['Alice']\n            self.obs[i]['Bob_action'] = action_dict['Bob']\n            self.obs[i]['Alice_utilities'] = reward['Alice']\n            self.obs[i]['Bob_utilities'] = reward['Bob']\n            self.done[i], info[i] = True, {}\n\n        self.rew = reward\n        self.done[\"__all__\"] = True\n        # print(\"Alice: \", action_dict['Alice'])\n        # print(\"Bob: \", action_dict['Bob'])\n        return self.obs, self.rew, self.done, info\n\n\ndef env_creator(_):\n    return rpsEnv()\n\n\nsingle_env = rpsEnv()\nenv_name = \"rpsEnv\"\nregister_env(env_name, env_creator)\npolicy_graphs = {}\nfor i in single_env.player_list:\n    policy_graphs[i] = (None, single_env.obs_space_dict[i],\n                        single_env.action_space_dict[i], {})\n\n\ndef policy_mapping_fn(agent_id):\n    return agent_id\n\n\nconfig = {\n    \"log_level\": \"WARN\",\n    \"num_workers\": 1,\n    \"num_cpus_for_driver\": 1,\n    \"num_cpus_per_worker\": 1,\n    \"lr\": 5e-3,\n    \"model\": {\"fcnet_hiddens\": [8, 8]},\n    \"multiagent\": {\n        \"policies\": policy_graphs,\n        \"policy_mapping_fn\": policy_mapping_fn,\n    },\n    \"env\": \"rpsEnv\"\n}\n\nexp_name = 'double_win'\nexp_dict = {\n    'name': exp_name,\n    'run_or_experiment': 'PG',\n    \"stop\": {\n        \"training_iteration\": 500\n    },\n    'checkpoint_freq': 20,\n    \"config\": config,\n}\n\nray.init()\ntune.run(**exp_dict)```"}
{"question": "Hi I am a Ray beginner. I submit a PR <https://github.com/ray-project/ray/pull/18364> and two of the CI test failed. I think those are  flaky tests and I would like to restart the test. Do I have permission to do that?"}
{"question": "Hi all, Documentation says that \"Note that to create these actors successfully, Ray will need to be started with sufficient CPU resources and the relevant custom resources.\". Is there  a way to timeout actor creation when resources are not available?\nThis is related to ray serve deployment where if deploy is called with more number of replicas than the available resources then serve will keep on trying to deploy those replicas."}
{"question": "<https://ray-distributed.slack.com/archives/C01DLHZHRBJ/p1630776140092900> so it seems there is an ongoing effort to target this as per <https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview> \"There is an ongoing effort to support high availability for the GCS, so that it may run on any and multiple nodes, instead of a designated head node.\" is there an issue or PR that I could look at to understand more?"}
{"question": "````Traceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/modules/node/node_head.py\", line 299, in _update_error_info\n    pid_errors.append({\nAttributeError: 'ImmutableList' object has no attribute 'append'\n2021-09-08 06:59:11,416 INFO node_head.py:272 -- Received a log for 10.240.0.108 and 194\n2021-09-08 06:59:11,417 ERROR node_head.py:308 -- Error receiving error info.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/modules/node/node_head.py\", line 299, in _update_error_info\n    pid_errors.append({\nAttributeError: 'ImmutableList' object has no attribute 'append'\n2021-09-08 06:59:11,521 INFO node_head.py:272 -- Received a log for 10.240.0.108 and 195\n2021-09-08 06:59:11,704 ERROR node_head.py:308 -- Error receiving error info.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/modules/node/node_head.py\", line 299, in _update_error_info\n    pid_errors.append({\nAttributeError: 'ImmutableList' object has no attribute 'append'\n2021-09-08 06:59:11,721 INFO node_head.py:272 -- Received a log for 10.240.0.108 and 194\n2021-09-08 06:59:11,747 INFO web_log.py:206 -- 127.0.0.1 [08/Sep/2021:13:59:11 +0000] \"GET /nodes?view=details HTTP/1.1\" 200 82598 \"<http://localhost:49487/>\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"\n2021-09-08 06:59:11,765 INFO web_log.py:206 -- 127.0.0.1 [08/Sep/2021:13:59:11 +0000] \"GET /logical/actor_groups HTTP/1.1\" 200 56631 \"<http://localhost:49487/>\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"\n2021-09-08 06:59:11,765 INFO web_log.py:206 -- 127.0.0.1 [08/Sep/2021:13:59:11 +0000] \"GET /tune/availability HTTP/1.1\" 200 284 \"<http://localhost:49487/>\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"\n2021-09-08 06:59:11,802 ERROR node_head.py:308 -- Error receiving error info.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/modules/node/node_head.py\", line 299, in _update_error_info\n    pid_errors.append({\nAttributeError: 'ImmutableList' object has no attribute 'append'\n2021-09-08 06:59:11,823 INFO node_head.py:272 -- Received a log for 10.240.0.108 and 195\n2021-09-08 06:59:12,119 ERROR node_head.py:308 -- Error receiving error info.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/modules/node/node_head.py\", line 299, in _update_error_info\n    pid_errors.append({\nAttributeError: 'ImmutableList' object has no attribute 'append'\n2021-09-08 06:59:12,161 ERROR node_head.py:308 -- Error receiving error info.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/modules/node/node_head.py\", line 299, in _update_error_info\n    pid_errors.append({\nAttributeError: 'ImmutableList' object has no attribute 'append'\n2021-09-08 06:59:12,187 INFO node_head.py:272 -- Received a log for 10.240.0.76 and 407\n2021-09-08 06:59:12,216 INFO node_head.py:272 -- Received a log for 10.240.0.108 and 189\n2021-09-08 06:59:12,232 INFO node_head.py:272 -- Received a log for 10.240.0.108 and 195\n2021-09-08 06:59:12,249 INFO node_head.py:272 -- Received a log for 10.240.0.108 and 194\n2021-09-08 06:59:12,521 ERROR node_head.py:308 -- Error receiving error info.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/modules/node/node_head.py\", line 299, in _update_error_info\n    pid_errors.append({\nAttributeError: 'ImmutableList' object has no attribute 'append'\n2021-09-08 06:59:12,542 ERROR node_head.py:308 -- Error receiving error info.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/new_dashboard/modules/node/node_head.py\", line 299, in _update_error_info\n    pid_errors.append({\nAttributeError: 'ImmutableList' object has no attribute 'append'\n2021-09-08 06:59:12,629 INFO node_head.py:272 -- Received a log for 10.240.0.108 and 195\n2021-09-08 06:59:12,646 INFO node_head.py:272 -- Received a log for 10.240.0.108 and 194\n2021-09-08 06:59:12,655 INFO node_head.py:272 -- Received a log for 10.240.0.39 and 206\n2021-09-08 06:59:12,905 ERROR node_head.py:308 -- Error receiving error info.\nTraceback (most recent call last):```\nHitting this issue on ray..."}
{"question": "Hi, What's the difference between version 2.0 and version 1.x? Is there any document for this?"}
{"question": "Does anyone know if the `distributed_torch_runner.py` uses a \u201cserver first\u201d protocol approach to establish a connection between the workers?"}
{"question": "Hi I have a question about the design of RLlib's `policy_optimizer` design. After a year of service, `multi_gpu_learner.py` is now deprecated and what will be in the future?"}
{"question": "&gt; Transformations are executed\u00a0_eagerly_\u00a0and block until the operation is finished.\nIs it possible to to map transformations async way?\n\n<https://docs.ray.io/en/latest/data/dataset.html#transforming-datasets>"}
{"question": "Hi, If I not using the\u00a0Ray cluster launcher,\u00a0just deploy a Ray cluster on k8s manually,  should I start Ray client server on the head node manually ?"}
{"question": "Hi!  I'm trying to use Ray for a bioinformatics pipeline that includes various python libraries.  Its giving me an error of data channel shut down.  Is it just not possible to run bioinformatics pipelines on ray?  Has anyone tried yet?  Here's the exact error:\n```\nConnectionError: Cannot send request due to data channel shutting down. Request: req_id: 8```\n"}
{"question": "Is it possible for external system to have read access to the Plasma Object Store used by each Ray worker? The reason I ask is that I want to do zero-copy reads between Ray and another, non-Python distributed framework"}
{"question": "For the older version of ray, I modified parts of ray.init() to pass port ranges which comply with our internal firewall config. The logic seems to have changed recently with ray==1.5.1. Is there some simple means to restrict allocated ports to a range of say 10000 to 10000+512?\n```    ray_params = ray.parameter.RayParams(\n        object_manager_port=10002,\n        node_manager_port=10003,\n        gcs_server_port=10001,\n        min_worker_port=10004,\n        max_worker_port=10040,\n        redis_shard_ports=[10000],\n        redis_port=6379, ```"}
{"question": "Ah, it looks like there has been a refactoring ray.parameter.RayParams =&gt; ray._private.parameter.RayParams. In any case, is there a more principled way to globally set all port ranges the library uses?"}
{"question": "Hello, everyone, I got some questions about the placement group API. If I build a placement group\nwith N bundles and with each bundle having M units of a resource X, for instance Object storage\nmemory or number of GPUs. If I use this placement group for instance as:\n\n```python \nbundle={\"X\":M} \npg = placement_group([bundle]*N,strategy=STRATEGY) \nray.get(pg) \n@ray.remote(resources={\"X\":1}) \ndef task(itask:int)-&gt;Any: \n    pass \nfor itask in range(ntask): \n    task.options(placement_group=pg).remote(itask) ```\n\nThe first M tasks (the quantity of resources of one bundle) will be allocated on only\none bundle (the first) is this correct? To spread the tasks in round robin (across bundles/nodes\nif using strategies like spread and strict_spread) manner would\nbe necessary to use the option `placement_group_bundle_index`. Is there any\nrecommendation against using this option.\n\nThe second question is if I had nested tasks/parallelism that also depends on the resource \"X\", f\nor instance CPUs would be possible that due the scheduling of the \"outer levels\" tasks would be impossible to\nschedule the \"inner tasks\" due to a lack of resources?\n\nthanks"}
{"question": "Hello everyone, I am trying to dump the ray dashboard logs for my model such that I can log where my model is deployed (on which nodes ) and what is the memory taken by my model. Since it is showing into dashboard i am very sure that this particular thing is getting saved in any object, but i am not getting that. Anyone knows about this ??\n\nThanks in advance :pray:"}
{"question": "Hi everyone, a very simple question. How to read the \"params.pkl\"? I tried pickle.load() it but it didn't work out that way. Any suggestions pls."}
{"question": "Hey all, I'm trying to use a runtime environment on ray 1.6, but I'm having no success when using a requirements file. Using this config\n```runtime_env = {\n    \"working_dir\": \".\",\n    \"pip\": \"./requirements.txt\",\n}```\nI am hitting this error: `ValueError: /home/ubuntu/requirements.txt is not a valid file` . I suspect this is due to the working dir not being properly shared to the ray nodes? Would anyone know how to resolve this? :slightly_smiling_face:"}
{"question": "Hi, is there an example of use Ray with BeautifulSoup?"}
{"question": "Hi  -  As someone trying to get into ray from prefect pipelines.  What would be the best approach Ray Core or Pipelines..  Is storing tasks and handling Tasks the main difference between Ray Core and Pipelines?"}
{"question": "Hi, just getting starting with ray on a local cluster. A few problems if anyone could share a tip:\n1. ~`raylet` is eating my RAM even after shutdown of all ray processes (`ray down cluster.yaml &amp;&amp; ray stop`). How can I stop it to reclaim my RAM?~ (apparently `ray stop` takes its time :slightly_smiling_face: )\n2. how to control for each node where object store will be spilled to?"}
{"question": "Hi, which image is based on python3.8 in docker hub? I find that images with py38 tag are based on python37 actually.  I have tried 3 images, including few hours ago pushed."}
{"question": "anyone used ray remote with jax and xla?"}
{"question": "Hi everyone. Is raytune is faster than combining optuna with ray ?"}
{"question": "```---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-67-bc7ab63d1a4e&gt; in &lt;module&gt;\n     39    mode=\"min\",\n     40    search_alg=OptunaSearch(),\n---&gt; 41    num_samples=25)\n     42 taken = time.time() - start\n     43 print(f\"Time taken: {taken:.2f} seconds.\")\n\nc:\\users\\naveen\\anaconda3\\envs\\expai_without_interpret\\lib\\site-packages\\ray\\tune\\tune.py in run(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\n    530     tune_start = time.time()\n    531     while not runner.is_finished() and not state[signal.SIGINT]:\n--&gt; 532         runner.step()\n    533         if has_verbosity(Verbosity.V1_EXPERIMENT):\n    534             _report_progress(runner, progress_reporter)\n\nc:\\users\\naveen\\anaconda3\\envs\\expai_without_interpret\\lib\\site-packages\\ray\\tune\\trial_runner.py in step(self)\n    508 \n    509         # This will contain the next trial to start\n--&gt; 510         next_trial = self._get_next_trial()  # blocking\n    511 \n    512         # Create pending trials. If the queue was updated before, only\n\nc:\\users\\naveen\\anaconda3\\envs\\expai_without_interpret\\lib\\site-packages\\ray\\tune\\trial_runner.py in _get_next_trial(self)\n    652         if not any(trial.status == Trial.PENDING\n    653                    for trial in self._live_trials) or wait_for_trial:\n--&gt; 654             self._update_trial_queue(blocking=wait_for_trial)\n    655         with warn_if_slow(\"choose_trial_to_run\"):\n    656             trial = self._scheduler_alg.choose_trial_to_run(self)\n\nc:\\users\\naveen\\anaconda3\\envs\\expai_without_interpret\\lib\\site-packages\\ray\\tune\\trial_runner.py in _update_trial_queue(self, blocking, timeout)\n   1115         self._updated_queue = True\n   1116 \n-&gt; 1117         trial = self._search_alg.next_trial()\n   1118         if blocking and not trial:\n   1119             start = time.time()\n\nc:\\users\\naveen\\anaconda3\\envs\\expai_without_interpret\\lib\\site-packages\\ray\\tune\\suggest\\search_generator.py in next_trial(self)\n     86         if not self.is_finished():\n     87             return self.create_trial_if_possible(self._experiment.spec,\n---&gt; 88                                                  self._experiment.dir_name)\n     89         return None\n     90 \n\nc:\\users\\naveen\\anaconda3\\envs\\expai_without_interpret\\lib\\site-packages\\ray\\tune\\suggest\\search_generator.py in create_trial_if_possible(self, experiment_spec, output_path)\n     93         logger.debug(\"creating trial\")\n     94         trial_id = Trial.generate_id()\n---&gt; 95         suggested_config = self.searcher.suggest(trial_id)\n     96         if suggested_config == Searcher.FINISHED:\n     97             self._finished = True\n\nc:\\users\\naveen\\anaconda3\\envs\\expai_without_interpret\\lib\\site-packages\\ray\\tune\\suggest\\optuna.py in suggest(self, trial_id)\n    356             # Use Optuna ask interface (since version 2.6.0)\n    357             if trial_id not in self._ot_trials:\n--&gt; 358                 self._ot_trials[trial_id] = self._ot_study.ask(\n    359                     fixed_distributions=self._space)\n    360             ot_trial = self._ot_trials[trial_id]\n\nAttributeError: 'Study' object has no attribute 'ask'```\nCan someone please tell me what it means ?"}
{"question": "Hi, just a quick question about RLlib. Is there an API that I can disable RLlib auto-adjusting rollout fragment length w.r.t. train batch size? I want my rollout fragment length to be fixed that I don't really mind the size of train batches."}
{"question": "And I don't really get this part. How does an increase in the number of environments running in parallel helps to increase \u201cthe performance of inference bottlenecked workloads\"? Thanks"}
{"question": "Hi everyone! Who has faced serious limitations with `Dask on Ray` implementations? Not that I have but this is just to make sure that , this \"too good\"  is true!!"}
{"question": "Hey Everyone.. I am trying to use PPO algorithm (available in ray[rlllib]).\nWith num_workers = 4, I get reproducible results on CPU machine (my local). However on a GPU machine with 104 cores, it is not giving reproducible results.  Can someone help ? I am setting the seed as follows:\n```import ray.rllib.agents.ppo as ppo\nconfig = ppo.DEFAULT_CONFIG.copy()\nconfig['num_workers'] = 20\nconfig['seed'] = 123```"}
{"question": "Can anyone point me towards an example of how to use FastAPIs `TestClient` against a composed model with ray serve? I would like to understand if I can use the TestClient, or if I need to use  a \"real\" one for functional tests"}
{"question": "has anyone gotten explainable boosted machines (<https://github.com/interpretml/interpret>) to scale well via ray?"}
{"question": "Hi everyone, I have a question about setting up prometheus to export Ray metrics for Ray clusters running on k8s. According to the doc (<https://docs.ray.io/en/latest/ray-metrics.html#getting-started-cluster-launcher>), a head node saves prometheus metric export network information as a file into its local filesystem, and this file should be configured and accessed by a Prometheus service. If the Prometheus service runs on a separate pod, this file cannot be directly accessed. Is there a suggested way to set up Prometheus for k8s Ray clusters?"}
{"question": "any idea why this might be happening? I followed the instructions from <https://docs.ray.io/en/master/auto_examples/tips-for-first-time.html>, and when I run that code it parallelizes and completes much quicker"}
{"question": "Hello there, is there any image that built on cuda10?"}
{"question": "Sorry for the innocent question, but it seems like there are no conda packages for ray on conda-forge, say. And it looks like they don\u2019t exist at all. I\u2019m interested in the reasoning behind that. Is it because of the pip extras mechanism that seems to be missing in conda?"}
{"question": "Hi, I use ray.init to connect to remote cluster and get all messages printed in remote functions . How can I disable this feature ?"}
{"question": "Is there anyone using Ray with Windows today?"}
{"question": "Hi all, I am very interested in the scheduling a normal task in the Ray. Does anyone know about if the task is spilled back to other nodeB according to the stale information, what is the next step? This task will be spilled back again or be queued on the nodeB until there is enough local resource?"}
{"question": "does it make sense to use Ray-Modin on a machine with 144 cores and 512 GB of RAM?"}
{"question": "or is Ray-Modin better across 2..n machines?"}
{"question": "sounds great, thanks! Is there any uppper bound for a single machine (cores and RAM)?"}
{"question": "is it possible for Ray to ready from Plasma object stores co-located with Ray workers?"}
{"question": "Hello everyone,\nI am trying to get ray to run in a docker container. The really strange thing is that I can access the endpoint (created using FastAPI - just in case it matters) at <http://127.0.0.1:8000> if I specify `--network host` in the docker run command but I cannot access it if I pass `-p 8000:8000` instead. I already read that ray uses lots of ports for different purposes. Do I really need to open additional ports to access the endpoint on port 8000?\nMy Dockerfile looks like this:\n```FROM rayproject/ray \n\nARG DEBIAN_FRONTEND=noninteractive\nENV TZ=Europe/Berlin\n\nWORKDIR /src/\nCOPY test.py /src/test.py\n\nCMD [ \"python\", \"test.py\" ]```\nAnd the test.py like this:\n```import time\n\nimport ray\nfrom fastapi import FastAPI\nfrom ray import serve\n\nray.init()\n\nserve.start()\n\napp = FastAPI()\n\n\n@app.get(\"/hello\")\ndef f():\n    return \"Hello from the root!\"\n\n\n# ... add more routes, routers, etc. to `app` ...\n\n\n@serve.deployment(route_prefix=\"/\")\n@serve.ingress(app)\nclass FastAPIWrapper:\n    pass\n\n\nFastAPIWrapper.deploy()\n\nwhile True:\n    time.sleep(1)```\nI build it with `docker build --tag ray_test:latest --file Dockerfile .`  and run the container with `docker run  -t --tty --interactive --rm --name ray_test --network host ray_test:latest`  or `docker run  -t --tty --interactive --rm --name ray_test -p 8000:8000 ray_test:latest`\nAny idea what the cause of this could be?"}
{"question": "Hello, I'm trying to run Ray on Mac OS (Big Sur), but currently I get this error (Ray 1.6.0):\n```arigazzi$ ray start --head\nLocal node IP: &lt;EXTERNAL_IP&gt;\nTraceback (most recent call last):\n  File \"/usr/local/anaconda3/bin/ray\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ray/scripts/scripts.py\", line 1923, in main\n    return cli()\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/click/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/click/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/click/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/click/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/click/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ray/scripts/scripts.py\", line 609, in start\n    ray_params, head=True, shutdown_at_exit=block, spawn_reaper=block)\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ray/node.py\", line 248, in __init__\n    self.start_head_processes()\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ray/node.py\", line 890, in start_head_processes\n    self.start_redis()\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ray/node.py\", line 714, in start_redis\n    port_denylist=self._ray_params.reserved_ports)\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ray/_private/services.py\", line 860, in start_redis\n    primary_redis_client.set(\"NumRedisShards\", str(num_redis_shards))\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/redis-3.5.3-py3.7.egg/redis/client.py\", line 1801, in set\n    return self.execute_command('SET', *pieces)\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/redis-3.5.3-py3.7.egg/redis/client.py\", line 898, in execute_command\n    conn = self.connection or pool.get_connection(command_name, **options)\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/redis-3.5.3-py3.7.egg/redis/connection.py\", line 1192, in get_connection\n    connection.connect()\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/redis-3.5.3-py3.7.egg/redis/connection.py\", line 567, in connect\n    self.on_connect()\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/redis-3.5.3-py3.7.egg/redis/connection.py\", line 643, in on_connect\n    auth_response = self.read_response()\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/redis-3.5.3-py3.7.egg/redis/connection.py\", line 739, in read_response\n    response = self._parser.read_response()\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/redis-3.5.3-py3.7.egg/redis/connection.py\", line 470, in read_response\n    self.read_from_socket()\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/redis-3.5.3-py3.7.egg/redis/connection.py\", line 429, in read_from_socket\n    raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\nredis.exceptions.ConnectionError: Connection closed by server.```\nAs far as I understand, the problem is that Ray starts on 127.0.0.1, but this address is then changed to the external IP (in `address_to_ip`), but this seems to prevent the Redis server (or one of them) to spin.\n\nI've also opened <https://github.com/ray-project/ray/issues/17907|an issue on GitHub>, but received no reply. Is Ray not supported on Mac OS?"}
{"question": "Hi, I find that there are 7 threads in my Python driver program after connecting to remote Ray cluster. I'd like to know what they do, just for io operations?"}
{"question": "Hey :slightly_smiling_face:\nIs it possible to have scheduling task(s) with Ray? preferred even with ray serve\nthe closest thing I saw in the documentation is scheduler in ray tune which isn\u2019t really what i\u2019m looking for"}
{"question": "Does anyone have any tips for testing functionality that uses Ray Actors without needing to spin up Ray in every test? So some way of mocking out Ray Actors?\n\ne.g.\n```def foo(remote_solver, local_object_a, **kwargs):\n    score = ray.get(remote_solver.evaluate.remote(local_object_a))\n    key = ray.get(remote_solver.get_key.remote())\n    return score[key]```\nIf I wanted to test this function (or classes that rely on this function), I'll need to spin up a `ray.init` call inside each test and that just feels wrong. The other option was that I had a MockSolver class that could simulate the `remote` API, but then the returned object is not `ray.get`-able."}
{"question": "Hi Team,\n\nWe have been using Ray on Kubernetes cluster and recently got the pen-test done of the setup. Pentesting team has found a venerability in which Ray server performs server-side DNS lookups of arbitrary domain names.\n\nFor example if we hit:\n`<https://ray.yourdomain.com/log_proxy?url=>&lt;value&gt;`\n\nDoes anyone has any clue how to fix the issue? Thanks"}
{"question": "Is it possible to update objects in the object store using their objectID? For instance, if I push an object to the object store and receive an ObjectID, but I want to  now update the object associated to that ObjectID. This would be analogous to initializing a variable in code and then updating that variable to a different value afterwards, but still use the same variable name"}
{"question": "Hi all, quick question: what\u2019s the proper way to return a the value function for a grouped model in rllib? I\u2019m currently returning a tensor of shape (batch, num_agents_in_group), but this causes errors in ppo, which expects a tensor of (batch,)."}
{"question": "Is anyone else experiencing this:<https://github.com/ray-project/ray/issues/19294|https://github.com/ray-project/ray/issues/19294> when upgrading from 1.6.0 to 1.7.0?"}
{"question": "Hey all! Is there currently a recommended way for implementing offline batch inference using ray?"}
{"question": "I'm passing large object to ray remote functions and am noticing that if I put an object to the object store (`ray.put`) &amp; want to retrieve it in the remote fun, and I try to pass the ref to the remote function, ray throws an exception. Also looking at the docs (<https://docs.ray.io/en/latest/ray-design-patterns/closure-capture.html?highlight=ray.put>) - the remote function gets the reference `big_array` from the context, not the remote function argument. Why can't the reference be passed as an  remote function argument?"}
{"question": "Hi, I'm having some troubles understanding what are the custom resources which we can define and assign to an actor. I see in the examples something like \"CUstom1\" \"Resource1\" But I cannot get what they could be and how would they map to something concrete. If I wanted to set, say, memory limit, or whatever else, what coudl it be and how woudl I do it?"}
{"question": "Hi everyone,\n\nI have a data. I applied 'rolling' in Pandas and its ok. But when I used in Ray, it has data count output different from Pandas. Why it's happen?\n\nThank you!"}
{"question": "Hi :slightly_smiling_face:\nGeneral question : Would you recommend Ray in order to do ETL task ?"}
{"question": "Hi, is there a way to read from Hive metastore using Ray? would you recommend it? i saw there is read_parquet() method.\nIn most of our cases, we would like to read from a partitioned table on top of parquet files(s3), not directly from storage. What is the recommended way to do that?"}
{"question": "When a worker process calling ray.get() to get values from remote actor, will it be scheduled to execute another task if this operation can not completed immediately ?"}
{"question": "Hi, Whenever i connect to a remote cluster. During shutdown of my process i get the following warnings printed:\n`Error in sys.excepthook:`\n`Original exception was:`\n\nAny ideas to why?\nPython 3.8 on Ubuntu using Ray 1.6.0"}
{"question": "Hi everyone. I have set up the ray cluster on k8s and trying to `ray.init(ray_service_DNS:6379)` from prefect flow job, in order to run some remote task, but somehow, i have some problems with reddis:\n```Task 'init_ray': Exception encountered during task execution!\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/prefect/engine/task_runner.py\", line 865, in get_task_run_state\n    logger=self.logger,\n  File \"/usr/local/lib/python3.7/site-packages/prefect/utilities/executors.py\", line 328, in run_task_with_timeout\n    return task.run(*args, **kwargs)  # type: ignore\n  File \"&lt;string&gt;\", line 57, in init_ray\n  File \"/usr/local/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 62, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/ray/worker.py\", line 844, in init\n    connect_only=True)\n  File \"/usr/local/lib/python3.7/site-packages/ray/node.py\", line 166, in __init__\n    self._init_temp(redis_client)\n  File \"/usr/local/lib/python3.7/site-packages/ray/node.py\", line 283, in _init_temp\n    session_dir = _get_with_retry(redis_client, \"session_dir\")\n  File \"/usr/local/lib/python3.7/site-packages/ray/node.py\", line 41, in _get_with_retry\n    result = redis_client.get(key)\n  File \"/usr/local/lib/python3.7/site-packages/redis/client.py\", line 1606, in get\n    return self.execute_command('GET', name)\n  File \"/usr/local/lib/python3.7/site-packages/redis/client.py\", line 901, in execute_command\n    return self.parse_response(conn, command_name, **options)\n  File \"/usr/local/lib/python3.7/site-packages/redis/client.py\", line 915, in parse_response\n    response = connection.read_response()\n  File \"/usr/local/lib/python3.7/site-packages/redis/connection.py\", line 756, in read_response\n    raise response\nredis.exceptions.ResponseError: WRONGTYPE Operation against a key holding the wrong kind of value```\nCould anyone give any advice, please? Thanks in advance!\n\nOne of the possible reasons that is stream_key is missmatched with a hash_key, but im not sure about it"}
{"question": "Hi! I started a cluster on a GCP instance and then tried to connect my local laptop as a node through the external IP of the head node, but after ray.init, the command hangs indefinitely in the 'connecting' state. I did port forwarding, both inwards and outwards, at the instance. In fact, I can connect to the dashboard and the client ports alright. It is only through port 6379 that things don't seem to work. I did also try to tunnel this port via ssh but the results are quite the same.\nWhat might I be doing wrong?"}
{"question": "When using Ray Serve, Should I think about each of the classes decorated with `deployment`  as actors that will exist on multiple processes with their own state? If so, is there a right way to store/manage singleton state?"}
{"question": "I am trying to setup a local cluster on my machine to persist/reuse detached actors that can be reused for inference. However, the local cluster appears to stall when running tune, with all of the trials pending until it times out. Any idea what could be the issue?"}
{"question": "I submitted too many tasks to Ray at once :sweat: Now it's taking 1.5 seconds per task to `ray.get` the results, which seems high to me. I think the tasks are all done, since the dashboard (and `htop` ) show nothing happening on the nodes in the cluster, so this is apparently just retrieval. Does that seem normal?"}
{"question": "What would be the solution for batching and minibatch prefetching when using ray datasets?"}
{"question": "I have tried to setup a local cluster, but I cannot connect the worker node to the head node, can anyone help here?"}
{"question": "When two or multi tasks been submitted to same ray cluster via `ray.init(\"<ray://ip>:port\")`, the remote tasks logs got mixed up if them are processing at the same time. Could these logs been separated for different ray client?"}
{"question": "Hi guys! Does anyone has an experience of initializing ray `ray.init(\"ip:port\")` that is working as a service from an another k8s service, so using internal cluster ip?"}
{"question": "Hi, I was wondering something, we are using ray in production, actually since version 1.2, would it be possible to have us in the list of who uses ray on your website?"}
{"question": "Is there a way to run a function on every worker node that joins a cluster? I'd like to call a few things like `mkl.set_num_threads(1)`  etc but I don't know how to make sure that happens as each node joins"}
{"question": "The use of the postprocess_trajectory method seems to be depreciated in favor of postprocess_fn. Is this the case or am I misunderstanding something? If so, I can write a PR to update the docs as they always refer to postprocess_trajectory (e.g. <https://docs.ray.io/en/latest/rllib-env.html#multi-agent>)"}
{"question": "Not Strictly a ray question, but I've mfa on aws account and ray cluster commands (submit,down,init) ask for mfa every time. With direct aws-cli commands, mfa is needed again only after some time, but it's not like that with ray. Is there any command or config option to get similar aws mfa behavior with ray?"}
{"question": "Trying to run RaySGD example: train_fashion_mnist_example, with Ray 1.7.1. Getting AssertionError: Default process group is not initialized... any idea? log attached."}
{"question": "Hey, is there any demo/documentation of building an on premise cluster with several of my personal computers? My goal is to collect data on one node, and experiment on another node. Any direction would greatly be appreciated :slightly_smiling_face:"}
{"question": "Hey guys, I was wondering if Ray allows multiple nodes to do the same trial? In other words, do sampling on two separate nodes (with separate IP and machine) and send the sampling data back to the node where it hosts the trainer/driver thread.\n\nThank you."}
{"question": "Is it a good design pattern that implement the driver program as a HTTP server? It calls ray.init() in startup and executes remote tasks according to http requests. Is there other better ways to do this?"}
{"question": "Would you say `ray.util.queue.Queue` is very cheap to use? For example, how does the idea of \"using 1000 Queues to schedule \u00b5s level computation tasks\" sound? Is this an overkill?"}
{"question": "Hello, is it possible to get the raw values for actor resource usage? The dashboard shows useful graphs, but I want to collect the raw resource usage values to understand how actor placement effects performance of different algorithms."}
{"question": "Hi guys, I'm new to ray. May I have a question that when I was following the example of raysgd, I found if I run same epoch compare to non-paraller program, raysgd need more epoch to achieve same loss. Raysgd split the dataset to num_workers part, and each processor train on a samller dataset. How can I confirm they compute the gradient rightly?"}
{"question": "Does anyone try integrate ray with kubeflow? After the survey, I only found a use case or tutorial the replace the kubeflow with ray."}
{"question": "I am using Ray tune for my hyperparameters but getting this error : ValueError: The parameter loc has invalid values. This occurs probably when we divide by zero, that is the model diverges. In Optuna there is a catch parameter that ignores value error and tries out the next set of hyperparameters. Is there a similar way to do this in ray tune?"}
{"question": "Did anyone manage to get local modules running in functions decorated with ray.remote? sys.append doesn't work."}
{"question": "Hi guys, I have a question regarding Ray client. Currently does ray support java version of Ray Client?"}
{"question": "*Problem*\nI'm trying to connect to a Dockerized Ray instance in `rayproject/ray-ml`.\nThe Dockerfile `rayproject/base-deps` specifies Python 3.7.7:\n<https://github.com/ray-project/ray/blob/2652ae7905e89c8c4ea2fff7565aea66e89f4716/docker/base-deps/Dockerfile#L14>\n\nAnd so I encounter this fatal error in my Django container (`python:3.8-slim`):\n&gt;  RuntimeError: Python minor versions differ between client and server: client is 3.8.12, server is 3.7.7\n*Questions*\n1. Do I need to use `python:3.7-slim` ?\n2. [alternatively] For a simple, single-node Ray Serve instance, would my using of the `python:3.8-slim` base image + `pip install \"ray[default]\"` be sufficient? I don't need GPU at the moment. "}
{"question": "It's not returning any large arrays or anything, so I guess the overhead is in calling it? How would I go about diagnosing this?"}
{"question": "Hi, is there a reason why this section of the documentation (<https://docs.ray.io/en/releases-1.5.2/iter.html>) is not available on 1.7.1 ?"}
{"question": "Is there anyone here doing distributed linear regression or clustering in practice?\n\nIf so, what tools are you using?"}
{"question": "Hi guys, Is there any documentation available to understand DeltaCat features and how to use it??"}
{"question": "Hey, I'm thinking of implementing Rust bindings to the C++ API, there are existing frameworks to do this relatively easily, and it's probably the cleanest way to do things since C++ is already native (rather than building a worker in Rust, which is probably months of work). \n\nJust curious, since C++ is native, couldn't the entire worker implementations of python and java just be bindings to C++? (I guess not due to reference counting language features, especially since those are GCed languages?)"}
{"question": "In my application I would like to download a data file to each node that is being reused across multiple tasks. I can just check whether the file is on disk and if not, download it. But how can I handle the cleanup after I don't need the file anymore? Can I run a `remove` task on all nodes?  Could I tie this into the object store somehow to handle the lifetime of the on-disk data?"}
{"question": "I was curious to know if something like Ray could be used to build a federated learning system?"}
{"question": "We are trying memory management on cluster node. Although we have given 5GB in function, it's just using 200MB. How we can solve this problem?"}
{"question": "Is there any way to configure the total memory used by Object Store in one node?"}
{"question": "In the docs <https://docs.ray.io/en/latest/cluster/guide.html#deploying-an-application>\nIt says: Using `ray.init(\"ray://&lt;host&gt;:&lt;port&gt;\")` is generally a best practice because it allows you to test your code locally, and deploy to a cluster with *no code changes*.\nHowever it's not clear to me. Don't I need to manually change in this very code line the host and port for local and cluster deployment? Moreover, it seems when deploying to a cloud service, we need to change the host:port every time we launch a new cluster?  I'm afraid I don't understand the use cases for this  vs ray.init(address=\"auto\") and ray.init()."}
{"question": "Hello everybody! great community you got here. One question, can it seems I cannot use runtime_env feature with linux subsystem for windows, is this correct or am i doing something wrong?"}
{"question": "Hello All, I am trying to parallelize a for loop and get the results back. I approach it as follows:\n`remote_tasks = []`\n`for s,c in sources_collectors_list[:2]:`\n\u00a0 \u00a0 `remote_tasks.append(detect_jump.remote(s, c, df_scores_all))`\n\n`ready_refs, remaining_refs = ray.wait(remote_tasks, num_returns=1, timeout=None)`\n\nAfter `ray.wait`, I get the following error\n`2021-11-01 21:03:02,316\tERROR worker.py:79 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): ray::detect_jump() (pid=1241, ip=198.18.7.8)`\n`RuntimeError: This function was not imported properly.`\n\nWhat is the reason and how can I debug this?"}
{"question": "Hello. I'm curious if there's a way for a worker node to access the parameters assigned in `@remote`? Specially, can a worker see how much memory and CPU it should have?"}
{"question": "Can I use Ray's remote functions in an environment where workers might get pre-empted and killed? I tried and got an error after a bit that was something like \"ObjectMissingError\" saying one of the inputs to the remote function was not available. It wasn't clear to me if that was recoverable or not"}
{"question": "Hi, I do it according the guide but It is very strange that I can not import train from ray I don't know what causes this Traceback (most recent call last):\n    from ray import train\nImportError: cannot import name 'train' from 'ray'. Has this module been adjusted places?"}
{"question": "Hi. Dashboard dependencies missing when installing `ray[default]`\n```$ python -m pip install ray[default]==1.6.0\n...\nray_head_1  | Collecting ray[default]\nray_head_1  |   Downloading <https://ssw-063409947478.d.codeartifact.eu-central-1.amazonaws.com/pypi/platform/simple/ray/1.7.1/ray-1.7.1-cp38-cp38-manylinux2014_x86_64.whl> (53.7 MB)\n...\nray_head_1  | /usr/local/lib/python3.8/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.```\nI am not using any venv because it's running inside a `python-3.8-slim` container on an ubuntu EC2 instance\n\nany ideas? .. all the required dependencies from <https://github.com/ray-project/ray/blame/master/dashboard/optional_deps.py> seem to be there ..."}
{"question": "I am setting up an automatic cluster on GCP to use Ray Tune but some weird things happen.\nFirst, when I start from no worker nodes but scale up to, say, two, only the first node will start working, and the other will spit out a `ValueError: count must be a positive integer (got 0)` related to `aiohttp`. But right after the first experiment ends, the second worker gets unstuck and both workers work as expected. This only happens when autoscaling, if I start with 2 workers to begin with, the problem does not happen.\nSecond, when I test my implementation with a small dataset, even though the problem above occurs, the tuning process completes, but when I run a larger dataset, I get a `ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task` at the very end and the experiment fails. That happens even if I run fewer epochs, meaning it is directly related to the size of the dataset, which I pass to `tune.run` with `tune.with_parameters`.\nThere are way too many parameters to tweak and I have no idea where to begin.\nI am on Ray 1.6 since 1.7 is having issues with memory allocation when using GPU.\nAny suggestions?"}
{"question": "Hi there,\n\nDoes anyone know why cluster status is empty when I do `kubectl -n ray get rayclusters` ? I deployed this via Helm"}
{"question": "How exactly do I have to configure the firewall rules to access the dashboard? I am working on GCP, I create a rule to the network my instances are being created on, for ingress from TCP 8265, accepting from IPs 0.0.0.0/0, but when I access localhost:8265, I get `No data received from localhost, ERR_EMPTY_RESPONSE`."}
{"question": "Hello am using a 2.0 py3.7 nightly installed yesterday. It doesn\u2019t seem to contain this PR from 20 days ago. Is this expected? <https://github.com/ray-project/ray/pull/19321>"}
{"question": "yes this link is definitely not current master <https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-2.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl> can someone please investigate?"}
{"question": "Hi <#C01DLHZHRBJ|general> , I am a data engineer working for an insurtech company in the UK (part of one of the biggest UK retail insurance companies). We run tech meetups and we'd love to have someone from the Ray project come and give us a demo of the tool. Would anyone be interested? (DM me if so :slightly_smiling_face: )"}
{"question": "I upgraded my ray cluster installed in kubernetes manually from 1.7 to 1.8. The YAML file I used is same as before, but I get many repeated logs in the dashboard:\n```\t2021-11-04 17:39:58,828\tINFO autoscaler.py:267 -- \n======== Autoscaler status: 2021-11-04 17:39:58.828594 ========\nNode status\n---------------------------------------------------------------\nHealthy:\n 1 node_b1c4f2496de3c2695f472c256b30ef70a193187ae814a4555fed4618\n 1 node_9a7bcdd2d61d6d1d3f4656e2ecdb1019c2bf16fd44bf503b00209429\n 1 node_0fb474ae15c557611a2633bde1dfe0e7ad1f66d0bc6749b7d3e6dbb8\n 1 node_4d2164f2a8f163c97efa500aca356af3f355d0ed0945d7b554505f5b\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/22.0 CPU\n 0.00/166.880 GiB memory\n 0.00/74.176 GiB object_store_memory\nDemands:\n (no resource demands)```\nIt seems autoscaler output thousands of lines of logs in a few minutes. How can I fix this? Thanks."}
{"question": "Something seems very off since those versions in the error message aren't these versions? <https://pypi.org/project/pytorch-lightning>"}
{"question": "Is anyone here doing *tabular deep learning?*"}
{"question": "Hi, are there any possibilities to limit the parallelism of the Ray's `dask` scheduler?\nI often run into the issue that `ray_dask_get` submits all Dask tasks at once and then kills some Ray workers due to OOM.\nThanks to Ray's redundancy the jobs will most often still complete after a while, but my pipeline still detects that a OOM event happened and considers the job as failed.\nWhen using the `threaded` Dask scheduler this does not happen."}
{"question": "When i execute \n<@UUWBZMU76>.remote\ndef func():\n     do heavy compute work..\n\nref = func.remote()\nray.get(ref)\n\nWill this function will run distributed in some way? Or just use the ray cluster resources? \n\nIf ill use a for loop, i understand it will run on parallel."}
{"question": "Hi, is there a way to allocate memory for an object directly in the plasma store and write directly to that memory location, and then commit it to the store once writing is complete?\nOr is this in fact the way large tabular data is backed in the store, by transferring ownership of the object directly\u2026?\nI don\u2019t think so or the api needs to take ownership of the object from the calling function\u2026 e.g. move semantics in C++"}
{"question": "Oh wait, now that I read about it, that\u2019s the whole point of plasma, right? Afterwards, you call seal to make it immutable"}
{"question": "Unfortunately, while that\u2019s the plasma API, there\u2019s no way to specify this zero-copy/direct buffer writing behaviour directly through the ray API, right?"}
{"question": "Also, is there a way to use epochs to tell the object store to eagerly collect stale objects instead of relying on memory pressure/per-object reference counting that must be synchronised with the GCS?"}
{"question": "Or is the behaviour already quite eager? Meaning a store tries to delete objects frequently even in the absence of memory pressure."}
{"question": "Hello, what is the recommended way to resolve dashboard failing to start issue? The issue seems to be with `aiohttp==3.8` not compatible with current codebase. Using latest ray release `ray==1.8.0`"}
{"question": "Hello, I am trying to build a docker image above the ray image, but it gives permission denied error.\n```FROM rayproject/ray-ml:latest-gpu\nENV TZ=Asia/Seoul\nRUN sudo ln -snf /usr/share/zoneinfo/$TZ /etc/localtime\nRUN sudo echo $TZ &gt; /etc/timezone```\nWhen I run the code above, I get `bash: /etc/timezone: Permission denied` . Is there any way to solve this? Putting sudo in front of it doesn't seem to solve the problem. Similar problems occur for `apt-get install` commands. Thanks."}
{"question": "did anyone have a change to use Ray on Spark on production? Can you please elaborate on the use case?"}
{"question": "Is there an existing Ray Prometheus based grafana dashboard?"}
{"question": "Greetings all,\n\nI am exploring Ray Serve. Have heard good things about it during the past year but haven't had the chance to implement it into a project. Now we are looking to do some batch model inferencing on historical documents (80M +). We have a fastapi real time model prediction service already in production. Could we use Ray Serve to adapt that model prediction service so that it can score those 80M + docs? If we can use a single service to do both real-time and batch that would be ideal."}
{"question": "Hi, I have a Ray cluster that is scaling very slowly even though `upscaling_speed` is set to 10.0. Is this a known issue?\n\nAt launch `ray status` shows 4 workers pending and over 500 pending tasks in the demands. After these workers launch `ray status` shows 1+ tasks pending and never more than 1 worker in the pending queue."}
{"question": "Hello everyone. This message is normal?\n```<http://reference_count.cc:1206|reference_count.cc:1206>: Object locations requested for ffffffffffffffffffffffffffffffffffffffff01000000eaa10000, but ref already removed. This may be a bug in the distributed reference counting protocol.)```"}
{"question": "I can't use ray for a cluster and use a bunch of CPU's to render images?"}
{"question": "Does anyone how what could be the root cause? The code works fine without Raytune"}
{"question": "Hi,\nlooking for help with logging. Trying to integrate graylog (graypy) into ray. especially with ray.remote().\nThe documentation (<https://docs.ray.io/en/master/ray-logging.html#how-to-set-up-loggers>) looks like there is no help from ray in creating a consistent logging experience \"out-of-the-box\".\n\nI'm currently thinking on overloading the ray.remote() decorator to include the logger setup ... any other ideas?"}
{"question": "Hi all, could you please point to an example where I could store an Arrow array from a task and use it in another using the plasma store?"}
{"question": "I'm a bit confused about how code is shipped across a ray cluster. When importing code (third-party libraries or local modules), what files have to be present on worker nodes, and what gets sent over the wire by ray?"}
{"question": "Hi, all! Could anyone advise, how to initialized a named actor with some parameters:\n```@ray.remote\nclass modelActor(object):\n    def __init__(model_id):\n        self.model_id = model_id\n        self.model_opts = None\n        self.model = None\n        \n    def set_model_id(self, model_id):\n        self.model_id = model_id\n    \n    def print_model_id(self):\n        print(self.model_id)  \n    \n    def init_model(self, collection):\n        self.model_opts = [i for i in collection.find({'mongo_id': self.model_id})][0]\n        self.model = cloudpickle.loads(model_opts['model'])\n        \n    def predict(self, collection, X_train, y_train):\n        self.init_model(collection)\n        self.model.fit(X_train, y_train)\n        prediction = model.predict(X_test)\n        return prediction```\nFor example with `model_id` ?\nIf the actor creation is the following:\n```actor = modelActor.options(name=\"orange3\", lifetime=\"detached\", namespace=\"actors\").remote()```"}
{"question": "Hey guys, I am quite new to ray and I am working on a large climate change dataset. My initial aim is to use ray to parallelize my task of tweet preprocessing, extracting the sentences and prepositions from it. I have code for each part.How should I go about it?"}
{"question": "Is ray on GCP or AWS easier to use than say on Ray on K8s ?   I mean ray running on plain GCP or AWS instances vs via K8s implementation."}
{"question": "Can we run non-python code on Ray? Specifically things like Golang and Rust?"}
{"question": "anyone know how to get around this error in ray 1.5.0??\n\n```(raylet) Traceback (most recent call last):\n(raylet)   File \"/opt/conda/lib/python3.7/site-packages/ray/new_dashboard/agent.py\", line 22, in &lt;module&gt;\n(raylet)     import ray.new_dashboard.utils as dashboard_utils\n(raylet)   File \"/opt/conda/lib/python3.7/site-packages/ray/new_dashboard/utils.py\", line 20, in &lt;module&gt;\n(raylet)     import aiohttp.signals\n(raylet) ModuleNotFoundError: No module named 'aiohttp.signals'```"}
{"question": "Is there any benefit of running <http://Ray.io|Ray.io> with Kubernetes backend vs <http://Ray.io|Ray.io> + just instances (cloud backend)?"}
{"question": "Hi, could anyone advise, please? Im trying to load the model into actor by call, from actor class:\n```import cloudpickle\n@ray.remote\nclass modelActor(object):\n    def __init__(self, model_id):\n        self.model_id = model_id\n        self.model_opts = None\n        self.model = None\n        \n    \n    def init_model(self, collection):\n        self.model_opts = [i for i in collection.find({'mongo_id': self.model_id})][0]\n        self.model = cloudpickle.loads(model_opts['model'])```\nbut then i get the error `TypeError: can't pickle _thread.lock objects` Is there a way to overcome this, except getting the model outside the actor?"}
{"question": "Hi there, I am new to ray and ray serve. Presently, testing out Ray locally (16 GB RAM). I am increasing the replica of the model to 2. Making 4 requests to the model at parallel. I am facing memory issues constantly. what should be done to overcome this?"}
{"question": "If I am using two replicas of the model, how the GPU will be used between the two replicas?"}
{"question": "is there a way to provision a cluster on AWS that use private IPs? The `ray submit` thread seems to never be able to get the private IP\u2026"}
{"question": "I was using the Ray Trainer and encountered an issue on the *window* machine, which is akin to <https://github.com/PyTorchLightning/pytorch-lightning/issues/5358>. The solution is to switch the accelerator fro 'ddp' to 'dp' for PytorchLightning. How do we change the config on Ray Train?"}
{"question": "Hi! Regarding `Ray Workflows` (can't find a channel for that, so putting the question in this channel). I have been noticing that if I keep a running Ray cluster and run multiple workflows, the first time it runs, things work well - for example, the driver program gets logs and `runtime_env` is loaded correctly in all `workflow.step` functions. However, when I run a new workflow, i no longer can see logs in the driver program and I noticed that `runtime_env` is not updated either - it seems to be using the same env from the first workflow request. I am suspecting that this has to do with the fact that the first time I run a workflow, `workflow.init()` creates the `WorkflowManagerActor` and runs the workflow. Next time I run a new workflow, it re-uses the `WorkflowManagerActor` and the steps are called from that Actor's runtime context, hence the logs don't go to the driver, and the runtime_env never gets updated. Is this a known limitation/bug of this feature? What are the plans around improving Ray Workflows?"}
{"question": "Hi! I observed that when connecting to a remote ray cluster, tune results are saved on the ray head (and not on the local machine that we are running the script from). Is this behavior expected? Can we instead save the results to the local machine running the python script (or, say, HDFS)? Thanks!"}
{"question": "Hi! I also have a workflow question. I'm trying to figure out how to do one database connection for each process (I think this is a similar request <https://github.com/ray-project/ray/issues/13938>). Does Ray have the equivalent of celery's `celeryd_after_setup`? Thanks!"}
{"question": "Hello everyone is there any measures that I need to be aware or problems to use an actor inside others and basically use the remote method of this actor to control the access (serialize) to an resource   ?\nLike the following snippet?\n```import ray\nfrom ray.util import ActorPool\n\n@ray.remote\nclass ResourceActor:\n    def __init__(self)-&gt;None:\n        # create resource\n    def store(self, dat):\n        # use resource\n\n@ray.remote\nclass Worker:\n    def __init__(self, resource_actor)-&gt;None\n        self._resource_actor = resource_actor\n        # other stuff\n\n    def doWork(self, x)-&gt;None:\n        # do work\n        self._resource_actor.store(work_done)\n\nresource_actor = ResourceActor.remote()\nworkers = [Worker(resource_actor) for _ in range(num_workers)]\n\npool = ActorPool(workers)\n\npool.map(fn, values)```"}
{"question": "Hi guys seeing this error\n```\n(ImplicitFunc pid=6738) Training Completed.\n2021-11-19 05:06:32,780 INFO tune.py:630 -- Total run time: 20.84 seconds (20.60 seconds for the tuning loop).\n2021-11-19 05:06:32,793 WARNING experiment_analysis.py:678 -- Could not find best trial. Did you pass the correct `metric` parameter?\nTraceback (most recent call last):\n  File \"main.py\", line 141, in &lt;module&gt;\n    main()\n  File \"main.py\", line 134, in main\n    print(\"Best trial config: {}\".format(best_trial.config))\nAttributeError: 'NoneType' object has no attribute 'config'```"}
{"question": "Hi - I'm seeing this error after the ray runtime is started (when I call ray.init()).\n```Exception in thread \"main\" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkNotNull(Ljava/lang/Object;Ljava/lang/String;Ljava/lang/Object;)Ljava/lang/Object;\n        at io.ray.runtime.util.BinaryFileUtil.getNativeFile(BinaryFileUtil.java:63)\n        at io.ray.runtime.util.JniUtils.loadLibrary(JniUtils.java:67)\n        at io.ray.runtime.RayNativeRuntime.start(RayNativeRuntime.java:77)\n        at io.ray.runtime.DefaultRayRuntimeFactory.createRayRuntime(DefaultRayRuntimeFactory.java:39)\n        at io.ray.api.Ray.init(Ray.java:38)\n        at io.ray.api.Ray.init(Ray.java:25)     ```\nI'm on a single machine and using Ray <http://1.8.in|1.8.0 in> a Java application on an Ubuntu 20.04 machine. Does anyone have any idea how I can resolve this issue?"}
{"question": "hello, I\u2019m looking for a way to set `--extra-index-url` when pip installing packages to the `runtime_env` \u2026 is this possible some how? I\u2019ve tried adding it inline while listing the packages, creating a `pip.conf` on the nodes of the cluster, haven\u2019t gotten anything to work yet"}
{"question": "Hi, Does Ray consider the location of the required data when scheduling computing tasks?"}
{"question": "Hello Community!! I\u2019m very new to Ray and I tried the basic example available on Github. I tried to run it on Google Colab and its GPU. I edited the code a bit to notice its working. Here the code:\n```#EXAMPLE OF RAY\nimport ray\nray.init()\nimport time\n@ray.remote\ndef f(x):\n    print(f\"With Process {x}\")\n    time.sleep(10)\n    return x * x\n\nfutures = [f.remote(i) for i in range(4)]\nprint(futures)\nprint(ray.get(futures))\nray.shutdown()```\nI noticed that at one particular time, only\u00a0*two*\u00a0parallel processes were running and others were waiting for them to finish.\nI\u2019m trying to run more than 2 parallel process.\nCan anyone please give me a clue how to do it? Thanks in advance!!"}
{"question": "Hi, I split my source code into 2 python files but got ModuleNotFoundError: No module named 'imagedata' when calling _myfunction.remote()._ I have checked sys.path and it contains the directory where imagedata.py is in. How can I fix this error? It is ok when all source code in single python file."}
{"question": "I know I can resolve it by installing them. How would these dependency reflect in `requirements.txt` file?"}
{"question": "Does ray already use work stealing by default? I have a lot of workers sitting idle but ~100 tasks pending"}
{"question": "Hi has anyone run into this issue? I just changed out my graphics card to a 3080, and it was running fine before with a 1660 super, but now I can't use gpus with rllib"}
{"question": "Hi all! I'm currently preparing a small demo of some ray basics for some colleagues. I would like to base it on the Google Cloud Platform. I would like to get the full install, and so I used the following command in the cloud shell (under Python 3.7):\n\npip3 install --user --upgrade 'ray[default]'\n\nThe installation works basically fine, but I get the following warning:\n\nray 1.0.1.post1 does not provide the extra 'default'\n\nDoes this mean that the cluster launcher has not been installed? At least \"ray up --help\" works as expected, so maybe it has been installed anyway..."}
{"question": "Hi, I\u2019m trying to use Raytune on an AWS EC2 cluster with HuggingFace and I want to use multiple GPUs per training run. However, when I try (without deepspeed), I get the error\n\n```RuntimeError: Expected tensor for 'out' to have the same device as tensor for argument #3 'mat2'; but device 1 does not equal 0 (while checking arguments for addmm)```\nWith deepspeed it just ends up using 1 GPU instead of multiple. Any suggestions as to what might be the issue?"}
{"question": "Hi all!\n\nI am aiming to maximize shared memory use, but having an issue (see attached screenshot).\n\nI.e. Function `series_scalar` takes output from `validate_and_preprocess_series` and deseralization time is 0,35 seconds.\n\nData passed between nodes is plain dicts with large numpy arrays and small other types. What are the requirements for getting low deserialization overhead and how should we go about debugging deserialization?"}
{"question": "Hi all, I'm using the Autoscaler to try and set up my first cluster on Google cloud platform. When I issue the ray up command, a head node is created with an external IP, and the client tries to contact the head node via the external IP. \n\nI have looked at the documents for the YAML configuration and several example files, but I can't figure out the answer to the following question: Is it possible to configure the communication with the head node to use the internal IP address and, if possible, maybe even avoid the creation of an external IP?"}
{"question": "Running `ray down` can take a long time because it seems to terminate VMs in series. Is there any way it can be run in parallel, reducing the shutdown time from O(n) to O(1)?"}
{"question": "When I call `ray.autoscaler.sdk.request_resources`` I can see VMs being created in my Google cloud console, but I don't see the same workers in `ray status`. Will they appear eventually? Does the status command just not know they are pending?"}
{"question": "Hi all! Is there a way to specify the GPU ID for a Ray Actor? I tried to set `CUDA_VISIBLE_DEVICES`  in the actor\u2019s `runtime_env` option, but it seems like it will be overwritten by ray\u2019\u2019s internal logic about `num_gpus`"}
{"question": "Hi, I am following the tutorial \u2018ray crash course\u2019 and got this with `ray.init()` :\n```{'node_ip_address': '127.0.0.1',\n 'raylet_ip_address': '127.0.0.1',\n 'redis_address': '127.0.0.1:6379',\n 'object_store_address': '/tmp/ray/session_2021-11-23_13-43-26_620474_37219/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2021-11-23_13-43-26_620474_37219/sockets/raylet',\n 'webui_url': None,\n 'session_dir': '/tmp/ray/session_2021-11-23_13-43-26_620474_37219',\n 'metrics_export_port': 61382,\n 'node_id': '89d80e0f337de32fd4f2e47f4b163c424642ced9b9c8ccc0bddf5099'}```\nWhy is my webui_url is None? I have installed Jupyter Lab and everything else is working"}
{"question": "Hello! Just getting started with Ray, and I was wondering does deploying a Ray cluster on AWS give me high availability with workers across AZs? If not, would I need to set up a cluster on each AZ with 1 head node per AZ?"}
{"question": "Hi All, I am trying to use tune and ray tasks in combination as follows:\n```import time\nimport random\nfrom ray import tune\n\n@ray.remote\ndef get_random_sleep():\n    sleep_time = random.randint(5, 15)\n    time.sleep(sleep_time)\n\ndef run_me(config):\n    ray.get([get_random_sleep.remote() for i in range(5)])\n\n    out = f\"{config['hello']} -&gt; {config['ray']}\"\n    tune.report(output=out)\n\nconfig = {\n    \"hello\": tune.grid_search([list(\"wo\")]),\n    \"ray\": tune.grid_search([list(\"tu\")]),\n}\nanalysis = tune.run(run_me,\n    num_samples=1,\n    config=config,\n)```\nFor some reason the processing keeps running. In the dashboard logs I was able to find some errors like this one:\n\n```2021-11-24 10:48:21,549\tINFO monitor.py:189 -- Starting autoscaler metrics server on port 442172021-11-24 10:48:21,550\tERROR monitor.py:196 -- An exception occurred while starting the metrics server.Traceback (most recent call last): File \"/home/ssa/.local/lib/python3.8/site-packages/ray/autoscaler/_private/monitor.py\", line 192, in __init__ prometheus_client.start_http_server( File \"/home/ssa/.local/lib/python3.8/site-packages/prometheus_client/exposition.py\", line 148, in start_wsgi_server httpd = make_server(addr, port, app, ThreadingWSGIServer, handler_class=_SilentHandler) File \"/usr/local/lib/python3.8/wsgiref/simple_server.py\", line 154, in make_server server = server_class((host, port), handler_class) File \"/usr/local/lib/python3.8/socketserver.py\", line 452, in __init__ self.server_bind() File \"/usr/local/lib/python3.8/wsgiref/simple_server.py\", line 50, in server_bind HTTPServer.server_bind(self) File \"/usr/local/lib/python3.8/http/server.py\", line 138, in server_bind socketserver.TCPServer.server_bind(self) File \"/usr/local/lib/python3.8/socketserver.py\", line 466, in server_bind self.socket.bind(self.server_address)OSError: [Errno 98] Address already in use2021-11-24 10:48:21,551\tINFO monitor.py:202 -- Monitor: Started```\nAm I doing something wrong in how I can combine these 2 (tasks and tune)?"}
{"question": "Is it usual to wrap a complex ray.remote call in a general try-except block to catch all exceptions? I'm doing this because I am making a lot of calls, doing a lot of work in the remote function and using a lot of machines. A single error will stop the entire job. It does work but I wonder if there's a neater way to handle errors"}
{"question": "How do i manage python env(packages) with ray cluster?"}
{"question": "I'm still trying to set up a Ray cluster with the cluster launcher on GCP. I can launch the cluster now, head node and workers. But when I submit a remote function, the work is only done one the head node. Looking at ray status shows that the workers reach status \"setting up\" and then fail, never becoming productive. Firewall should be OK, ports 22, 3389, 6379 and 8076 are open for TCP. use_internal_ips is set to true in the YAML. What's wrong with this setup?"}
{"question": "Hello, I got a question that can raysgd support distributed dataframes and distributed data loading like xgboost-ray?"}
{"question": "Hi -- I'm running into a problem getting Ray set-up. Ray requires that a lot of classes be altered to implement Serializable. This is fine if it's a custom class that I can edit. For Java classes, such as JarFile, I created wrapper classes that implement Serializable. However, I am running into a case that is stumping me a bit. I am getting the following error:\n```Caused by: java.lang.RuntimeException: Class java.util.WeakHashMap does not implement Serializable or externalizable                                       at org.nustaq.serialization.FSTClazzInfo.&lt;init&gt;(FSTClazzInfo.java:144)  \n\nat org.nustaq.serialization.FSTClazzInfoRegistry.getCLInfo(FSTClazzInfoRegistry.java:129)                                                               \n\nat org.nustaq.serialization.FSTObjectOutput.getFstClazzInfo(FSTObjectOutput.java:534)                                                                     \n\nat org.nustaq.serialization.FSTObjectOutput.writeObjectWithContext(FSTObjectOutput.java:416)                                                              \n\nat org.nustaq.serialization.FSTObjectOutput.writeObjectWithContext(FSTObjectOutput.java:369                                                                      \n\nat org.nustaq.serialization.FSTObjectOutput.writeObjectFields(FSTObjectOutput.java:664)                                                                    \n\nat org.nustaq.serialization.FSTObjectOutput.defaultWriteObject(FSTObjectOutput.java:546)\n\n...```\nI've created a wrapper class around WeakHashMap in my project. But the project relies on the ZipFile class ( a java class). I think the error is occurring due to this section in the constructor:\n```public ZipFile(File var1, int var2, Charset var3) throws IOException {\n    this.closeRequested = false;\n    this.streams = new WeakHashMap();```\nDo you have any suggestions for getting Ray to work when it's having issues with the Serializability of fields within Java classes?"}
{"question": "I tried out using `ray.init(include_dashboard=True` and get an error that dependencies are missing, even after I ran `pip install ray[default]`. It looks like it depends on `aiohttp.signals` which may have become `aiosignal.Signal` in <https://github.com/aio-libs/aiohttp/commit/7f0cd0d3bfbd3566b3035f670c8f351836c1ba08#diff-2f6aee0ecbf53b6b3760ba0a15c5cbc47898bdcdab1db521a63ad99a7bca3d98|https://github.com/aio-libs/aiohttp/commit/7f0cd0d3bfbd3566b3035f670c8f351836c1ba08#diff-2f6aee0ecbf53b6b3760ba0a1[\u2026]898bdcdab1db521a63ad99a7bca3d98> about a month ago? or am I trying to include the dashboard incorrectly? Thanks!"}
{"question": "Is it this again? <https://github.com/ray-project/ray/issues/19940|https://github.com/ray-project/ray/issues/19940>"}
{"question": "We\u2019re frequently getting this `OwnerDiedError` when returning an ObjectRef from an actor. We switched to using the pattern shown below of doing `ray.put()` and returning the result of that from an actor because we frequently ran into gRPC serialization errors that we couldn\u2019t identify the root cause of. Is there a way to keep the object alive until the client has retrieved it, or a better way to do this?\n\nError trace:\n```ray.exceptions.OwnerDiedError: Failed to retrieve object 1e9d04d3b7e4dfb27f4cbd73dd96fa155845c8260100000003000000. To see information about where this ObjectRef was created in Python, set the environment variable RAY_record_ref_creation_sites=1 during `ray start` and `ray.init()`.\n\nThe object's owner has exited. This is the Python worker that first created the ObjectRef via `.remote()` or `ray.put()`. Check cluster logs (`/tmp/ray/session_latest/logs/*ac133efbcddec161ce2b5b333722508592faeefbde494515a4927e70*` at IP address 20.10.42.227) for more information about the Python worker failure.```\nSample Code:\n```\n# Rough Actor method code:\ndef f():\n    # do work\n    ref = ray.put(work_result)\n    return ref\n\n# Rough client code:\n# create an actor\nref = actor.f.remote()\nresult_ref = ray.get(ref)\nresult = ray.get(result_ref)```"}
{"question": "Is there a way in RLlib to load new parameters into a policy after each rollout?"}
{"question": "`How to change EC2 Security group in ray up command ?`\n\nThe default security group is too broad   and  is quarantined :x: by our AWS environment.\nHow can specify `my own` aws security group <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/example-full.yaml|here> ?\n\ncc <@U02K9585Q8J>"}
{"question": "Hi all! I'm trying to use a Ray cluster (manually managed, no autoscalers used) to compute some function with different sets of arguments in a simple way like\u00a0`futures = [foo.remote(arg) for arg in args]`\u00a0. It often works like a charm, but sometimes (usually when\u00a0`len(args)`\u00a0is greater than the number of CPU cores available on the cluster) at some point computations stop with no obvious reason: CPU usage on nodes is ~0, and\u00a0`ray.wait(futures)`\u00a0 is stuck inside GRPC code waiting for new results. The dashboard doesn't help much: I see a bunch of workers with no CPU usage. If I kill all the nodes except the head one, Ray seems to become more stable, but I need more computational power than one node can offer. The whole cluster doesn't become completely stuck, however: if I submit new tasks, they are executed, and the old ones remain stuck. Any ideas on how to debug this?"}
{"question": "Hi guys,\nI\u2019ve been trying to implement Ray remote tasks to run in the background\nCurrent implementation: (Python)\n\t1. Initialize and connect Ray to an existing Cluster\n\t2. Execute the function through\u00a0`func.remote()`\n\t3. Get the job id by assigning the remote function to a variable:\u00a0`job_id = func.remote()`\n\t4. Save the job id to a Database - The job id is a class object (ClientObjectRef), so I pickled it and saved it to the database as a Memory View\n\t5. To get the status of the running task/job within the cluster, I have to pass the job id to\u00a0`ray.wait()`\n\nCurrent problem:\n\t1. The pickled(from database) ClientObjectRef seems to be invalid when being passed to\u00a0`ray.wait()`\u00a0function\n\t\t- Current suspicion is that the ClientObjectRef is having a session id or the like that is being used by the cluster to give information\n\nError log:\n```&lt;_InactiveRpcError of RPC that terminated with:\n        status = StatusCode.INTERNAL\n        details = \"Failed to serialize response!\"\n        debug_error_string = \"{\"created\":\"@1638193138.331058000\",\"description\":\"Error received from peer ipv4:35.185.220.144:10001\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1068,\"grpc_message\":\"Failed to serialize response!\",\"grpc_status\":13}\"\n&gt;```\nIs there a proper way to reach this goal with Ray?"}
{"question": "Hi, running `bazel run cluster_mode_test` in the `cpp` directory only runs the first test, and the process shuts down after that. Am I doing something wrong?"}
{"question": "hi all! does anyone know the design reasons why `ray.wait()` does not raise the remote task exception while `ray.get()` raises it?"}
{"question": "Is the integration between ray and wandb written by the wandb team or the ray team?\n`import ray.tune.integration.wandb`"}
{"question": "Hi, there's no way of using the autoscaler with a language other than python via the ray client (\"ray://\"), is there?"}
{"question": "I am getting this error. Not sure how I can fix it. Any ideas?\nUnable to connect to Redis at 127.0.0.1:6379."}
{"question": "Hi guys, how many ways can I use ray dataset to iterate data by column? Like to_modin(). Is there other ways?"}
{"question": "Hello, I\u2019m using ray dataset API to import data from GCS storage. I\u2019m not exactly sure how GCP should be configured on Ray cluster (we host it on GKE)\n```from gcsfs import GCSFileSystem\nray.init(\"<ray://127.0.0.1:10001>\", runtime_env = {\"pip\": [\"xgboost-ray==0.1.5\", \"modin==0.11.2\", \"gcsfs==2021.11.1\"]})\nfs = GCSFileSystem(project=\"ml-sketchbook\")\ndf = ray.data.read_csv(\"<gs://ml-sketchbook-keshi/ray/data/simpleHIGGS.csv>\", filesystem=fs)```\nAs the above code snippet shows, I\u2019m trying to read the data from `ml-sketchbook` GCP project, but my ray cluster is running on a different project but the head node and worker pods are configured with the workload identity and should be able to access the data in that project, but I\u2019m stilling getting the following errors:\n```ValueError: User-provided project 'ml-sketchbook' does not match the google default project 'ray-platform'. Either\n\n  1. Accept the google-default project by not passing a `project` to GCSFileSystem\n  2. Configure the default project to match the user-provided project (gcloud config set project)\n  3. Use an authorization method other than 'google_default' by providing 'token=...'```\nBut I run the same code locally it works for me\n```ray.init()\nfs = GCSFileSystem(project=\"ml-sketchbook\")\ndf = ray.data.read_csv(\"<gs://ml-sketchbook-keshi/ray/data/simpleHIGGS.csv>\", filesystem=fs)```\nAny suggestions on how to config gcloud on Ray cluster?"}
{"question": "Hi everyone...I am new to Ray so pardon my ignorance. I have a need to analyze stocks and I am using a for loop to iterate through the list of stocks (tickers). At the moment I can only loop through one ticker at a time however I would like to have parallel processing where I can send multiple tickers to the function that further analyzes the stock. What is the best way to accomplish this? I just need help with the loop at the moment. Thanks for your help."}
{"question": "Hi - I've been moving my dataset + tasks from development mode in my VS Code notebook, where I can saturate the 12 cores on my laptop and clearly see the dashboard meters pegging in the red zone, to workflow steps.  The dashboard seems to show high utilization, but it has lost all granularity.  Is this expected?"}
{"question": "ON *PB2*\nHelp please guys,\nIs PB2 only working with `metric=\"episode_reward_mean\"`?\nis `metric=\"mean_accuracy\"` not supported anymore?\n(like in ray v1.0.1 <https://docs.ray.io/en/ray-1.0.1/tune/examples/pb2_example.html>)\n\nWhen I use `metric=\"mean_accuracy\"` in PB2 with ray 1.6, I get next error:"}
{"question": "The default log level is Info. What is the way to set log level to `DEBUG` ?"}
{"question": "Is there a way to iterate through ray futures as they finish, rather than all at once (like `ray.get`)?"}
{"question": "hello, regarding the task resource declaration i.e. `ray.remote(num_cpus = 1)`, if one does not declare them, does ray logically attribute 1 or 0 cpus as being busy?\n\nDoesn't this matter for fractional compute types such as async actors?"}
{"question": "Hi,\n\u00a0\nI modified the xgboost example on Tune tutorial by replacing xgboost by ExtraTreeRegressor:\n```\u00a0import sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nfrom ray import tune\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score\n\ndef train_breast_cancer(config):\n    # Load dataset\n    data, labels = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    # Split into train and test set\n    train_x, test_x, train_y, test_y = train_test_split(data, labels, test_size=0.25)\n    predictor = ExtraTreesClassifier(**config)\n    predictor.fit(train_x, train_y)\n    pred = predictor.predict(train_x)\n    accuracy = accuracy_score(train_y, pred)\n    tune.report(mean_accuracy=accuracy, done=True)\n    \nconfig = {\n    'n_estimators': tune.randint(10,100),\n    'criterion': tune.choice([\"gini\", \"entropy\"]),\n    'max_depth': tune.randint(2, 32),\n    'min_samples_split': tune.randint(2, 7),\n    'min_samples_leaf': tune.choice([1,2]),\n    'max_features': tune.choice(['auto','sqrt','log2']),\n    'bootstrap': tune.choice([True, False])\n}\n\nanalysis = tune.run(\n         train_breast_cancer,\n         resources_per_trial={\"cpu\": 1},\n         config=config,\n         num_samples=10)```\n\u00a0\nAnd it does not work. The xgboost example works well on my windows laptop. I spent a day looking for what I did wrong without succeed. The message error I got is:\n```\u00a0---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n~\\Anaconda3\\lib\\site-packages\\tensorboardX\\record_writer.py in open_file(path)\n     57         prefix = path.split(':')[0]\n---&gt; 58         factory = REGISTERED_FACTORIES[prefix]\n     59         return factory.open(path)\n\nKeyError: 'C'\n\nDuring handling of the above exception, another exception occurred:\n\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-3-14fd4856f2cf&gt; in &lt;module&gt;\n----&gt; 1 analysis = tune.run(\n      2          train_breast_cancer,\n      3          resources_per_trial={\"cpu\": 1},\n      4          config=config,\n      5          num_samples=10)\n\n~\\Anaconda3\\lib\\site-packages\\ray\\tune\\tune.py in run(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, loggers, _remote)\n    599     progress_reporter.set_start_time(tune_start)\n    600     while not runner.is_finished() and not state[signal.SIGINT]:\n--&gt; 601         runner.step()\n    602         if has_verbosity(Verbosity.V1_EXPERIMENT):\n    603             _report_progress(runner, progress_reporter)\n\n~\\Anaconda3\\lib\\site-packages\\ray\\tune\\trial_runner.py in step(self)\n    687         may_handle_events = True\n    688         if next_trial is not None:\n--&gt; 689             if _start_trial(next_trial):\n    690                 may_handle_events = False\n    691             elif next_trial.status != Trial.ERROR:\n\n~\\Anaconda3\\lib\\site-packages\\ray\\tune\\trial_runner.py in _start_trial(trial)\n    678             with warn_if_slow(\"start_trial\"):\n    679                 if self.trial_executor.start_trial(trial):\n--&gt; 680                     self._callbacks.on_trial_start(\n    681                         iteration=self._iteration,\n    682                         trials=self._trials,\n\n~\\Anaconda3\\lib\\site-packages\\ray\\tune\\callback.py in on_trial_start(self, **info)\n    233     def on_trial_start(self, **info):\n    234         for callback in self._callbacks:\n--&gt; 235             callback.on_trial_start(**info)\n    236 \n    237     def on_trial_restore(self, **info):\n\n~\\Anaconda3\\lib\\site-packages\\ray\\tune\\logger.py in on_trial_start(self, iteration, trials, trial, **info)\n    405     def on_trial_start(self, iteration: int, trials: List[\"Trial\"],\n    406                        trial: \"Trial\", **info):\n--&gt; 407         self.log_trial_start(trial)\n    408 \n    409     def on_trial_restore(self, iteration: int, trials: List[\"Trial\"],\n\n~\\Anaconda3\\lib\\site-packages\\ray\\tune\\logger.py in log_trial_start(self, trial)\n    613             self._trial_writer[trial].close()\n    614         trial.init_logdir()\n--&gt; 615         self._trial_writer[trial] = self._summary_writer_cls(\n    616             trial.logdir, flush_secs=30)\n    617         self._trial_result[trial] = {}\n\n~\\Anaconda3\\lib\\site-packages\\tensorboardX\\writer.py in __init__(self, logdir, comment, purge_step, max_queue, flush_secs, filename_suffix, write_to_disk, log_dir, comet_config, **kwargs)\n    297         # and recreated later as needed.\n    298         self.file_writer = self.all_writers = None\n--&gt; 299         self._get_file_writer()\n    300 \n    301         # Initialize the Comet Logger\n\n~\\Anaconda3\\lib\\site-packages\\tensorboardX\\writer.py in _get_file_writer(self)\n    348 \n    349         if self.all_writers is None or self.file_writer is None:\n--&gt; 350             self.file_writer = FileWriter(logdir=self.logdir,\n    351                                           max_queue=self._max_queue,\n    352                                           flush_secs=self._flush_secs,\n\n~\\Anaconda3\\lib\\site-packages\\tensorboardX\\writer.py in __init__(self, logdir, max_queue, flush_secs, filename_suffix)\n    103         # actually the ones passing in a PosixPath\n    104         logdir = str(logdir)\n--&gt; 105         self.event_writer = EventFileWriter(\n    106             logdir, max_queue, flush_secs, filename_suffix)\n    107 \n\n~\\Anaconda3\\lib\\site-packages\\tensorboardX\\event_file_writer.py in __init__(self, logdir, max_queue_size, flush_secs, filename_suffix)\n    104         directory_check(self._logdir)\n    105         self._event_queue = multiprocessing.Queue(max_queue_size)\n--&gt; 106         self._ev_writer = EventsWriter(os.path.join(\n    107             self._logdir, \"events\"), filename_suffix)\n    108         self._flush_secs = flush_secs\n\n~\\Anaconda3\\lib\\site-packages\\tensorboardX\\event_file_writer.py in __init__(self, file_prefix, filename_suffix)\n     41             socket.gethostname() + filename_suffix\n     42         self._num_outstanding_events = 0\n---&gt; 43         self._py_recordio_writer = RecordWriter(self._file_name)\n     44         # Initialize an event instance.\n     45         self._event = event_pb2.Event()\n\n~\\Anaconda3\\lib\\site-packages\\tensorboardX\\record_writer.py in __init__(self, path)\n    174         self.path = path\n    175         self._writer = None\n--&gt; 176         self._writer = open_file(path)\n    177 \n    178     def write(self, data):\n\n~\\Anaconda3\\lib\\site-packages\\tensorboardX\\record_writer.py in open_file(path)\n     59         return factory.open(path)\n     60     except KeyError:\n---&gt; 61         return open(path, 'wb')\n     62 \n     63 \n\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\NTL.ARG\\\\ray_results\\\\train_breast_cancer_2021-12-06_10-19-51\\\\train_breast_cancer_aace3_00000_0_bootstrap=True,criterion=gini,max_depth=9,max_features=log2,min_samples_leaf=2,min_samples_split_2021-12-06_10-19-52\\\\events.out.tfevents.1638782392.ARGON2534'```\nDo you have any idea ? I cannot use the Tune sklearn adapter because I will make my code much more complicated."}
{"question": "Can someone please help me figure out what I am doing wrong here? I am trying to run parallel processing however it seems like there is limited parallel processing happening here. I have a list of about 11k stock symbols that I need to iterate through to get data for the last two years. I am scrolling through the list and then have a nested loop for each symbol to pull data for the last 2 years one day at a time. Here is the screenshot of the debug output I have. Here is my code. By the way, I am using Python.\n```\n\n@ray.remote\ndef get_trades(symbol, start_date, end_date):\n    try:\n\n        #s = stocks.StocksPolygon\n        client = polygon.StocksClient(stocks_key)\n\n\n        #request_date = date(2020, 01, 01)\n        #end_date = date(2021, 12, 04)\n        delta = timedelta(days=1)\n        #get_bars(symbol)\n\n        while start_date &lt;= end_date:\n            print(symbol, start_date)\n\n            resp = client.get_trades_vx(symbol=symbol, timestamp=start_date, sort=polygon.enums.SortOrder.ASCENDING ,order=polygon.enums.StocksTradesSort.TIMESTAMP, limit=50000)\n            #resp = s.get_trades(symbol=symbol, requestdate=requestdate, timestamp=timestamp, reverse=reverse, limit=limit)\n            #print('resp=')\n            #print(resp)\n            if resp == 'results':\n                df = None\n            else:\n                if 'results' in resp: \n                    df = pd.DataFrame.from_dict(resp['results'])\n                    #pprint.pprint(df[:5])\n                \n                    #if 'c' in df.columns:\n                    if len(df) &gt; 0:\n                        df = df[['conditions', 'id', 'price', 'sequence_number', 'size', 'sip_timestamp', 'exchange', 'participant_timestamp', 'tape']]\n                        df.columns = ['conditions', 'id', 'price', 'sequence_number',\n                                      'size', 'sip_timestamp', 'exchange', 'participant_timestamp', 'tape']\n                        df.insert(0, 'symbol', symbol)\n\n                        df['tdate'] = df['sip_timestamp'].map(lambda x: unix_convert.remote(x))\n                        df['save_date'] = datetime.utcnow()\n                        df.columns = ['symbol', 'conditions', 'id', 'price', 'sequence_number',\n                                      'size', 'sip_timestamp', 'exchange', 'participant_timestamp', 'tape', 'tdate', 'save_date']\n                    \n                        #ray.get(redis_message('stocksdata_hist_trades', df.to_dict()))\n                        #print(df.to_dict())\n                        df=None\n            start_date += delta\n        start_time2 = time.time()\n\n        \n        #print(time.time() - start_time2)\n    except Exception as exe:\n        print(exe)\n        traceback.print_exc()\n        time.sleep(10)\n        df = None\n\nray.init(ignore_reinit_error=True, num_cpus=6)\n@timebudget\ndef download_my_stocks(operation, input):\n    mydata = ray.get([operation.remote(ticker,start_date,date_end) for ticker in input]) \n    return mydata\n\nmydata=download_my_stocks(get_trades, tickers)\nray.get(mydata)\nray.shutdown()```"}
{"question": "Hi\u2026just a quick q \u2014 the cluster launcher yaml spec defines a\u00a0`head_node_type`\u00a0parameter, where i can specify which vm image / head node type will be used. However, i can\u2019t seem to find the equivalent for telling the system which image to use for the worker node type. Am I missing something?"}
{"question": "How much ram do raylets need? I am trying to run ray workers on machines with as little as 0.5GB ram, and I am getting tons of dead nodes."}
{"question": "I am unable to send messages via redis as it gives me an error about serialization. Can anyone please help me figure out how to send messages via redis? Here is my code to send redis message\n```\nr = redis.Redis(host=config.redis_host, port=config.redis_port, charset=\"utf-8\", decode_responses=True)\nfor message in messages:\n    r.rpush.remote(list, json.dumps(message))```\n"}
{"question": "Can I have multiple tasks running in different sections of the program and keep all the refs in a global result_refs?"}
{"question": "Is there any plans to optimize shared-memory treatment of protobuf messages, similar to numpy and simple types?"}
{"question": "Has anyone succeeded in using a sampling profiler like py-spy with ray in a locked-down HPC environment? I can't attach to a running process, even my own, under their security policy... but perhaps I can have ray invoke py-spy to launch the worker?"}
{"question": "In the tensorboard I have my custom reward as function of step. What should I do to plot my reward as function of iteration?"}
{"question": "Hiya, I\u2019m trying to use runtime_env with the Ray Job Submission sdk and my jobs keep failing on the setup runtime_env step.\n\n<https://docs.ray.io/en/latest/ray-job-submission/overview.html>\n\nI attempted to reproduce the Job Submission sdk example job in the above link and it failed even then.  I tried to reinstalling with\n`pip install \"ray[default]==1.9.0\"`\nand I\u2019m still getting the same error. Has anyone run into a similar issue when using runtime_env?"}
{"question": "How should I define custom RAY NN structure with LSTM cells and dropouts in tune.run?"}
{"question": "Hey everyone, I am trying to call a remote function from a remote function however I am not getting any values. I don't see any errors either. Below is the function that is being called and also the call to the function. Can someone please help and let me know what I am doing wrong? Thank you for your help.\n```\n@ray.remote\ndef unix_convert(ts):\n    if len(str(ts)) == 13:\n        ts = int(ts/1000)\n    else:\n        ts = int(ts/1000000000)\n    tdate = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S.%f')\n    return tdate\n\ndef main():\n.\n.\n.\n        \n    sip_timestamp = msg['sip_timestamp']\n    msg['tdate'] = ray.get(unix_convert.remote(sip_timestamp))```"}
{"question": "Hi, is someone knows how to kill these idle workers? After read some data from ray dataset, these idle workers take too much memories. Is there any elegant method that can kill them?"}
{"question": "Hi all, what's the current Apache Arrow dependency for Ray? A quick grep on requirements shows me pyarrow 4.0.1. Can that be upgraded to latest arrow?"}
{"question": "Hi everyone! I\u2019m using Ray with Kubernetes to manage an AKS cluster. I\u2019ve followed the documentation of deploying via Helm charts and everything worked out fine, however, there seem to be some limitations:\n\u2022 What is the best way of starting a ray tune experiment on the Kubernetes cluster? Currently, I have to ssh into the head node and execute `python main.py` \n\u2022 Autoscaling only seems to work for scaling up the number of pods. When an execution is finished the autoscaler doesn\u2019t seem to downscale (terminate pods)? Documentation on <https://docs.ray.io/en/latest/cluster/kubernetes-advanced.html#cleaning-up-resources|cleaning up resources> does it by terminating the cluster, but it is highly manual.\nAppreciate any help I can get, and thank you for the effort in providing an excellent framework!"}
{"question": "Hi, I have a `partial`  instance that i would like to run remotely. But I get the error: `ray.remote decorator must be applied to either a function or to a class` . I don't really understand why it doesn't allow arbitrary callables?\n\n<https://github.com/ray-project/ray/issues/8105|Other people> apparently ran across this, but it remains unresolved. My workaround for now is a general-use function that just calls its first argument - which works."}
{"question": "Hello, question, I am running a large job which creates lots of tasks, and what happens is my head node disk space gets cramed up and my head node dieas. Namely in the overlay filesystem, as the new tasks pop up and as the cluster scales up, the disk space slowly gets filled. Currently it uses over 100 gb disk space. What is the reason for this?"}
{"question": "Ray is crashing when I use it with scikit-learn. I am running into the ``unsupported pickle protocol`` issue in this ticket from last year which still seems to be open <https://github.com/ray-project/ray/issues/11547>\n\nIs there a workaround for this issue other than setting n_jobs to 1? That restriction would be a tremendous setback for my project"}
{"question": "Hi, I am using rllib with `isaacgym` simulator, but I faced some runtime resource bottleneck. In short, `isaacgym` could run multi-robot in the same time. So I use one rollout-worker to run an `isaacgym` with 400 cartpoles in parallel, but it seems the rollout-worker forces the `isaacgym` to run on one core only (which is really slow). I also tried to specify the \"`num_cpus_per_worker`=12\", but the `isaacgym` still uses one core only. In contrast, if I just run the `isaacgym` simulator directly, it can use multi-process by default. Is it because the rollout-worker is an Actor which forces all internal member-calls to run in a single process (correct me if I am wrong)? If not, how should I tackle this bottleneck? Thanks in advance."}
{"question": "Hi, why isn't huge_pages part of the possible arguments in ray.init? its implemented all over the code, but not exposed outside"}
{"question": "Hi all, I recently switched from ppo to qmix and videos aren\u2019t being recorded now. Is this a qmix issue or is it due to now using agent grouping?"}
{"question": "Hi everyone! I\u2019m using Ray with Kubernetes to run on Azure. I can access the cluster using the `kubectl` and `ray.init(\"localhost:10001\")`. and submit tasks. But when I include `tune` in the script, the ray init command starts a local ray cluster on my laptop.\nWhat I am missing in running the ray tune task on the azure cluster?"}
{"question": "Does anyone know how to set *Autoscaler (python)* log level to *DEBUG*?"}
{"question": "Hi, I am getting the following error in the Azure deployment:\n`raylet <http://agent_manager.cc:237|agent_manager.cc:237>: Failed to delete URIs, status = IOError: , maybe there are some network problems, will retry it later`\n\nAny ideas what can be the cause of it?"}
{"question": "Hi, I am working with 2 packages which I want to use in my investigation. Pkg A is included in the `init` using the `working_dir` key in `runtime_env` like\n```runtime_env={\n\u00a0 \u00a0 \"working_dir\": \"../src\",\n},```\nFor Pkg B, I had already installed it as editable in the local env as `pip install -e B`\nPkg B is used within Pkg A as one of the dependencies. When I try to run the notebook cell which contains A where Pkg B is imported then it executes fine. But the cell execution fails when I try to list the output of the function which contains the above submitted code. It returns the following error:\n```ModuleNotFoundError: No module named 'B' ```\nCan anyone point out what I am doing wrong in this?"}
{"question": "Hi, I am having issues setting env variables for my ray cluster.  What is the best way of doing this?\n\nIdeally I would be able to pull them into my workflow using os.environ[\u2018MY_VAR\u2019] .  I have tried setting the variables in setup commands with an export MY_VAR=var, as well as using a script to export the variables with the command source var_script.sh.  This works locally but when I try to use the cluster launcher my script fails because the variables are not set."}
{"question": "Does anyone have compared the lightgbm and lightgbm on ray?\nI found with same data and almost result, the property 'feature_importances_' was quite different. Here is my result"}
{"question": "Hi, i am trying out `pipeline` as described here: <https://docs.ray.io/en/latest/serve/pipeline.html>\nBut torch is unable to register any cuda devices. How can i declare `num_gpus` like in deployment decorators?"}
{"question": "I want to operate this function. Specifically, parallelly operate each class's function in a regular time basis(5seconds). every time interval, main function check the condition of the ray actor object of the class(taking a rest? or have not finished the task before 1 time interval). And if the object is not taking a rest, then not operating the function. if the object is taking a rest, operating the function."}
{"question": "Could you guys give some advice about it? And I couldn't find how to operate the function by using the object references. (which can be obtained from the function \"wait\")"}
{"question": "Has anyone used Ray to loop through a list of stock symbols and do technical analysis in parallel? (in python) Right now I am using pandas_ta and tried to use multithreading but each loop takes about 3 secs and a list of 50 stocks at 5 time intervals takes more than 10 mins which is not acceptable.  I would like to run the technical analysis in parallel to get the complete result in seconds rather than mins. Any help would be much appreciated."}
{"question": "are there ways to see the objects in the object stores?  I wanna know more about how object &lt;-&gt; object-store works\n```# python 3.7.12\nimport ray\nprint(ray.__version__)  # 1.9.1\nds = ray.data.range(10000)\n# 2021-12-25 17:55:44,620\tINFO services.py:1340 -- View the Ray dashboard at <http://127.0.0.1:8265>```\nalso, the UI memory tab is always loading, am I miss something?"}
{"question": "Hi, how does Ray differentiate between a task returning a single object, and multiple objects...? Specifically, what is the API to specify this...?"}
{"question": "Hi guys, are you confident in using an IDE for debugging ray code? I have tried to use pyCharm and Wing IDE but when I put a breakpoint in a 'forward' method of a custom model, the debugger does not stop at the breakpoint:"}
{"question": "What is keep_checkpoints_num in tune.run?"}
{"question": "Hello, where can I find the code for `ray.init()` in the githib repo? thank you"}
{"question": "GPU support in Ray\n<https://docs.ray.io/en/latest/using-ray-with-gpus.html>\nFollowing the link above, usage of GPU can be controlled by the parameter num_gpus.\n`ray.init(num_gpus=N)`\nHowever, on GPU machine, ray is automatically detecting the GPUs on the server and using them. To turn down the usage, I had to explicitly use\n`os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"`\nDo I need to use this every time ?\nOr should it be like :\u00a0`ray.init(num_gpus=0)` "}
{"question": "Hi everyone! I\u2019ve been using ray for the last little while and it\u2019s great!  Does anyone here use spot instances on AWS a lot? I\u2019ve noticed that when a particular type of worker is unavailable because the spot price is higher than the bid in the ray config, ray does not understand this constraint (and ideally try a different type of machine) but instead continues to retry that same constrained machine type forever. Is this a common use case and is this a known issue? I see the feature request at  <https://github.com/ray-project/ray/issues/20774> and the associated code changes, but those seem to be only a change to the reporting infra and not the actual scheduling infrastructure\u2026"}
{"question": "Anyone run into this error before? I have a Ray Host setup and running. I am trying to connect a worker node to the Host. I get the same error if I try on Windows or Ubuntu. I'm using ray version 1.9.1 &amp; python 3.8.10 (through conda)\n```2021-12-28 15:19:45,423 ERROR node.py:1286 -- ERROR as &lt;_InactiveRpcError of RPC that terminated with:\n        status = StatusCode.UNAVAILABLE\n        details = \"failed to connect to all addresses\"\n        debug_error_string = \"{\"created\":\"@1640726385.422430902\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3134,\"referenced_errors\":[{\"created\":\"@1640726385.422428339\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/lib/transport/error_utils.cc\",\"file_line\":163,\"grpc_status\":14}]}\"```"}
{"question": "The issue seemed to be firewall related. Once I disabled the firewall, the Host and the Worker were able to connect.\n\nI'd rather not leave the firewall disabled, but I can't find the documentation for the ports required by Ray. Here is what I have set:\n\nDashboard: `8265`\nWorker connection: `6379`\nClient connection: `10001`\n\nAnyone know what I am missing?"}
{"question": "Hi all, is there a good way to send large Arrow tables to a ML model with `ray-serve`?\nI'm thinking of how to best serve a ML model via some HTTP server and Apache Arrow + Pandas seems to me like the best way to guarantee a consistent table schema while keeping performance high."}
{"question": "How does one sync data from a worker node to the head node?"}
{"question": "Hi all I'm new here,\nfinding some problems with examples (cup paste from <http://44.228.130.106/serve/index.html#ray-serve-quickstart|here>) in version *2.0.0.dev0*\n\nHere the problem:\n\n*AttributeError: 'Client' object has no attribute 'create_backend'*\n\nI guess I'm not the first,\ndoes anyone have any suggestion?"}
{"question": "Do actor method calls execute concurrently or serially? That is, if I do this:\n```my_actor = Actor.remote(...)\nmy_actor.foo.remote()\nmy_actor.bar.remote()```\nThen will `my_actor.bar` wait for `my_actor.foo` to finish executing, or will they run in parallel? For my case I want the latter (serial) behavior."}
{"question": "Hi everyone, attempting to launch a Ray job and monitor it from a separate Python process via Python multiprocessing. I find that the ray.get call just hangs when I pass in the ClientObjRef that I am sharing via Manager.dict() between the Python process where the Ray job is launched and the monitor Python process. Question -&gt; is it because I am pickling the ClientObjectRef between Python processes?"}
{"question": "Hi everyone. I am using google colab. my problem is that Ray finds the GPU but does not use it. I have attached 4 photos named a1,a2,a3 and a4 to explain more.\na1 photo is when I use only CPU and it is normal. a2 and a3 are using 0.5 and 1 GPU respectively.\nCan anyone please explain me the dashboard status? am I correct that the Ray finds the GPU?\nmy code is on a4, which is very simple. I appreciate any guide to use the colab GPU."}
{"question": "Hi everyone, is there a documentation on how Ray namespace partitioning is done in K8s? At which level is a Ray actor name unique? Is it unique per k8sNamespace/RayNamespace? Is the same RayNamespace accessible in different K8sNamespace, etc."}
{"question": "Hi everyone, I have a Ray cluster deployed on Azure K8s. I  connect to it using `kubectl` command as given in the documentation.\nWhen I am trying to run a task on the cluster, It throws the following error:\n&gt; Put failed:\n&gt; ---------------------------------------------------------------------------\n&gt; ModuleNotFoundError                       Traceback (most recent call last)\n&gt; &lt;ipython-input-9-bd8872eca93e&gt; in &lt;module&gt;\n&gt;      16     'combined_dfs': combined_dfs_ray,\n&gt;      17 }\n&gt; ---&gt; 18 run_validation = validate_source.remote(config)\n&gt; \n&gt; ModuleNotFoundError: No module named 'sklearn'\nBut sklearn is available in the local conda env. Also, for the algorithm we are running, we are using `runtime_env` option while running the `ray.init` command to make available our custom code. All the dependencies are installed in the conda env.\n\nWhat could be the reason for this error? What is the best way to make sure the local environment is available in the cluster as well?"}
{"question": "Hi, regarding, the log4j vulnerability. I can't upgrade one of my projects easily to the latest ray as another component uses an older redis version. How does log4j affect my python project. I see this comment \"If you need a very short term fix right now, you can remove the `ray_dist.jar` from the wheel (it is not used by Ray unless you are using Ray Java bindings) like this (for example for macOS):\" from (<https://github.com/ray-project/ray/issues/21055>)\n\u2022 When does the python ray library use log4j?\n\u2022 If we don't use macOS, does that mean we are safe and the log4j library is not used?"}
{"question": "How much does Ray Train transform how PyTorch Datasets, Dataloaders and generated batches behave? I'm getting a\n```TypeError: 'generator' object is not subscriptable```\non my PyTorch mini-batches when I convert my Pytorch code to Ray Train\n\n<https://discuss.ray.io/t/ray-train-creates-typeerror-generator-object-is-not-subscriptable/4605>"}
{"question": "Hi, I wonder if we can use GPU custom environment (simulation needs GPU) to use RLLIB (if possible, is there any examples)? My environment is a fluid simulator and requires GPU for interacting. I tried to follow the custom_environment.py script provided in the docs, but my backend environment keeps warning me that the simulator cannot detect a GPU device, which looks something like :\n``` CUDA error: code=100(cudaErrorNoDevice) \"cudaMalloc((void **)&amp;data_gpu.nodeIdx, sizeof(int))\"```\nI confirm that I can directly initialize the environment in the example like:\n```    #for sanity check\n    def env_creator(env_name):\n        if env_name == 'CustomEnv-v0':\n            from gym_fish.envs.fish_env_cruising import KoiCruisingEnv as env\n        else:\n            raise NotImplementedError\n        return env\n\n    env = env_creator('CustomEnv-v0')\n\n    tmpenv=env() #no error, everything works fine, simulator initialized with GPU (as this is the wat we call a normal gym environment)```\nHowever, if I follow the example practice to register the environment, it gives the above error:\n```    def env_creator(env_name):\n        if env_name == 'CustomEnv-v0':\n            from gym_fish.envs.fish_env_cruising import KoiCruisingEnv as env\n        else:\n            raise NotImplementedError\n        return env\n\n    env = env_creator('CustomEnv-v0')\n\n    tune.register_env(\"myEnv\", lambda cfg: env()) \n    trainer = ppo.PPOTrainer(config=ppo_config, env=\"myEnv\") #this is problematic, and give the error.```\nThe detailed error calling stack can be found <https://gist.github.com/PeppaCat/5f01987ce205f1814fe99062c0d71d4a|here>."}
{"question": "I have a PyTorch Ray Train script that hangs - 7min idle on 4*T4 instance while same script takes 2min to run without Ray Train. I don't get what's happening, nothing gets logged after\n\n```(BaseWorkerMixin pid=20816) 2022-01-07 15:24:57,369\tINFO torch.py:239 -- Moving model to device: cuda:0\n(BaseWorkerMixin pid=20816) 2022-01-07 15:25:06,322\tINFO torch.py:242 -- Wrapping provided model in DDP.```\nIs there a more verbose mode?\n\nI added a logging.getLogger().setLevel(logging.DEBUG) in the script, but Ray Train is still silent after copying the model on 1 out of the 4 cards\n\nalso asked here <https://discuss.ray.io/t/ray-train-silent-for-7-min/4610>"}
{"question": "Hi, is there in Ray Model dropout definition?"}
{"question": "Hi all, Happy New Year!\n\nIs there a way to prevent workload running on head node?\n\nThanks,"}
{"question": "I am working on a project where we call\u00a0`trainer.run`\u00a0and pass dataframe as one of the config parameters. The following error shows up after some random period of training. Which means everything seems to work, trains for 2-3 epochs and then returns the error. A note here is that for the data we use Modin dataframe, but the error also appears if we remove modin and use instead plain pandas dataframe instead. I am looking for suggestions of what can cause this type of error and how to possibly solve it?\n```The autoscaler failed with the following error:\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/monitor.py\", line 423, in run\n    self._run()\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/monitor.py\", line 311, in _run\n    self.update_load_metrics()\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/monitor.py\", line 232, in update_load_metrics\n    request, timeout=60)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/grpc/_channel.py\", line 923, in __call__\n    return _end_unary_response_blocking(state, call, False, None)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/grpc/_channel.py\", line 826, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)\ngrpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n        status = StatusCode.DEADLINE_EXCEEDED\n        details = \"Deadline Exceeded\"\n        debug_error_string = \"{\"created\":\"@1641674485.444519145\",\"description\":\"Deadline Exceeded\",\"file\":\"src/core/ext/filters/deadline/deadline_filter.cc\",\"file_line\":81,\"grpc_status\":4}\"\n&gt;```"}
{"question": "Hello, I have a question about `ray.remote(max_retries)`\n\nDoes this help if I'm using spot instances? The docs say \u201cThis specifies the maximum number of times that the remote function should be rerun when the worker process executing it crashes unexpectedly.\u201d, but what if the whole worker instance dies due to a spot interrupt? Does Ray handle retrying the function on another worker?"}
{"question": "Should the Ray team invest more in supporting streaming workloads?"}
{"question": "Does RLLib support an environment that requires using GPU for simulation? It seems the RLLib environment register wrapper will mess up the instantiation of a GPU environment (simulator)."}
{"question": "Hi there, hope you guys had a great weekend. For the Ray Helm Chart, is there any way to expose the IP of the ray head service externally just through the helm chart itself? We're hoping to not have to go to the terminal whatsoever and have this preconfigured (i.e. not have to do `kubectl edit` or `kubectl port-forward`)"}
{"question": "I don't manage to print PyTorch losses when wrapping my PyTorch code in Ray Train. Is there something specific to do? <https://discuss.ray.io/t/how-to-get-pytorch-losses-from-ray-train/4659>"}
{"question": "Hi all, I was wondering if there is a reason why the ray head service (when deploying on k8s) is of type clusteip rather than nodeport? Exposing a nodeport service to external traffic is more convenient for engineers as native cloud ingress controllers / load balancers will work automatically. This is not the case with clusterip; in fact on gcp the native ingress / http(s) load balancer does not support clusterip at all"}
{"question": "I am trying to change the Python version from 3.9.5 to <http://3.9.in|3.9.7 in> the Dockerfile. I've tried running the docker builder scripts in the repo, but didn't have much luck.\n\nSpecifically getting this error in the ray-deps and ray image building steps:\n``` &gt; [3/4] COPY .whl .whl:\n------\nfailed to compute cache key: \"/.whl\" not found: not found```\nHas anyone used the script before?"}
{"question": "Hi! I'm trying to use the Ray Cluster Launcher for an automatically managed cluster with on-premise machines. I've been using the example cluster.yaml for local setups as a template: <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/local/example-full.yaml> . I want to put as much of my setup in a docker container as possible so setup commands don't have to run on each node every time I launch a new application. Is it possible to configure cluster.yaml to use a custom docker container instead of a publicly hosted one? The custom container's dockerfile would have a `FROM rayproject/ray:latest-cpu` declaration and would run some pip install commands on python wheels uploaded to file_mounts."}
{"question": "Hi!\n\nI want to use the ray-ml image to use with the autoscaler. It works nicely, but my beef with it is that it\u2019s just too huge! However, trying to build a lighter image from scratch I fail miserably at the point where the head node isn\u2019t able to connect to the worker nodes, so I assume there is some configuration in the ray image that I\u2019m missing\u2026 Does anyone have a solution for building a lightweight docker image which works on the cluster?"}
{"question": "Out of curiosity, a general question regarding Slack as ray's discussion channel: have you already checked out [Zulip](<https://zulip.com/>)? This is a really nice open source alternative to Slack and can also be self-hosted. (I maintain a Zulip instance for thousands of students at a large german university and it works pretty nice :slightly_smiling_face:)"}
{"question": "Hi, this time a ray-related question :slightly_smiling_face: <@U02SC1PQTHC> and I are trying to adapt this benchmark (<https://github.com/stephanie-wang/ownership-nsdi2021-artifact/blob/main/recovery-microbenchmark/reconstruction.py>) to use actors. Although our implementation works fine sometimes, it sometimes gives us the following error message. I wonder whether someone could give us a hint about this error?\n```Started local cluster on 192.168.178.30:6379\nAdded node 192.168.178.30:44787\n2022-01-15 18:58:22,291\tINFO worker.py:842 -- Connecting to existing Ray cluster at address: 192.168.178.30:6379\nAll nodes joined\nRunning 1000 rounds of 10 ms each\nKilling 192.168.178.30 after 5.0s\nRestarted node at 192.168.178.30:44787\n2022-01-15 18:58:57,079\tWARNING worker.py:1245 -- The node with node id: 84c1297cb1305137149386c3878cfc8c445228d52d55df235f292a4d and ip: 192.168.178.30 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a raylet crashes unexpectedly or has lagging heartbeats.\n^CTraceback (most recent call last):\n  File \"/home/robert/dev/tum/sys-seminar/ray_persistent_actor/./benchmark_persistent_actor.py\", line 197, in &lt;module&gt;\n    main()\n  File \"/home/robert/dev/tum/sys-seminar/ray_persistent_actor/./benchmark_persistent_actor.py\", line 192, in main\n    benchmark(args, restart_worker=lambda k: restart_worker(cluster, k))\n  File \"/home/robert/dev/tum/sys-seminar/ray_persistent_actor/./benchmark_persistent_actor.py\", line 109, in benchmark\n    duration = run(args.delay_ms, args.large, args.safe_state, num_rounds)\n  File \"/home/robert/dev/tum/sys-seminar/ray_persistent_actor/./benchmark_persistent_actor.py\", line 163, in run\n    ray.get(dep)\n  File \"/home/robert/.local/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/robert/.local/lib/python3.9/site-packages/ray/worker.py\", line 1706, in get\n    values, debugger_breakpoint = worker.get_objects(\n  File \"/home/robert/.local/lib/python3.9/site-packages/ray/worker.py\", line 353, in get_objects\n    data_metadata_pairs = self.core_worker.get_objects(\n  File \"python/ray/_raylet.pyx\", line 1163, in ray._raylet.CoreWorker.get_objects\n  File \"python/ray/_raylet.pyx\", line 154, in ray._raylet.check_status\nKeyboardInterrupt\n2022-01-15 18:59:16,962\tERROR import_thread.py:89 -- ImportThread: Connection closed by server.\n2022-01-15 18:59:16,962\tERROR worker.py:478 -- print_logs: Connection closed by server.\n2022-01-15 18:59:16,962\tERROR worker.py:1247 -- listen_error_messages_raylet: Connection closed by server.```"}
{"question": "deleting Ray objects...does anyone have an example of how to ensure Ray deletes an object from the Plasma object once the program using that object is finished with it?"}
{"question": "hello All/<@U02K9585Q8J>,\nI am new to Ray and interested in multigpu multinode HPO on a local cluster using SLURM.\nI have an issue where clients via python API can connect to the head node from within a job when they are launched without srun, but when with srun (as NERSC documentation suggestion for multiple worker nodes), the connection fails. Any ideas/suggestions?"}
{"question": "Question: why does ray.put return a ClientObjectRef and code executed on ray via ray.remote returns an ObjectRef?"}
{"question": "Hello, I'm curious what is the protocol Ray uses between client and remote cluster? the docs say `ray://` ?"}
{"question": "Hi, it looks like <http://docs.ray.io|docs.ray.io> is showing 1.9.2, it used to be 2.0.0 dev I think. Some contents are missing, e.g., runtime_env container support, where can I find them?"}
{"question": "Hey All,\nDoes anyone has an idea on the following:  if increasing number of workers in PPO config (ray[rllib]) always results in decrease in the time taken during the training process ?"}
{"question": "Can somoene please take a look at my code and let me know what I am doing wrong? This code used to run pretty fast unfortunately I made some changes that I don't recall what they were that have slowed down the process.\n\nThis process uses WebSockets to fetch historical stock market data and enter it into a postgres database. Prior to using Ray, this process took about 10 hours and with Ray this process took about an hour. This process is no longer running that fast.\n\nWhile you may not be able to run this code, perhaps pointing out certain things that may look wrong would be much appreciated. I have spent a lot of time trying to fix this script but no luck so far.\n\nThank you for your help in advance.\n```\n\nimport polygon\nfrom polygon import enums, cred\nimport pandas as pds\nimport modin.pandas as pd\nfrom modin.db_conn import ModinDatabaseConnection\n#from sqlalchemy import create_engine\nimport datetime\nfrom datetime import datetime, timezone\nimport time\n\nfrom time import time\nimport json\nimport threading\nimport config\nimport traceback\nimport requests\nimport re\nimport redis\nimport pprint\nfrom datetime import date, datetime, timedelta\nimport multiprocessing as mp\nimport numpy as np\nimport ray\nfrom timebudget import timebudget\nimport concurrent.futures\n#from sys import getrefcount\nimport numpy\nimport asyncio\n\nstart_time = time()\nray.init(ignore_reinit_error=True, num_cpus=3)\n\nresult_refs = []\nmax_tasks1 = 250\nmax_tasks2 = 500\n\nprint(\"starting options agg m hist...\")\noptions_key = config.polygon_key_options\n#stocks_key = config.polygon_key_stocks\nstocks_key = cred.KEY\n\n\n\n\n\nsymbols_df = pd.read_csv('options_1206-1221-21.csv') # ('select ticker from companies where active = true order by ticker', con=con)\n#symbols_df.to_csv('symbols.csv')\ntickers = symbols_df['symbol'].to_list()\nprint(tickers)\n\n\nstart_date = date(2021, 12, 6)\n\ndate_end = date.today()\ndate_end = date(int(date_end.strftime(\"%Y\")), int(date_end.strftime(\"%m\")), int(date_end.strftime(\"%d\")))\n\n\n@ray.remote\ndef unix_convert(ts):\n    if len(str(ts)) == 13:\n        ts = int(ts/1000)\n    else:\n        ts = int(ts/1000000000)\n    tdate = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S.%f')\n    return tdate\n\n\n#@timebudget\n@ray.remote\ndef get_bars(symbol, start_date, end_date,  multiplier=1, timespan='minute'):\n    try:\n        global result_refs\n        sub_results = []\n\n        client = polygon.OptionsClient(options_key)\n\n        symbol_poly = polygon.convert_from_tda_to_polygon_format(symbol)\n\n        delta = timedelta(days=1)\n\n\n        resp = client.get_aggregate_bars(symbol=symbol_poly, from_date=start_date, to_date=date_end, adjusted=True, sort=polygon.enums.SortOrder.ASCENDING, limit=50000, multiplier=multiplier, timespan=timespan)\n\n\n        \n        if resp == 'results':\n            df = None\n        else:\n            if 'results' in resp: \n                for i in range(len(resp['results'])-1):\n\n                    #if 'a' in df.columns:\n                    if len(resp['results']) &gt; 0:\n\n                        msg = resp['results'][i]\n                        msg['symbol'] = symbol_poly\n                        msg['frequency'] = str(multiplier) + timespan\n                        msg['volume'] = msg.pop('v')\n                        msg['vwap'] = msg.pop('vw')\n                        msg['open'] = msg.pop('o')\n                        msg['close'] = msg.pop('c')\n                        msg['high'] = msg.pop('h')\n                        msg['low'] = msg.pop('l')\n                        msg['timestamp'] = msg.pop('t')\n                        apple = msg.pop('a', None)\n                        op = msg.pop('op', None)\n                        nada = msg.pop('n', None)\n                        \n                        ts = msg['timestamp'] \n                        if len(str(ts)) == 13:\n                            ts = int(ts/1000)\n                        else:\n                            ts = int(ts/1000000000)\n                        tdate = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S.%f')\n\n                        msg['tdate'] =  tdate #msg['t'].map(lambda x: unix_convert(x))\n\n                        msg['save_date'] = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')\n\n\n                        asyncio.run(process_resp(symbol, msg))\n\n\n            else:\n                df=None\n\n\n    except Exception as exe:\n        print(exe)\n        traceback.print_exc()\n\nasync def process_resp(symbol, resp):\n    \n    global result_refs\n    try:\n        \n\n        df = pds.DataFrame([resp])\n\n        \n        df.to_sql('optionsdata_hist_bars', config.psql, if_exists='append', index=False)\n\n                \n\n    except Exception as exe:\n        print(exe)\n        traceback.print_exc()\n\n\n@timebudget\ndef download_my_options(operation, input):\n    global result_refs\n\n    for ticker in input:\n        if len(result_refs) &gt; max_tasks1:\n            ray.wait(result_refs, num_returns=max_tasks1)\n        result_refs.append(operation.remote(ticker,start_date,date_end))\n        \n        \n\n\ndownload_my_options(get_bars, tickers)\n\nray.shutdown()\n\nprint(time() - start_time)```\n"}
{"question": "Hello, I have two quick questions.\nWhen requesting resources for tasks, does Ray enforce that tasks only use the requested resources? I believe it does not and that it's up to the programmer to ensure these are accurate. Is this correct? I'm thinking of cases where the programmer requests fractional CPU, but the task is CPU intensive.\nAlso, given the distributed nature of the Ray scheduler, is it feasible to believe that the same program, executed multiple times, will result in the same scheduling/placement of tasks? Again, I think not, but I want to make sure.\nThank you!"}
{"question": "is there an api to list all actors?"}
{"question": "Quick question about fault-tolerance with Ray actors: is there a way to detect and trap when an actor cannot be created, for instance with _memory-aware scheduling_ if there's not enough memory available on any worker?  This seems to create a condition where we're forced to restart Ray -- here's a more detailed description <https://discuss.ray.io/t/how-to-detect-when-creating-an-actor-fails/4765>"}
{"question": "Hi all! I\u2019m trying to pool my python function with Ray however im facing an error.. I suspect it\u2019s due to the PYTHONPATH of the custom Python module (custom_module) that\u2019s being used in the function.. When i use the multiprocessing package instead of ray, it works just fine! Does anyone have guidance on this?\n```#033[2m#033[36m(PoolActor pid=220)#033[0m ModuleNotFoundError: No module named 'custom_module'```\nstack trace:\n```traceback: Traceback (most recent call last):\n  File \"/miniconda3/lib/python3.7/site-packages/ray/serialization.py\", line 281, in deserialize_objects\n    obj = self._deserialize_object(data, metadata, object_ref)\n  File \"/miniconda3/lib/python3.7/site-packages/ray/serialization.py\", line 194, in _deserialize_object\n    return self._deserialize_msgpack_data(data, metadata_fields)\n  File \"/miniconda3/lib/python3.7/site-packages/ray/serialization.py\", line 172, in _deserialize_msgpack_data\n    python_objects = self._deserialize_pickle5_data(pickle5_data)\n  File \"/miniconda3/lib/python3.7/site-packages/ray/serialization.py\", line 160, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band, buffers=buffers)```"}
{"question": "It seems that when using _placement groups_ with actors, it's possible to create more actors than there are bundles. Is this a known behavior?"}
{"question": "Hi almighty All! We trying to collect logs from different `Ray Workflows` and using Redis for fetching it. Not too bad, but have some issues:\n\u2022 if logs not fetched right at the moment workflow is running - they gone from Redis (I suspect Ray itself fetches it and stores at FS)\n\u2022 maybe better then access logs at FS than Redis? (the question is that we plan. to use it in highly distributed system, and likely best way to use Ray Workflows API for such a purpose?\n\u2022 probably my fault, but setting log level when starting ray `ray --logging-level debug start --dashboard-host 0.0.0.0 --head` nor in workflow task (by Python\u2019s `logging.basicConfig(level=logging.DEBUG)` ) has no effect. Most likely I miss something.\nIf it is a PR/branch in progress - I\u2019ll be happy to test it or to help with making it working.\nPlease help with the right direction, how is the best way to work with `Workflow Logs` in Ray ecosystem."}
{"question": "Hi, guys. Will CUDA threads also be restricted (divided) when specifying `gpu_num=0.x` for a worker (I understand it divides the GPU memory )? My use case is that I got a simulator that is written in c++, and it will try to invoke all CUDA *threads* for calculation. In this case, I can not get parallelism with one GPU, as parallel processes will sequentially quest all the CUDA threads and then return to the next process. I wonder if replacing the process with `ray worker` would help increase parallelism in this case.\nAny suggestions or comments are welcomed. Thanks in advance."}
{"question": "Hi there. I had a question regarding workflow considerations for submitting jobs to ray on k8s. The documentation suggests running a client pod that calls a ray cluster to submit a job via the ray client API. How is this functionally different from ssh-ing in to the head node and running the application? Is there any benefit in spinning up this extra pod to submit a job?"}
{"question": "Is there any way get the worker id (or node id / task id) within a remote task?\n```@ray.remote\ndef f():\n  print(&lt;worker id for this task&gt;)```"}
{"question": "Can we start ray remote worker with a `spawn` context?\nThe module I am working with could be called with multiprocessing like this(following <https://pytorch.org/docs/stable/notes/multiprocessing.html>):\n```context=mp.get_context('spawn')#NOTE: without spawn context will fail as it uses CUDA!\n<http://self.ps|self.ps> = context.Process(target=worker, args=(...))```\nAnd I wonder if there's any equivalence of it in ray. There's a similar <https://github.com/ray-project/ray/issues/13568|issue> but I cannot figure out the right way to do this.\nI tried to directly specifying `mp.set_start_method('spawn')`  in the `main()` , however it does not work."}
{"question": "I want to implement a distributed SVD I read about in the literature, and ultimately run it in a distributed fashion on Ray. It's most useful if I have deferred loading, such that I can store my array in chunks and load them in the individual workers. Is the best solution to just use dask with dask-on-ray?"}
{"question": "Hello everyone, I am Romain and I am using Ray on large HPC clusters (1k nodes and more). Would you have some information about expected Ray performance with respect to the number of workers (`ray start ...`) in the cluster?\n\nI found this link: <https://github.com/ray-project/ray/blob/master/benchmarks/README.md>\n(Ray Scalability Envelope)\n\nDoes it mean that we are limited by about 250 nodes in the cluster?"}
{"question": "For <http://Ray.io|Ray.io> in a cluster environment, is there a way to get historical logs ? Will logs be spread across worker nodes?"}
{"question": "Hi, is it possible to run a Task with a different docker image than the worker? Imagining something like `ray.remote(runtime_env={\"docker_image\": \"python:3.9\"})`  \u2026?"}
{"question": "Can you not touch the state of an actor?\n```class State:\n    def __init__(self, a, b, c, d, e):\n        self.a = a\n        self.b = b\n        self.c = c\n        self.d = d\n        self.e = e\n\nrState = ray.remote(State)\nrs = rState.remote(a,b,c,d,e)\nrs.a\n&gt;&gt;&gt; AttributeError: 'ActorHandle' object has no attribute 'a'```"}
{"question": "Hi. I am have a challenge with using Dask with Ray Clustaer and Ray as a scheduler.  I have a custom resource assigned to Ray Cluster head/worker. I am using Dask with Ray scheduler and I want dask-with-ray to respect custom resources assigned to each head/worker.\nI am aware that there is something like:\u00a0`dask.annotate`\u00a0to specify custom resources for the job, but this seems not to work with Ray Cluster resources.\nThere is a Ray docs section\u00a0<https://docs.ray.io/en/latest/advanced.html?highlight=dynamic%20remote%20parameters#dynamic-remote-parameters%60|Advanced Usage \u2014 Ray v1.9.2>\u00a0on resource options, but I can\u2019t make it work either with Dask and Ray.\nCan anyone drop some hint where can I find some information on how to put some resource constraints/requirements on specific Dask operations that are scheduled buy Ray?"}
{"question": "Hello. I\u2019m using Ray tune with pytorch lightning. Is there a way to get Ray to allow the PL progress bar to render correctly on the console? Instead of staying in one place and increasing the percent, it keeps scrolling. Thanks"}
{"question": "sup folks, i'm evaluating ray and running into a problem with leaking both workers and actors for this simple code:\n```import ray\n\n@ray.remote\ndef foo():\n    return ray.data.range_arrow(10000, parallelism = 1)\n\nray.init(address=\"auto\")\nref = foo.remote()\nprint(ray.get(ref))\nray.shutdown()```\ni'm wondering if this is expected -- ie. what am i doing wrong here?  eventually, the cluster will run out of memory after spamming me ad nauseum about redundant exports of remote functions, actors, etc."}
{"question": "I'm following the instructions <https://docs.ray.io/en/latest/cluster/cloud.html#cluster-cloud|here> to launch a cluster on Azure but I'm getting an error (see reply in thread please). It looks like azure cli related issue? Any help is highly appreciated"}
{"question": "Hi.\nI am a beginner of 'rllib and 'robosuite'. Is it possible to apply \u2018rllib\u2019 to \u2018robosuite\u2019 in the same way as applying \u2018rllib\u2019 to \u2018gym\u2019?"}
{"question": "Hi.\nIs it possible to run multiple workers on a single GPU with DD-PPO?"}
{"question": "Hi.\nI am using an industrial 7-axis robot.\nI am interested in 'Visual reinforcement  learning' using 'VAE', 'AE' etc. and 'Hierarchical reinforcement learning' .\n\nDoes anyone have good examples?"}
{"question": "I\u2019m seeing this error on my job:\n\n```ERROR worker.py:432 -- SystemExit was raised from the worker. The worker may have exceeded K8s pod memory limits.```\nmy platform team thinks it\u2019s ray\u2019s issue. Is this a ray issue and how should I fix?"}
{"question": "Hi all! We're looking into Ray as a distributed backend for the full data science pipeline. For data loading, modelling, deployment, etc I see integrations with relevant libraries to solve the needs.\n\nOne question remaining is about visualization of distributed data. Many plots for large datasets (histograms, kernel density estimates, bar-charts, etc) are based on a first summarization of the data and then a computationally inexpensive rendering of that summary.\n\nDoes anyone know of any good plotting libraries that utilise the power of the distributed data + compute of Ray, e.g. by under-the-hood computing the summaries (or even rasterized layers in the visualization), and then just render them on the client?"}
{"question": "Hello :slightly_smiling_face:\nWe in Outbrain have an interesting use-case where we evaluate ray integration. During that evaluation, we facing some errors, and technical questions. My question is whom can I schedule a introduction session, to discuss our use-case, and get support if needed?\nThanks"}
{"question": "Hello everyone is there a ray  pip package for arm 64 ? If not is there any know issues regarding the compilation of ray for this arch ?"}
{"question": "I see that each ray worker node stores logs under the\u00a0`/tmp/ray/session_xxx/logs`  directory. Is there any particular info/mark from a worker if it is done with producing logs?"}
{"question": "Hey guys, I really love the framework and it made my app a killer but unfortunately, it cannot work well when Actor creation on worker nodes take few minutes. It successfully creates Actor on head node in seconds but in worker it hangs for a few minutes in pending creation state. I'm using official Ray Docker images, latest build, python 3.7. Tried switching servers so making head the other one and it's the same, worker node still is slow. Would anyone be able to assist me with it? Here is link to forum topic: <https://discuss.ray.io/t/creation-an-actor-on-worker-nodes-takes-long-time/4942>"}
{"question": "Hi all, quick question: any idea how to gracefully shut down the connection after invoking `ray.get(handle.remote(request))` ? Right now I\u2019m getting `WARNING long_poll.py:138 -- LongPollClient connection failed, shutting down.`"}
{"question": "Hello, for dataset `map` API, I am wondering how this is sharded across actors ? What is default numbers of actors used for that ?"}
{"question": "Hi, all, I am new to Ray and I have a question: suppose there are two nodes, A and B, A has a variable x = 10 and B has a variable y = 20, how to write very simple code to calculate x + y?"}
{"question": "Hi everyone, I'm trying to reduce the frequency at which the status logs are being printed to the terminal. Would anyone know how to do this please? Posted the question on the forum: <https://discuss.ray.io/t/remove-status-logs/4993>. Thank you :)"}
{"question": "Hi everyone. I want to launch a cluster with an existing EC2 instance as the head node. From the documentation, it looks like `ray start` does not allow autoscaling and requires to manually create the worker nodes, is it possible to add autoscaling in this case?"}
{"question": "Hi! Any Ray Workflows developers here? I\u2019m in the process of <https://github.com/ray-project/ray/issues/22200|submitting a PR to fix a Workflows bug>, but could use a tiny bit of help (detalied in the linked gh issue). Any workflow devs want to talk asyncio threading?"}
{"question": "Can Ray Dataset help me read S3 pictures into a PyTorch dataloader? PyTorch is a bit weak for this use-case.. also asked here <https://discuss.ray.io/t/can-ray-dataset-be-used-between-s3-and-pytorch/4998>"}
{"question": "Hi everyone!\nSomebody knows how to continue training of a RLLib agent (PPO i.e) that was not trained enough?\nWhen I say \u201cto continue\u201d I mean to start a second round of training from the last checkpoint of the first training round\nThanks in advance for any comment"}
{"question": "is there a roadmap for python-3.9 support?"}
{"question": "Hello,\nI'm trying to set up Ray on AWS but I have a problem with tags. In my case all instances should have a few tags otherwise they are terminated. The problem is with TTL. I set it up via TagSpecifications but Ray changes it with stage/state information what results in termination. How can I change this behavior in Ray (disable changes of TTL tag or add some prefix???) "}
{"question": "Hi everyone!\nAfter launching the ray named actor with python, I want to call the actor from the another cpp process.\nHowever, there is no \"namespace\" argument for cpp only.\nSo, is it possible to call python ray named actor from cpp?\nAnd I wonder if there could be another problem besides the namespace problem."}
{"question": "Is there any  benchmark published with Ray and Tensorflow? There is one between pytorch and Ray <https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead|here>."}
{"question": "With Ray Actors, Is it possible to have an actor system with some actors written in Python, and others in C++ and have them call each others' methods?"}
{"question": "Hi Anyscale team, for whenever people wake up: any chance an engineer / pm / human would be available for a ~30m-1h chat around a Ray Serve implementation sometime between 9 and 11 AM PST this morning? Thanks in advance!"}
{"question": "Hi.\n\nMy ray computations are held in Docker container roughly in the following scenario.\n\nOn the container start the ray cluster is initialized, this container waits for incoming tasks and when one appears, separate process is started which sends this new task to the already initialized cluster.\n\nIf container is set up locally, no problems occur and all computations are done as expected. However, if it comes to deploying container with Kubernetes there might be timeout problems (I restrict `ray.get` with timeout of 300 seconds which should be more that enough to compute).\n\nIt is worth noticing that somethimes the computation in Kubernetes may be finished and this might happen on \u2014 if I understand it correctly \u2014 new pod creation and thus some artifacts are required to be downloaded from AWS.\n\nCould you, please, provide any insights on where to look at to start solving this problem? What could be the reason for such huge behavoiral difference dependent on Kubernetes?"}
{"question": "do I need all checkpoints for TensorBoard visualization or is only json file enough for visualization in TensorBoard?"}
{"question": "In many places in the ray documentation, the ray address is set as:\n\n```export RAY_ADDRESS=\"<http://127.0.0.1:8265>\"```\nfor example, in the job submission AI: <https://docs.ray.io/en/latest/ray-job-submission/overview.html>\n\nI had to reset the cluster after submitting a job to port `8265` which was being used for the dashboard. Is there something I am missing here or is it the case of the documentation needing to be updated?\n\nI know this port for job submission is configurable but should this not be set to 10001 in the documentation?"}
{"question": "hi everyone, i'm trying to get ray set up and am having some problems with running ray start on another node manually. is this the right channel to ask?"}
{"question": "I have another question. The cluster has been running now and I'm trying to use the ray multiprocessing module to replace the standard library multiprocessing which I had been using in my code. It seems like the cluster is only every being used at around 33%. Is this a setting or a limit?\n\nI initialized the Pool() with no parameters which from the documentation says it should use all the CPUs available in the cluster. Am I missing something in the config to force it to use more CPUs?"}
{"question": "Hi all, I have a question about how and when to read data when using `ray.tune`\n\n*Background*\n\u2022 I have a dataset containing eight weeks of data. It is O(10Gb) in size.\n\u2022 I want to train and validate on sliding windows:\n    \u25e6 Train on (W1, W2), Test on (W3, W4)\n    \u25e6 Train on (W2, W3), Test on (W4, W5)\n    \u25e6 etc etc\n\u2022 For each split, I want to perform grid search over O(100) hyperparameters\n\u2022 I\u2019ll then take the hyperparameters that give the best mean test score over all windows\nI am assuming that ray.tune will be important here, so that I\u2019ll do something like:\n```analysis = tune.run(\n    train_one_model,\n    verbose=False,\n    config={\n        \"max_depth\": tune.grid_search([2, 4, 8]),\n        \"min_child_weight\": tune.grid_search([0, 0.01, 0.1])\n    }\n)```\n*My question*\nHow and when should data be read?\n\n*Some observations*\n1. As a first stab, I could read the dataset in inside the body of\u00a0`train_one_model` :\n```def train_one_model(configs):\n    df = load_data(directory)\n    train = df[df[\"week_index\"].isin((0, 1))]\n    test = df[df[\"week_index\"].isin((2, 3))]```\nThis seems to be\u00a0<https://docs.ray.io/en/latest/tune/tutorials/tune-xgboost.html#tuning-the-configuration-parameters|how some of the examples do it>. However, this feels like a lots of unnecessary IO. I could save a bit of work by splitting the data on disc. But assuming I have O(10) workers and O(100) parameterisations, then each worker will still end up reading the same thing over and over.\n\n2. I could use\u00a0<https://docs.ray.io/en/latest/tune/api_docs/trainable.html#tune-with-parameters|tune.with_parameters>\u00a0. The example in the docs does look like what I want:\n```from ray import tune\n\ndef train(config, data=None):\n    for sample in data:\n        loss = update_model(sample)\n        tune.report(loss=loss)\n\ndata = HugeDataset(download=True)\n\ntune.run(\n    tune.with_parameters(train, data=data),\n    # ...\n)```\nBy analogy with Spark, this feels weird - you wouldn\u2019t typically read several gigs onto the driver and broadcast it out to your executors. But Ray isn\u2019t Spark, and I don\u2019t have a good mental model of Ray. Maybe in the context of Ray this pattern makes sense?\n\n3. Is there a way to configure this so that the first time\u00a0`train_one_model`\u00a0gets called on a worker, it reads the data into that worker\u2019s memory. Then in subsequent runs the data is already sitting there? Something like:\n```def train_one_model(configs):\n    if not df:\n        df = load_data(directory)\n    train = df[df[\"week_index\"].isin((0, 1))]\n    test = df[df[\"week_index\"].isin((2, 3))]```\nCan this be achieved via\u00a0`ray.put`\u00a0and\u00a0`ray.get`? Would this be fundamentally different / better / worse than option 2?"}
{"question": "Hey all,\nI'm developing a tool which allows users to run their own custom plugins which uses Ray as a remote backend. I'm trying to improve the developer experience for plugin writers so that they can easily test that their code is working.\n\nThe runtime_env recommendations have been very helpful:\n<https://docs.ray.io/en/latest/handling-dependencies.html#library-development>\n\nThe issue that I'm having right now is that I want to use a runtime environment and set \"py_modules\" to the module containing the developer's code, and install any extra dependencies (possibly using the \"pip\" key in `runtime_env`, or one of the other mechanisms), but the Ray cluster already has 99% of the required dependencies, so I don't want to have to specify and reinstall them all in the runtime environment. Is there a way to have the `runtime_env` virtualenv inherit the base environment on the cluster node, and just allow the user to specify \"extra dependencies\" during development?"}
{"question": "Hey all, on Kubernetes if I had one Ray operator and 3 Ray clusters that were running some jobs, what would happen if I restart the operator? Would something go wrong with the Ray clusters?"}
{"question": "Hi\n\nI am trying to install Pytorch on Ray cluster(on existing Kubernetes) via Runtime env argument in Ray init API.  I have tried passing requirement as list and also list of libraries. In both cases it fails to install pytorch\n\n```RuntimeError: Failed to create runtime_env for Ray client server: Failed to install conda environment /tmp/ray/session_2022-02-17_09-20-23_032570_114/runtime_resources/conda/0a9159fa710dc9e2a910941ab254c693f883ba44:\nCollecting torch==1.10.2\nDownloading torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n\nfailed\nPip subprocess error:\n/tmp/ray/session_2022-02-17_09-20-23_032570_114/runtime_resources/conda/0a9159fa710dc9e2a910941ab254c693f883ba44/.tmp280w_4l4: line 3:  4401 Killed                  /tmp/ray/session_2022-02-17_09-20-23_032570_114/runtime_resources/conda/0a9159fa710dc9e2a910941ab254c693f883ba44/bin/python -m pip install -U -r /tmp/ray/session_2022-02-17_09-20-23_032570_114/runtime_resources/conda/condaenv.n8hbjjnu.requirements.txt```\nFor linux machine,  following is the usual way to install pytorch but I am unable to do the same using list of requirements to runtime env or adding following line to requirements.txt\n```pip3 install torch==1.10.2+cpu -f <https://download.pytorch.org/whl/cpu/torch_stable.html>```\nI tried to get rid of torch from requirements.txt and created a custom Ray image by adding torch installation on there but that doesn't seem to solve the issue. I get `ModuleNotFoundError` if I dont add the torch to requirements.txt.\n\nFinally, I set \"eager_install\": False flag in runtime_env expecting it to not fail because dependecies are not downloaded and installed when ray.init() is call. But I still get `ModuleNotFoundError`\n\nI am using Ray 1.10.0\n\nAny thoughts on how to proceed here?"}
{"question": "when using dask-on-ray I see\u00a0`_wrapped`\u00a0processes on ray head node, is it normal?\nI frequently get my head node OOM and I'm guessing this might be the cause."}
{"question": "Hi all, I have a question regarding the deployment of ray with optuna on SLURM. I followed this tutorial:\n<https://docs.ray.io/en/latest/cluster/slurm.html?highlight=slurm#python-interface-slurm-scripts>\n\nI understand I can run some workloads distributed now. My question now is how to use ray.tune. Am I supposed to start hyperparam optimization on the head node and the work will be sent automatically to worker nodes?"}
{"question": "Hello, is it possible to run cluster with a local head node and workers from AWS? and if so, can someone share yaml example? thanks"}
{"question": "using dask, why do the\u00a0`ray::wrapped()`\u00a0 processes be placed in ray head node?"}
{"question": "I'm using manually started up nodes to connect to a local cluster. is there a configuration / option / startup switch that I can use to pick which disk to use? the main OS disk is pretty full but i have some additional drives on the systems which may be used. i can't find anywhere in the documentation where I can tell Ray which disk to use?"}
{"question": "I'm pretty sure this was the cause at `xgboost_ray` for *OOM in ray head node*\n*<https://github.com/ray-project/xgboost_ray/issues/195>*\nanyone available for xgboost_ray pr review?"}
{"question": "I am just starting to work with <https://github.com/ray-project/xgboost_ray> and am running the first training example in the readme.\nI have a system with 4 GPUs. If I understand correctly then i should set the `RayParams`  `num_actors=4` and `gpus_per_actor=1`.\nI have experimented with `cpus_per_actor` (setting it anywhere from 0 to 8 ), following the instructions in the readme (see 4th screenshot attachment).\n\nI can see the GPUs inside Python and can use them with other Python libraries (see third screen shot):\nMy GPUs do not get used and the CPUs race along merrily (seemingly as a single actor) regardless of how I modify the parameters.\n\nWhat have I configured wrongly - there must be something very basic staring at me in the face?!"}
{"question": "Hi there,\n\nI would like to know if there is a performance difference if actors beeing defined in one python file and deployed once\u00a0 or in several python files deployed seperatlely?\n\n*1\u00a0 All in one\u00a0 (deploy them all together to the cluster)*\n```@serve.deployment\ndef model_one(data):\n    print(\"Model 1 called with data \", data)\n    return random()\n@serve.deployment\ndef model_two(data):\n    print(\"Model 2 called with data \", data)\n    return data\n@serve.deployment(route_prefix=\"/composed\")\nclass ComposedModel:\n    def __init__(self):\n        self.model_one = model_one.get_handle()\n        self.model_two = model_two.get_handle()\n\n    def __call__(self, starlette_request):\n        data = starlette_request.body()\n        pred = self.model_one.predict(data)\n        pred2 = self.model_two.predict(data)\n        return pred, pred2\n    \nserve.start(detached=True)\nmodel_one.deploy()\nmodel_two.deploy()\nComposedModel.deploy()```\n\n\n\nVS\n\n\n\n*2\u00a0 one per file\u00a0 (deploy them seperately to the cluster)*\nmodel1.py\n```@serve.deployment\ndef model_one(data, name=\"model1\"):\n    print(\"Model 1 called with data \", data)\n    return random()\nserve.start(detached=True)\nmodel_one.deploy()```\n\nmodel2.py\n```@serve.deployment\ndef model_two(data, name=\"model2\"):\n    print(\"Model 2 called with data \", data)\n    return data\nserve.start(detached=True)\nmodel_two.deploy()```\n\nmodel_comp.py\n```@serve.deployment(route_prefix=\"/composed\")\nclass ComposedModel:\n    def __init__(self):\n        serve_client = serve.start()\n        self.model_one = serve_client.get_handle(\"model1\")\n        self.model_two = serve_client.get_handle(\"model2\")\n\n    def __call__(self, starlette_request):\n        data = starlette_request.body()\n        pred = self.model_one.predict(data)\n        pred2 = self.model_two.predict(data)\n        return pred, pred2\n    \nserve.start(detached=True)\nComposedModel.deploy()```\n\nmore precisely is there a difference in using get handle within the python process going over a global variable like in case  1\n\nbetween case 2  where a python process would need to go over network to get the handle?"}
{"question": "i'm wondering if i'm deploying code incorrectly. i have my package with some modules and currently have ray set up to replace multiprocessing. i have a manually spun up cluster of ray nodes which will take and execute the code. however, when the code runs on the cluster, it requires the package to be installed in the python environment where ray is running from.\n\nwhere my question and problem lies is when i make changes to the code. every time i make some change to the package or reference methods, the nodes will fail to execute until i deploy and install the newer version of the code. this is fine when i am deploying, but when i'm only looking to do some small change to debug or one off changes, it seems cumbersome.\n\nam i missing something here or using ray inappropriately for my use case?"}
{"question": "Hi team!\nI'm  trying to debug my Ray actor, but I don't know how to do.\nI'm using PyCharm, but the ray actor does not appear in the attachable process.\nHow can I debug ray actors?"}
{"question": "Hey Guys!\nI am trying to initialize ray on WSL-2 but cannot access the dashboard. I tried logging and found this\n`Process STDOUT and STDERR is being redirected to /tmp/ray/session_2022-02-23_10-46-35_417595_21205/logs` any idea how to fix this?"}
{"question": "Hi all, I see on the documentation that the version 2.0.0 is incoming (<https://docs.ray.io/en/master/>). Does anyone know where I can find the current changelog or an overview of the update? This would be to help foresee any non-compatible changes for my project that uses RLlib."}
{"question": "Can I connect to a remote ray cluster using ingress??? I.e https or <ray://route:443|ray://route:443>. The docs only say to use an IP address of the node where the head node is running."}
{"question": "One qq -- I only see 1 process ID (PID) for the training which suggest its only using one of the 15 executors to train.  Am I understanding this correctly?  It should have 15 pids if 15 executors are used for training?   Below is the code we're using.\n```trainer1 = TorchTrainer(\n    training_operator_cls=CustomTrainingOperator,\n    num_workers=4,\n    use_gpu=False,\n    wrap_ddp = True,\n    num_cpus_per_worker = 5, # changed from 8 to 5 since we are using 20 cpus\n    use_tqdm=True,\n    config={\"batch_size\": 64}\n)```"}
{"question": "Hey folks, I am using <https://docs.ray.io/en/latest/train/train.html|Ray Train>. I would like to override the default log directory to a gcs path. The gcs path is not posix compliant so `DEFAULT_RESULTS_DIR` is appended to the gcs path, `/root/ray_results/gs:/ml-staging-adhoc/tmp/starscream/`. Is there a way around this issue?"}
{"question": "Hey! I'm seeing this error a lot (multiple times per second) on my ray cluster:\n`(raylet, ip=172.22.20.239) E0225 10:53:03.120203984    2766 <http://fork_posix.cc:76]|fork_posix.cc:76]>           Other threads are currently calling into gRPC, skipping fork() handlers`\nThis is on ray 1.10, running on Ubuntu LTS 16.04.7. Tasks also seem to be hanging when this happens. Any ideas what might be causing this?"}
{"question": "I am looking to use ray remote. This works ok in standalone poc. I want to be able to deploy this on fargate, where supervisor or head runs as single task and worker can autoscale based on the load, I am not able to find any documentation to be able to make this kind of deployment. Can anyone help?"}
{"question": "Hi. I have a folder containing image files, and I want to use Ray\u2019s dataset pipeline feature, to load the files and then feed them to a (PyTorch) model. I read the docs but it is not clear to me which \u201cdatasource\u201d method I need to use in order to load the images. \nCan anyone help?"}
{"question": "Hi everyone!\nIs there a tutorial somewhere in Ray Tune on how to create custom HPO methods?"}
{"question": "Hi, I am trying to find more information on `xxx.partd` files that are created by Ray in `/tmp` folder. These folders are created in each run and accumulate over time creating unnecessary storage blockage. Unfortunately I can't find anything on it in docs or on github. Are these files spilled objects? Shared memory? Can I somehow mange them or clean them up automatically? Appreciate any hints"}
{"question": "Hey, it seems <https://discuss.ray.io/t/error-when-installing-dependencies-for-worker-nodes/5221?u=tian_sang|a bug> in Ray 1.10 causes `pip install` in runtime_env  doesn\u2019t honor pip options such as `--index-url`? Is there any workaround without downgrading Ray version on our cluster?"}
{"question": "Hi, everyone here.\n\nI'm just setting up to use Ray. I want to configure a cluster on the GCP.\n\nI created a Head node and Worker nodes created through `ray up-y cluster-config.yml`. After that, when trying to http communication with another instance on the same VPC(e.g. `requests.get(\"<https://api.mydomain.com>\")`), an error called 'Name or service not knowing' occurs. Addresses such as `<http://google.com|google.com>` are communicated well.\n\nI think I can fix `/etc/resolv.conf` or `/etc/hosts` well, but has anyone experienced or solved the same problem as me?\nHelp me plz.\n\nThank you :pray:"}
{"question": "The google cloud autoscaler automatically downloads a funny ray version `ray-2.0.0.dev0` <https://github.com/ray-project/ray/blame/db0c16824c46bf7b7b098845d6ed9cfabca15815/python/ray/autoscaler/gcp/defaults.yaml#L133> which seems like a convenient way to setup cluster with latest version, however it doesn't work for communication with 1.10 :slightly_smiling_face: -- pickle load can't find some classes. What is recommended worflow:\n\u2022 a) setup head node manually \n\u2022 b) update the pip install line?"}
{"question": "Hi, I have a question about loading a trained xgboost model. I would like to load it to do inference / prediction in parallel with Ray. Questions:\n\u2022 do I have to use Ray's xgboost lib to load the model?\n\u2022 I tried to load it with standard xgboost. Somehow only one worker is running and the rest of them are idle.\n    \u25e6 I had trouble using ray.put(my_xgboost_model) as it crashes with `cloudpickle`. So I simply pass `my_xgboost_model` to my remote function"}
{"question": "has anyone had issues running ray worker pods on preemptible GKE/k8s nodes? for some reason everything works fine for me with non preemptible workers. I've checked node selector configs and whatnot"}
{"question": "Hey all, quick question for 1.10.0- is there a python API for managing launching and shutting down clusters? IE in the docs (<https://docs.ray.io/en/latest/cluster/quickstart.html#launch-a-cluster-on-a-cloud-provider>) I see the example CLI\n\n```ray up -y config.yaml\nray submit config.yaml script.py\nray down -y config.yaml```\nI was wondering if there's equivalent functionality that can be called from python natively, without having to dip in and out of shell scripts. The reason being the computations are all going to be called from Airflow in our work, and it'd just be cleaner to wrap the submit call in a context manager that can automatically handle cluster management as opposed to having three individual tasks. Thanks!"}
{"question": "hey team, is there a way for me to increase the size for `runtime_env. working_dir`  ? it currently set limit to 100mb"}
{"question": "Hey all, after a preliminary evaluation of Ray it seems like it could be a significant upgrade for my current application setup(many python processes doing work in parallel across k8 pods). But my application is highly dependent on an embedded key value store that is constantly being synced between pods. Looking for more info on the object store, there was some good info in the Ray 1.x <https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview#heading=h.rtepjltxoeb7|whitepaper>, but I haven\u2019t found any info on if its advisable to use it similar to an embedded kv store(immutability concerns/best practices for updates/maxsize/production hardened/etc)  and how exactly synchronization happens(if not local, what is the cost of pulling from elsewhere?). apologizes if this is too vague a question or if this info is hiding in plain sight in the docs somewhere. any info or resource docs would be much appreciated, cheers"}
{"question": "hey all, is there an easy way/example to make the actor pool auto-scale asynchronously ?"}
{"question": "Hey! First off, I just started using Ray and I\u2019m loving it \u2013 thank you for creating such a powerful tool! I have some questions about using 1st-party code in Ray cloud clusters. I\u2019m using the <https://docs.ray.io/en/latest/ray-job-submission/overview.html|Ray Job\u2019s API> to submit Ray scripts to a multi-node cluster in GCP. I\u2019ve created a customer Docker image that layers on some additional 3rd-party packages and adds some of my 1st-party code to the Python path to make it useable in tasks / actors. I\u2019m using the Cluster Launcher\u2019s `docker` support to use this image on my cluster. I also start updated the `head_start_ray_commands` and `worker_start_ray_commands` to add my 1st-party code to the Python path during startup. *First question:* what\u2019s the proper way to make 1st-party code available when running Ray in `docker` on a cloud cluster? I\u2019ve pretty much resorted to updating `PYTHONPATH` everywhere I can think of (doing it _only_ in the Docker image doesn\u2019t seem to propagate to Ray workers), but I\u2019d love to find out the recommended way to do this (`py_modules` in the `runtime_env` isn\u2019t a great option for me \u2013 my 1st-party modules are huge, and I\u2019d prefer to put them in a pre-built customer Docker image). *Second question*: When I submit a ray script through the Jobs API (I\u2019m guessing this ends up being roughly equivalent to rsyncing the script to the head node and running it there) that connects to the cluster using `ray.init(address=\"auto\")`, my 1st-party code cannot be imported: I get `No module named my_module` errors. But when I update the script to connect with `ray.init(address=\"<ray://127.0.0.1:10001>\")`, my 1st-party code _can_ be imported and everything works. Is there a different between the two ways of connecting? Can someone enlighten me on why the latter can import my 1st-party code and the former doesn\u2019t? Thanks in advance :slightly_smiling_face: if full configs and more details would be useful, let me know and I can include them in the thread!"}
{"question": "Hello, everyone :slightly_smiling_face:\n\nCan I run job on only worker nodes, not the head nodes?"}
{"question": "Hi, may I check if it is possible to use rllib PPO with the SHap library? Or some way that can make it work?"}
{"question": "Hi all, I am trying to run this Ray Lightning example on a Ray cluster, from a jupyter notebook (the VM instance where the notebook runs is not part of the cluster):\n\n<https://medium.com/pytorch/getting-started-with-ray-lightning-easy-multi-node-pytorch-lightning-training-e639031aff8b>\n\nWhen I run the cell that contains this piece of code:\n\n```# Initialize ray\nray.shutdown()\nray.init(address=\"<ray://10.128.0.5:10001>\")\n\n# Instantiate model\nmodel = LightningMNISTClassifier(config, data_dir=\"./\")\n\n# Create Trainer and start training\ntrainer = pl.Trainer(\n    max_epochs=10, \n    plugins=[RayPlugin(num_workers=num_workers, use_gpu=use_gpu)])\ntrainer.fit(model)```\nIt downloads the MNIST data set to the VM instance where the notebook is running; not to a worker node in the cluster.\n\nIf I save that cell as a python file and run it with `ray submit`, it works fine... but we would like to run it directly in the notebook, not using ray submit. Any ideas?"}
{"question": "Is there a way to kill/cancel all pending ray jobs that are pending due to insufficient resources? I know I can kill the jobs IF i have the job ID, but I don't so, other than killing the complete Head node, what are my options?"}
{"question": "Hello, can someone point me to the code where ray detects worker resource ids? Trying to figure out why ray doesn't detect `GPU` on my node."}
{"question": "Hey all, I'm looking to deploy Ray Cluster on Azure, and am running into issues with the fact that my codebase depends on python 3.7 while the env setup on the VMs appears to be 3.8. Specifically, cloudpickle evidently can't unpickle the packages serialized using 3.7 and I'm hitting\n\n```2022-03-07 20:14:39,172\tERROR worker.py:409 -- Failed to deserialize b\"\\x80\\x05\\x95H\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08builtins\\x94\\x8c\\tTypeError\\x94\\x93\\x94\\x8c'an integer is required (got type bytes)\\x94\\x85\\x94R\\x94.\"\nTraceback (most recent call last):\n  File \"/Users/wgoldenberg/afresh-core/af-env/lib/python3.7/site-packages/ray/util/client/worker.py\", line 407, in _put_pickled\n    raise cloudpickle.loads(resp.error)\nTypeError: an integer is required (got type bytes)\n2022-03-07 20:14:39,564\tINFO util.py:282 -- setting max workers for head node type to 0 ```\nIs the Azure Cluster not supported for 3.7, or has anyone here found a workaround?"}
{"question": "Hi, I just got the released note for Ray v1.11.0.\nI have a question about the Highlight message: `Ray no longer starts Redis by default. Ray metadata previously stored in Redis are stored in Ray GCS now.`\n\nIs it means Ray GCS no longer implementing on Redis? If true, does any new technical white paper or references we can read?\n\n<https://github.com/ray-project/ray/releases/tag/ray-1.11.0>"}
{"question": "Is there a way to pass custom headers when creating a gRPC channel in Ray? Seems that gRPC supports it, but Ray does not wrap it. Thanks!"}
{"question": "A follow up question about this setup. I see that checkpoint_callback and logger should be false. Does this mean *all* callbacks? like an EarlyStopping callback or the LearningRateMonitor?\n\nAlso, with no checkpointing I am not sure how I will compare epochs.\n\nAnd a comment: kind of a bummer to turn off all logging since this precludes using TensorBoard to do human sanity checks on what's going on with my fit.\n\nUPDATE:\nusing this config:\n\n```plugin = RayPlugin(num_workers=10, cpus_per_worker=4, use_gpu=False)\ntrainer = pl.Trainer(\n        strategy=[plugin],\n        callbacks=[early_stop_callback, checkpoint_callback],\n        logger=False\n      )```\nI am writing checkpoints locally (on my laptop). Does this mean I am not using my cloud cluster?\n\nUPDATE 2:\nchanging 'strategy' to 'plugins' seems to fix my problem even tho I get warning that 'plugins' has been deprecated. I am at least running this thru the ray cluster now"}
{"question": "Hi, so I see Ray 1.11.0 was just released. Congrats! The highlight is \u201cRay no longer starts Redis by default. Cluster metadata previously stored in Redis is stored in the GCS now.\u201c. What is the impact of moving Cluster metadata from Redis to GCS? I\u2019m not that familiar but I am curious. Feel free to point me to docs too."}
{"question": "In addition to <@U02T4STETE1> question, I'm wondering what is the reason to move from Redis to GCS?"}
{"question": "Hello everybody. My HPC administrator allows me to install ray only via SPACK (<https://spack.readthedocs.io/en/latest/index.html>) but won't give me further instructions...\nDo you have any idea how to do it? The platform is IBM ppc64le\nSorry for the (probably noob) question"}
{"question": "What's the process around deciding which commits make it into the ray release (or not)? There's an autoscaler fix merged into master prior to the release of 1.10 which didn't make it into 1.11 - the branch structure between master and 1.11 is quite complex (master isn't ahead of ray-1.11.0 and ray-1.11.0 isn't ahead of master), is that normal for most releases or is this a bit special?"}
{"question": "Are Graviton instances supported for Ray?"}
{"question": "I have a question about using the RayPlugin for lightning. What do people do without checkpoints? Do you assume that the last epoch was the best one? Is there no way to write checkpoints to a shared volume? The same thing applies to logging. Prior to using the plugin I would do something like this:\n```best_model_path = trainer.checkpoint_callback.best_model_path```\nThanks!"}
{"question": "hey team, i have a question around `ray.dataset.map_batches(callable, compute='actors')`, the `callable` here is going to be wrapped into actor. which require it be pickle friendly. and i\u2019m using a tensorflow keras model which isn\u2019t. Meanwhile, define the `callable` as an actor, the model  loading would happen in remote. So it won\u2019t run into pickle issues. wondering is there are any way i can provide my actors to `map_batches`  directly? Since i also want to shuffle the dataset so there are enough partitions to keep more actors busy."}
{"question": "Hi, I have a question about `ray.util.queue.Queue` (new to asyncio concept as well). My current setup has only one node and I would like to limit the number of in-flight tasks (read the backpressure example already). I implemented my own 'queue' with `ray.wait` . I feel that `Queue` is offering something similar, but there is no `ray.wait` , i.e., returns a bunch of \"ready-to-consume\" tasks. So is there anyway to check \"ready\" tasks in a `Queue`?"}
{"question": "Hello, is it possible to stop ray from redirecting the worker node logs to the head node in a ray cluster?"}
{"question": "hi, can anyone please help me a bit on what are the possible reasons for having\n```Map Progress (31 actors 8 pending): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:10&lt;00:00,  5.74s/it]\n....\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.```\nwhile running\n```dataset.map_batches(TensorflowModelPredictor(model_ref),\n                batch_size=256,\n                batch_format=\"pandas\",\n            ).write_parquet(output_path, filesystem=fs)```\n   ? there wasn\u2019t any information on memory or cpu  in the logs."}
{"question": "Is there some way a network REST api can start <http://Ray.io|Ray.io> workflows using some sort of RPC type call?"}
{"question": "I am running a python script that loops through different configs and runs experiments. If I schedule too many experiments to run in one loop, my process is killed because of memory issues. I noticed from watching the UI that workers are slowly increasing their amount of RAM after each iteration of my experiment. Is this indicative of a memory leak? Or could this be something else? When looking at the output of `ray memory` between iterations it shows nothing in the object store. Im not sure how to tell if this is a problem with my functions or if I did something wrong with ray accidentally leaking memory while running my remote functions?"}
{"question": "Hey, when I try to install extra dependency or copy my local working dir to Ray workers via `ray.init(\"ray://..\", runtime_env={..})`, how do I get more visibility on what\u2019s happening on the cluster? Sometimes, I noticed that that runtime env installation got stuck, but not sure what actually happened. How can I get more insights on the runtime env setup process?"}
{"question": "My laptop has a CPU with 4 cores. When I run this code snippet,\n```@ray.remote\ndef fun():\n    print('fun')\n    time.sleep(1)\n[fun.remote() for i in range(10)]```\nI got 4 printouts in the console. But If the time.sleep(1) was removed. I got 10 printouts. Could anyone give me an explanation of this? Thanks."}
{"question": "Hi, I am using Ray Lightning , Pytorch Lightning, Ray Tune to distribute training and tuning.  It is unclear to me on how to do resource allocation using RayPlugin from the documentation\n\nHere(<https://github.com/ray-project/ray_lightning/blob/65f497a3c8bedb2f24bf04a5dbf0ea62b5bcb4d6/ray_lightning/ray_ddp.py#L84>) it says, specify GPUs in Pytorch Lightning Trainer to a value &gt; 0\n\nand Here(<https://github.com/ray-project/ray_lightning/blob/65f497a3c8bedb2f24bf04a5dbf0ea62b5bcb4d6/ray_lightning/ray_ddp.py#L107>)  in example it says NOT to specify resource in Trainer\n\nWhich one is right? I am able to run tuning experiments in parallel but unable to distribute training.I have 5 workers nodes with each 8 GPUs on each node"}
{"question": "Hi everyone, how can I change the tensorboard event files with rllib? I wanna save the end of episode reward instead of mean reward."}
{"question": "Under the new 1.11.x system, without modifying logging, will all worker logs get written to the head nodes /tmp/ray/* files?"}
{"question": "We have a cluster with two node types for workers: one for a machine with 16 A100 GPU's and another worker node type for T4 GPU machines. When calling `ray start` in `worker_start_ray_commands:` , we would like to pass different values to `--object-store-memory`  to these two different node types, as the machines have different RAM. Is there a way to have different worker_start_ray_commands per node types? At least I didn't find this in the docs... cc/<@U0317V91RCG>"}
{"question": "Is *del object_ref* an efficient way to free the memory on the object store? object_ref = ray.put(object)"}
{"question": "Does anyone have an runtime_env example that installs conda dependencies on the cluster nodes using a environment.yml file?\n\n_Alternatively, you can specify a conda environment, either as a Python dictionary or via a environment.yml file._ <https://docs.ray.io/en/latest/ray-core/handling-dependencies.html>\n\nI am trying with this runtime_env:\n\n```# Define runtime environment\nruntime_env = {\n    \"working_dir\": \".\",\n    \"conda\": \"./conda_ray-lightning.yaml\",\n}\n\n# Initialize ray\nray.shutdown()\nray.init(address=\"<ray://10.128.0.5:10001>\", runtime_env=runtime_env)```\n(the file conda_ray-lightning.yaml is in my local working dir \".\")\n\nBut I get this error:\n\n```ConnectionAbortedError: Initialization failure from server:\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/client/server/proxier.py\", line 627, in Datapath\n    client_id, job_config):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/client/server/proxier.py\", line 284, in start_specific_server\n    specific_server=specific_server,\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/util/client/server/proxier.py\", line 237, in _create_runtime_env\n    \"Failed to create runtime_env for Ray client \"\nRuntimeError: Failed to create runtime_env for Ray client server: [Errno 2] No such file or directory: '/tmp/ray/session_2022-03-16_03-52-05_375263_234/runtime_resources/conda/21e48857648ef721633adab007a9bc925d0da446'```"}
{"question": "Is someone having problem with Kubernetes deployed clusters on AWS, after the new release of <https://hub.docker.com/layers/rayproject/ray/latest/images/sha256-029ec7b7b9e59d49b6ea3a66b2813c3c07407239e0f5991064aa561829ec69a9?context=explore|ray:latest>?\nKeep getting `ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.` and an rpc exception on the ray-operator pod"}
{"question": "Does anyone use NATS to collect Ray logs? Or fluentd/fluentbit ?"}
{"question": "```class Counter:\n    def __init__(self):\n        self.value = 0\n    def increment(self):\n        self.value += 1\n\n@ray.remote\ndef increase_counter(counter_ref):\n    counter_ref.increment()\n    return counter_ref\n\ncounter_ref = ray.put(Counter())  # ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000001000000)\ncounter_ref = increase_counter.remote(counter_ref)  # ObjectRef(a67dc375e60ddd1affffffffffffffffffffffff0100000001000000)```\nIs the change of *counter_ref* because the objects in the Object Store are immutable and the changes have to be made to a copy of the object?"}
{"question": "Hi, I launch two apps on the cluster head node (`ray==1.9.2`). Each app starts ray clients separately (with `ray.init` ). When one app invokes a `@ray.remote` , I get the following\n```    artifacts = await execute_pipeline.remote(*training_opts, use_amp=True)                                                                                                                    \n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/remote_function.py\", line 124, in _remote_proxy\n    return self._remote(args=args, kwargs=kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py\", line 295, in _invocation_remote_span\n    return method(self, args, kwargs, *_args, **_kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/remote_function.py\", line 223, in _remote\n    return client_mode_convert_function(\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 173, in client_mode_convert_function\n    return client_func._remote(in_args, in_kwargs, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/common.py\", line 130, in _remote\n    return self.options(**option_args).remote(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/common.py\", line 380, in remote\n    return return_refs(ray.call_remote(self, *args, **kwargs))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/api.py\", line 106, in call_remote\n    return self.worker.call_remote(instance, *args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/worker.py\", line 449, in call_remote\n    task = instance._prepare_client_task()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/common.py\", line 386, in _prepare_client_task\n    task = self._remote_stub._prepare_client_task()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/common.py\", line 156, in _prepare_client_task\n    self._ensure_ref()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/common.py\", line 152, in _ensure_ref\n    self._ref = ray.worker._put_pickled(\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/worker.py\", line 404, in _put_pickled\n    resp = self.data_client.PutObject(req)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/dataclient.py\", line 356, in PutObject\n    resp = self._blocking_send(datareq)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/dataclient.py\", line 261, in _blocking_send\n    self._check_shutdown()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/util/client/dataclient.py\", line 315, in _check_shutdown\n    raise ConnectionError(msg)\nConnectionError: Request can't be sent because the Ray client has already been disconnected due to an error. Last exception: &lt;_MultiThreadedRendezvous of RPC that terminated with:\n        status = StatusCode.NOT_FOUND\n        details = \"Attempted to reconnect a session that has already been cleaned up\"\n        debug_error_string = \"{\"created\":\"@1647442249.207832542\",\"description\":\"Error received from peer ipv4:10.82.146.250:10001\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":903,\"grpc_\nmessage\":\"Attempted to reconnect a session that has already been cleaned up\",\"grpc_status\":5}\"```\nWhat am I doing wrong? Is it incorrect to start ray.init two times from two different system processes? Thanks!"}
{"question": "Hi! I'm trying to get all detailed logs from a ray task. Here is a simple example of my code:\n\n``` import ray\n import logging\n  \n # Start Ray.\n ray.init()\n  \n  \n @ray.remote\n def f():\n   log = logging.getLogger()\n   log.setLevel(logging.DEBUG)\n   log.debug(\"MY DEBUG STH\")\n   <http://log.info|log.info>(\"MY INFO STH\")\n   log.warning(\"MY WARNING STH\")\n   log.error(\"MY ERROR STH\")\n  \n ray.get(f.remote())```\nThe outcome is:\n```(f pid=73317) MY WARNING STH\n(f pid=73317) MY ERROR STH```\nDo you know what might be the reason of not getting logs for logs for `log.debug` and `<http://log.info|log.info>`?"}
{"question": "Hi Everyone, I have a ray cluster on AWS with one head node (CPU only) and four worker nodes (GPU machines). I'm trying to run PyTorch ResNet trianing. I get the following error:\n`ModuleNotFoundError: No module named 'torch'`\nI have pytorch installed on the worker nodes but not on the head node. How do I tell the `TorchTrainer` to only use the worker nodes?"}
{"question": "My machine has 4 cpus and 1 gpu. Is setting ray.remote(num_gpus=0) enough to make sure that the task runs exclusively on the cpu?"}
{"question": "The code runs as intended on my lap, but when it runs on another machine with more CPUs, it is incredibly slow. I have been attempting to figuring out what might cause this peculiar thing. Anyone having any thoughts about it? Thank you."}
{"question": "Have anyone faced similar issue? or Is there any configuration that can be used to solve this problem?"}
{"question": "Hey ya'll. Has anyone noticed at times ray on k8s (GKE in my case) will run the worker pods but for some reason some of them intermittently fail to connect with the head node? For reference I'm using helm to deploy pod clusters, am seeing the worker pods as running, but the head node fails to find them i.e. when I check the dashboard I only see a subset of workers associated with the head"}
{"question": "Hello Everyone.\nCan anyone help me why I am getting 0 values in resources usages?\n```Resources\n---------------------------------------------------------------\nUsage:\n 0.0/2.0 CPU\n 0.00/2.877 GiB memory\n 0.00/0.673 GiB object_store_memory\n\nDemands:\n {}: 2+ pending tasks/actors```\nCan this might be the reason for worker node not scaling up? I am using rya:1.11.0-py38 tag image for all of the services(autoscaler, head and worker)\nAlso I can see from the dashboard, CPU is fully used."}
{"question": "Hello everyone :slightly_smiling_face:. My team and I have been successfully and happily using ray and ray-tune for hyperparameter search for the past year. In our current infrastructure, we are deploying a ray-operator pod, using kubernetes, on AWS and, at the same time we also deploy a head-node, which is in charge of the workers distribution. Life was going well until the 10th of March! Suddenly, we could not run any more experiments. We could not figure out right away the problem (it took us 3 days of log reading and trial-error experiments) to understand that the problem was related to the ray cluster deployment. Something was wrong with the ray-operator and we could never reach a *healthy status.* After understanding our system stopped working after the \u201clatest\u201d release (which was the ray-image that was being used by the ray-operator), we changed it to the 1.10.0-py37 and, after that, we were able to get our healthy status and launch jobs on it. I wanted then to ask help to the community to better understand what happened. Does anyone has any idea of what changed so much between previous and latest releases to actually break the system we had been using for a year ? and, maybe, how could we fix it?"}
{"question": "Does anyone have an example for Ray 1.11.* that starts the ray process and redirects stderr/stdout to a handler stream?  Or is there a more civilized way to get logs into python code for processing, like a tcpip:port facility. Maybe with io.StringIO capture?"}
{"question": "I get the following error when trying to use `TorchSquashedGaussian`  action dist for PPO\n``` ValueError: KL not defined for SquashedGaussian!```\nAnyone know how to fix this?"}
{"question": "Hey guys - I\u2019m running ray on local multi-machine cluster in docker. From log, I saw autoscaler constantly restarting worker node possibly b/c of the AUTOSCALER_HEARTBEAT_TIMEOUT_S, which eventually gave me docker out of space error.\n\nWhat is the recommended way to resolve this, other than setting the env var? Thanks!"}
{"question": "Hi Everyone - I am working on updating the TD3 algorithm in Ray. I am using colab. I make the changes in my github, and then try to build from source using bazel. Is there an example notebook on how to successfully implement such solution?"}
{"question": "Does ray have a  \"broadcast\" (pub/sub) version of  `ray.util.queue.Queue`?"}
{"question": "Hi everyone, how can I change the tensorboard event files with rllib? I wanna save the end of episode reward instead of mean reward"}
{"question": "Hi everyone,\nHas anyone tried to use the `ray up` command from docker?\nWe are trying to launch/relaunch our cluster from Gitlab CI."}
{"question": "I\u2019m trying to run a python ML process via Ray, which I\u2019m hosting on a k8s cluster, the main script will be on my local computer and it is connecting via <ray://cluster.address>:port. I have created a custom docker image to ensure that the Python &amp; ray versions are the same on my local computer, and the cluster, and also to install various packages which I will need to run my main script. I ran one of the test files in the documentation to ensure that I am able to connect to the ray cluster and run remote functions, which I can do. However, when I try to run the main script, I am finding that Ray seems to have problems with dependencies. Specifically, I have a set of helper functions in a script \u2018NLPHelper.py\u2019, which are shared across various scripts in my pipeline so I found it very helpful to modularize it &amp; import the functions. i.e. from NLPHelper import *. However, I found that Ray seems to have difficulty with this as I am getting an exception on cloudpickle, when I call myprocess.remote(arguments): ModuleNotFoundError: No module named \u2018NLPHelper\u2019. Do I need to copy my NLPHelper module into the docker container (and thus into k8s) as well?"}
{"question": "Hi! We\u2019re noticing occasional errors like\n```Error processing your query\nRuntimeError: Insufficient resources for model, requested: {'num_cpus': 0.1, 'num_gpus': 1}, available: {'CPU_group_0_ee022c5f504f9a33e6479ce6faf0d107': 0.1, 'bundle_group_ee022c5f504f9a33e6479ce6faf0d107': 1000.0, 'bundle_group_0_ee022c5f504f9a33e6479ce6faf0d107': 1000.0, 'memory': 26221011252.0, 'accelerator_type:T4': 1.0, 'node:10.244.16.3': 0.98, 'CPU': 3.8, 'bundle_group_fde5ab4549b0aff09b7cf40c8b95daf4': 1000.0, 'CPU_group_0_fde5ab4549b0aff09b7cf40c8b95daf4': 0.1, 'bundle_group_0_fde5ab4549b0aff09b7cf40c8b95daf4': 1000.0, 'GPU_group_0_fde5ab4549b0aff09b7cf40c8b95daf4': 1.0}```\nbased on this message, it seems that there *are* available resources to fulfill the request, which is confusing. Am I reading the error incorrectly somehow? Any hints for debugging appreciated!"}
{"question": "Hi! I would love to register for the recommender systems tutorial next week but it's from 5am to 7am in my country. I was thus wondering if the tutorial materials would be made available afterwards?"}
{"question": "does ray dataset support multiple output? e.g. split a dataset into train &amp; test folders based on values in some column."}
{"question": "Hi everyone, how can we change the ray such that if all the healthy workers are down, we can reset the workers again?"}
{"question": "I want to use tune.grid_search but even with num_workers = n &gt; 1 RAY calculates only single env in serial mode, how should I force ray to make parallel calculations?"}
{"question": "Hey all, this may be me completely misapplying Ray but I was hoping to use it as a way to manage sending parallel requests to a GRPC server\n\n```ds = ray.data.read_csv(tmpfile.name)\nclient = GrpcClient2(...)\n\ndef do_work(row):\n    client.send_request_using(row)\n\nds.map(do_work)```\nHowever I'm running into two issues:\n\n1. Transformations to the Arrow Row objects don't seem to be occuring\nFor example\n```ds.map(lambda row: row[\"substring\"])\nds.map(lambda substring: json.loads(substring))\nds.take(1) # &lt;&lt;&lt; Still prints the original rows```\n2. The GrpcClient2 instance cannot be serialized\n\nIs there a way of sharing non-serializable resources between tasks? (probably not, since to share the resource by definition it needs to be serializable)\n\nIs this sort of IO work a possible usecase of Ray or am I committing a sin here haha"}
{"question": "Hello Guys,\nCan anyone help me look at this thread please? I am blocked due to this issue.  <https://discuss.ray.io/t/autoscaler-not-scaling-up-the-worker-node-when-using-image-rayproject-ray-1-11-0-py38/5501>\nAlternatively what do you suggest about using HPA for worker node? I tried a lot of things, but could not get autoscaler working for ray-1.11.0 with python 3.8.x."}
{"question": "Hi All, I am new to <http://Ray.io|Ray.io> when i am trying to use <http://Ray.io|Ray.io> for hyperparameter tuning along with XGBoost i am Resource Exahausted error\n\nfrom xgboost_ray import RayDMatrix\n\ntrain_set = RayDMatrix(X_train, y_train)\ntest_set = RayDMatrix(X_test, y_test)\n\nfrom xgboost_ray import RayDMatrix, RayParams, train\n\nstart = time.time()\n\nnum_actors = 1 # num of actors\nnum_cpus_per_actor = 4\n\nray_params = RayParams(\nnum_actors=num_actors,\ncpus_per_actor=num_cpus_per_actor)\n\ndef train_model(config):\n#train_x, train_y = load_breast_cancer(return_X_y=True)\n#train_set = RayDMatrix(train_x, train_y)\n\nevals_result = {}\nbst = train(\nparams=config,\ndtrain=train_set,\nevals_result=evals_result,\nevals=[(test_set, \"eval\")],\nverbose_eval=False,\nray_params=ray_params)\n\njoblib.dump(bst,\"model.pkl\") # Best model?\n\nfrom ray import tune\n\n# Specify the hyperparameter search space.\nconfig = {\n\"objective\": \"reg:squaredlogerror\",\n\"eval_metric\": [\"rmsle\"],\n\"min_child_weight\": tune.randint(1, 3),\n\"max_depth\": tune.randint(5, 10),\n\"gamma\": tune.loguniform(0.1, 0.5),\n\"subsample\": tune.loguniform(0.5, 1.0),\n\"colsample_bytree\": tune.loguniform(0.5, 1.0),\n\"eta\": tune.loguniform(0.1, 0.5),\n\"seed\": tune.randint(110, 135)\n}\n\n# Make sure to use the `get_tune_resources` method to set the `resources_per_trial`\nanalysis = tune.run(\n\ntune.with_parameters(train_model),\n#train_model,\nconfig=config,\nmetric=\"eval-rmsle\",\nnum_samples=4,\nmode=\"min\",\nresources_per_trial=ray_params.get_tune_resources())\n\nprint('Runtime(HH:MM:SS) :', get_runtime(time.time()-start))\n\nprint(\"Best hyperparameters\", analysis.best_config)\nThis is the error i am getting\n\n_InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\nstatus = StatusCode.RESOURCE_EXHAUSTED\ndetails = \"Sent message larger than max (734768174 vs. 536870912)\"\ndebug_error_string = \"{\"created\":\"@1648431702.877243956\",\"description\":\"Sent message larger than max (734768174 vs. 536870912)\",\"file\":\"src/core/ext/filters/message_size/message_size_filter.cc\",\"file_line\":264,\"grpc_status\":8}\"\n&gt;\n[1:10 PM] can someone help me in this..?"}
{"question": "Hi, I remember seeing some work where I can specify python libraries installable by pip(and perhaps folders?) as context for a Ray Actor. I can't find that page right now and it's full feature set. I was wondering if someone could point it out to me. The idea is being able to have isolated actors where each actor has it's own dependencies and it's useful when ray actors have to have incompatible versions of the same library or a binary with different versions?"}
{"question": "So still trying to understand the details of using shared memory across multiple workers.  In my use case I am exploring, I have a python dict (rather large, say using 10GB of memory) and I want each of say 50 workers on each box to have read access to the dictionary without reallocating 10GB of memory 50 times.  Is there a way to do this?  Or does this require maybe that the shared data structure is something in numpy vs a python dict?"}
{"question": "Is using ray actors to send tasks to the cluster bad behaviour?\n\n```@ray.remote\nclass Foo(object):\n    @ray.method(num_returns=1)\n    def bar(self):\n        self.count = 0\n\n        @ray.remote\n        def parallel_task(self):\n            time.sleep(4)\n            return 1\n\n        result = ray.get([parallel_task.remote(self) for i in range(2)])\n        for i in result:\n            self.count += i\n        return self.count```"}
{"question": "If I were wanting to have a muti-agent (2 agents) environment where the actions of each agent was motivated by a combination of the rewards of each, what would be the correct way of implementing the custom choosing of actions?  Specifically, each agent is predicting a payoff matrix and the actions chosen are supposed to be the joint action that maximizes the combined matrices."}
{"question": "Hi everyone, how can I change the tensorboard to plot episode rewards? The distribution on tensorboard is not clear."}
{"question": "The function `fun` is decorated by `ray.remote` and the Ray cluster consists of 1 GPU and multiple CPUs, When I call `fun.remote()` , does it run on a CPU or GPU?"}
{"question": "Hello everyone.\nIs there an environment variables like `RAY_OVERRIDE_RESOURCES`  that can be used in worker node so that after the provided resources limit, no more jobs/actors are not scheduled on that particular worker node.\nAlso, I noticed that the actors/jobs goes to `DEAD` state due to memory not being available, does not rescheduled. Is it expected result?"}
{"question": "Hi all, I am running a manual cluster for ray cluster (ie running ray start with a head node and connecting worker nodes to the head node). I have a set of code which runs across all the nodes. It works fine when there is not a lot of iterations but when I increase the iterations, one of the worker nodes will unexpectedly crash and quit the cluster. It is always the same one. Each of the nodes in the cluster are running from the same base image VM (on different hardware though). \n\nI was wondering what is the best way to start troubleshooting? Would there be logs somewhere on the crashed node that I could go through? Or some other way to capture what's going before it crashes?\n\nIt isn't reproducible all the time either which is a head scratcher. \n\nMaybe alternatively are there any load testing scripts I could run to ensure it isn't some weird bug in my code (which I wouldn't be surprised if there are some). \n\nCheers!"}
{"question": "This question was also posted on the Discuss board: <https://discuss.ray.io/t/trying-to-build-containers-from-ray-wheels/5618>\nAre there any lists of Ray wheels available for download?\n\nSo far, I found that the only ones which seem to work are:\n\u2022 `2.0.0.dev0` for Py 3.7 or 3.8\n\u2022 `1.7.0`\nThe example given in the docs about constructing a URL for a wheel for a specific version currently returns a 404 error.\n\nAlternatively, are there any full `Dockerfile` examples for building a container image for Ray to run on Kubernetes on Azure?\n\nI'm asking because:\n\u2022 Ray container images on DockerHub fail corporate security audits (e.g., `log4j` vulnerability, etc.)\n\u2022 the `build-docker.sh` script in the Ray repo is not suitable for use in a regulated environment (need `Dockerfile` source not cached layers)"}
{"question": "~There is a Ray meetup in 15 mins.~ Just curious if the recordings will be available on youtube later? This event will be in 2 weeks..\n<https://www.meetup.com/Bay-Area-Ray-Meetup/events/284051590/>"}
{"question": "Hi everyone, I\u2019m new to Ray so this question may sound silly. I\u2019m wondering how do I install the Ray CLI? I\u2019m trying to run the ray up command to start a new cluster however I get cannot find command error on Ubuntu. I couldn\u2019t find any installation docs mentioned anything about how to install the Ray CLI command so I\u2019m hoping that if anyone here knows how to install it. Thanks!"}
{"question": "Is there a problem with using a normal torch nn.Module model (that does not inherit from TorchModelV2) as part of a custom (ray compliant, so inheriting from TorchModelV2) torch model? I somehow am getting all zeros in my input_dict for the observations related entry when I start using those models in the forward method, but without it, it does not seem to happen."}
{"question": "Is there a questions channel? Or is that what <#C01DLHZHRBJ|general> is for?\n\nI have a use case where I want the ray remote function to run with a different UID. In our setup, we have an NFS server mounted to all Ray workers. Whenever I run a ray function that outputs files, I would like those files to be owned by a different uid. Is this possible? Is it possible even if I have to fork and customize Ray?\n\nMy initial thought is maybe it\u2019s possible through runtime env. Can the Ray driver tell the Ray cluster to use a specific UID?"}
{"question": "When using this Ray wheel to build a container image:\n<https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-2.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl>\n\n**What is the actual Ray release number???**  Clearly, it's not `ray-2.0.0.dev0`\n\nThis is important when running in a K8s environment and needing to use compatible releases of Ray across the images/pods/pools.\n\nWe haven't found anywhere where the `2.0.0.dev0` is explained.\n\n```$ docker run -it ray-base /bin/bash\n(base) ray@b00ca3a4d965:~$ pip show ray\nName: ray\nVersion: 2.0.0.dev0\nSummary: Ray provides a simple, universal API for building distributed applications.\nHome-page: <https://github.com/ray-project/ray>\nAuthor: Ray Team\nAuthor-email: <mailto:ray-dev@googlegroups.com|ray-dev@googlegroups.com>\nLicense: Apache 2.0\nLocation: /home/ray/anaconda3/lib/python3.8/site-packages\nRequires: virtualenv, grpcio, filelock, pyyaml, attrs, click, aiosignal, requests, jsonschema, frozenlist, msgpack, numpy, protobuf```\ncc: <@UNCRYAV9N> <@U02K9585Q8J>"}
{"question": "Thank you <@UVD989S0K>, then that's what `2.0.0.dev0` means \u2013 simply the nightly build from head commit in `master` ?"}
{"question": "Hi everyone, I am new to ray. I am facing a problem when try to parallelize the task across workers.  I have created a local ray cluster and connected all the workers to it, so I am facing module not found error (RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:) for the first time when I am running my python file. But when I re-run the same file for the second time, I am not receiving any error. Can anyone help me in this?"}
{"question": "Hi guys, I\u2019ve been playing around Ray Tune. Do you guys know if there\u2019s anyway to have Trial-wise dependencies? A practical use case would be cross validation (w/o using Repeater). "}
{"question": "I have a problem that occurs on Ryzen 5950x (Ubuntu 20.04.1) but not on Intel(R) Xeon(R) Platinum 8259CL (18.04.1) or Ryzen Threadripper  2950x  (20.04).. Might be related to Linux 20.04.1?\n\nI have a script that uses multiprocessing pool to initialize RLlib PPO agents (# agents = num threads - 1) and run through 800 steps in a custom environment. It continues until I run a total of ~1000 actors through the environment. I am using reuse_actors = False.\n\nOn Ryzen 5950x, at first, the \"trainables\" initialize quickly. As time goes on, they initialize slower and slower. Trainable set up time will increase from ~10s to 10 minutes.\n\nThe processes are stuck in uninterruptible sleep (state D -- see pic).\n\nI am running inside a docker container."}
{"question": "For 1.11.0 is it possible to `breakpoint()` inside a worker?"}
{"question": "Also: how can we customize logging in <https://github.com/ray-project/ray/blob/releases/1.11.0/python/ray/worker.py#L83=> so that we can include the workflow id for job tracebacks? Right now there is no workflow context provided in tracebacks."}
{"question": "I've been struggling to get a good spread across nodes for remote actors being created for ActorPool. I tried placement groups and strict_spread and it seems to want to always want to stick to a single node. Is there a specific way to create the list of actors for the ActorPool so it knows to use different nodes for the actors?"}
{"question": "How to change concurrent trial count for PB2? I want 8 but default starts with 16 and it crashes on no memory"}
{"question": "Hi all, anyone else having issues installing ray with pip right now? I can `pip install -U pip` and `pip install tqdm` but `pip install ray[default]` gives me\n```ERROR: Could not find a version that satisfies the requirement ray[default] (from versions: none)\nERROR: No matching distribution found for ray[default]```"}
{"question": "Aha, Python 3.10 is the issue apparently? I thought I was using ray with it before, though"}
{"question": "Is there another high level way of installing/adding dependencies within a ray cluster other than\n*runtime_env* ?\n\nEspecially is there a cluster  scope  besides  job or actor scope?\n\nUsually you do not want to run the install process again and again with every  task or actor deployed in the cluster"}
{"question": "When trying to run a container as an `runtime_env` argument at task execution time, I have managed to get the container to run using `podman` on the head node, but then it seems to time out.\n\nThe current dictionary I am passing to `ray.init()` is:\n\n```{\n        \"eager_install\": True,\n        \"container\":  {\n            \"image\": \"<http://rayclusternodes.azurecr.io/smartreturntoolsbayes-ray-2.0.0dev-cpu:1.0.0|rayclusternodes.azurecr.io/smartreturntoolsbayes-ray-2.0.0dev-cpu:1.0.0>\",\n            \"run_options\": [\n                \"--ulimit nofile=65536:65536\"\n            ]\n      }\n}```\nIs there any additional setup needed to get these docker container based environments to run?\n\nIt is failing on the head node, my notional understanding was that the head node needs the execution environment to deserialize the incoming tasks input objects, before submitting resource requests to the autoscaler. Is that right?\n\nThe ray client log reports the following:\n\n```Copying config sha256:4171d8fb66aeb33c639b3784f402bdb69fe49cd3464e8c8d09fbc798a6a4feb6\nWriting manifest to image destination\nStoring signatures\n2022-04-05 04:49:22,751 INFO server.py:843 -- Starting Ray Client server on 0.0.0.0:23010\n2022-04-05 04:49:27,794 INFO server.py:890 -- 25 idle checks before shutdown.\n2022-04-05 04:49:32,804 INFO server.py:890 -- 20 idle checks before shutdown.\n2022-04-05 04:49:37,812 INFO server.py:890 -- 15 idle checks before shutdown.\n2022-04-05 04:49:42,822 INFO server.py:890 -- 10 idle checks before shutdown.\n2022-04-05 04:49:47,832 INFO server.py:890 -- 5 idle checks before shutdown.```"}
{"question": "Hi All\nI setup a cluster with 3 nodes (10 cpus, 8 cpus. 8cpus), one is a head node and I'm trying to run APPO using tune, all is good recources are presented correctly but my trial is still in a PENDING state, what is happening?"}
{"question": "I\u2019m running with ray.init() from a python script from one of the nodes and selecting 20 workers.\nHow to run this algo in a distributed manner ?"}
{"question": "Is it possible to specify the optimizer to use in an algorithm? For example, I'm using the QMIX algorithm, which I believe uses RMSProp by default. Is it possible to tell it to use Adam, instead?"}
{"question": "Hey guys ... got a problem with our ray setup similar to <https://github.com/ray-project/ray/issues/19792>. However we are using ray==1.8.0 and never changed any dependencies. Since 05-04-2022 it stopped working. anyone else seen this issue?"}
{"question": "Anyone have perspectives on Gradio vs FastAPI for deploying transformer based products?\n\nJust built something fun using Huggingface/Ray, and team is talking SWOT on the options :dna::pray::innocent:"}
{"question": "I'm having the same problem but not using GKE/k8s. I get an error about the node/actor disappearing. I also set my max retries to 5, but I'm starting to wonder if this doesn't cover the node going away, just numerical problems with the work. Anyone else experience this? Are we missing something in the config to gracefully recover?"}
{"question": "Hey Is there any provision to pass instance id instead of AMI image id  in the cluster launcher config file for both head and workers? I checked the whole documentation I didn't find any."}
{"question": ":wave: Hello, team! I would like to know if we can start/stop the cluster from different machines? How do we automate the process of updating ray cluster whenever there is a code checkin on github repo? Thanks."}
{"question": "Is there anyway to profile a ray  remote call? (GPU and CPU resource, for example)"}
{"question": "Hey,\n\ncould someone with ray in production outline the go to way for handling dependencies within a ray cluster?\nDependency Management for the 2 different ways of deployments:\n\u2022 kubernetes\n\u2022 aws/gcp/azure  cluster\nHow to install new dependencies within the cluster without breaking the microservice architecture, so to have basically different dependencies for different actors/deplyoments or tasks"}
{"question": "Is <https://github.com/ray-project/ray/releases/tag/1.12.0|Ray 1.12.0> meant to be a pre-release or a regular release? The github page says so, but the tag or the version name have no alpha/beta/dev component in it. The tag convention used (`1.12.0` ) is also now a departure from the previous `ray-MAJOR.MINOR.PATCH` format"}
{"question": "Is there an easy pythonic way to terminate a workflow?"}
{"question": "Hello,\n\nHow can l specify a stopping criteria in `tune.run()`  for instance stop running if validation_loss &lt; 0.005. Is it possible ?\n\n`tune.run(num_samples=200,mode=\"min\",metric=\"loss_validation\",stop=\" SOMETHING HERE\")`"}
{"question": "Hello,\nHow can l add to my table view a column \"`training_iteration`\" ? useful to see for each experiments at which iteration it has been stoped by the ASHAScheduler because it's is a bad configuration.\n\ni did the following:\n\n`metrics = {`\n        `\"loss_validation\": \"loss_validation\",`\n        `\"training_iteration\": \"training_iteration\",`\n    `}`\n    `callbacks = [TuneReportCallback(metrics, on=\"validation_end\")]`\n`pytorch_lightning.Trainer(...,callbacks=callbacks)`\n\n`def validation_step(self):`\n  `self.log(`\n             `\"training_iteration\",`\n             `set.current_epoch,`\n             `on_step=False,`\n             `on_epoch=True,`\n         `)`\n\nHowever it is not reported."}
{"question": "Hello,\nAfter launching `run.tune()` l followed the provided http link to open the dashbord. However, the dashboard appears then remains blank. My experiments have finished correctly.\nAny cue to open the dashboard ?\nI tried also the following:\nray start --head --dashboard-host=0.0.0.0 --port 7000\nthen run the following script\n`import ray`\n\n`ray.init(address=\"<ray://10.132.0.48:7000>:10001\")`\nIt doesn\u2019t solve the problem.\nHow can l access the dashboard alive during training and what is the appropriate command line to open the dashboard at the end of `tune.run()` execution.\nThank you."}
{"question": "Hi Ray community!\nWe have a question we hope you can help us solve:\nWe want to achieve some parallelism with an ML model (eg. a Tensorflow/Keras model) we have. The ML model itself is quite fast, but it has a huge memory footprint. So much so we are memory bound rather than CPU bound, hence we cannot replicate the model enough to cover all CPU cores (example: model 2GB, available RAM 4GB, CPU 4 cores).\n\nDo any of you know a way to share memory across Ray Actors? Our ideal goal would be to load the model once and then have all actors share the *same* memory in a sort of read-only mode. Maybe multiprocessing.shared_memory could do it in your opinion? If not, any suggestion or alternative strategies? Thanks :slightly_smiling_face:.\n\nOf course, the provided example is a rather simplified view meant only to highlight the problem.\n\nSome contextual assumptions:\n1. inputs are already maximising a single model performance (for instance they are batched)\n2. we cannot scale to a cluster, we are bounded to a single machine.\nEXAMPLE (python):\n```# ---------------------------------------------------------\n# @author Ludovico Frizziero &lt;ludovico.frizziero@gmail.com&gt;\n# ---------------------------------------------------------\nimport itertools\nfrom multiprocessing import cpu_count\n\nimport ray\n\n\ndef load_HUGE_model(file_handle):\n    ...\n\n\n@ray.remote\nclass ModelRunner:\n\n    def __init__(self):\n        self.model = load_HUGE_model(...)\n\n    async def predict(self, *args):\n        return self.model(*args)\n\n\nif __name__ == '__main__':\n    num_replicas = cpu_count()\n\n    actors = [ModelRunner.remote() for _ in range(num_replicas)]\n\n    # map inputs to actors\n    inputs = [...]\n    results = []\n    act_iter = itertools.cycle(actors)\n    for _in in inputs:\n        act = next(act_iter)\n        results.append(\n            act.predict.remote(_in)\n        )\n\n    results = ray.get(results)```"}
{"question": "Is combined algorithm selection and HPO on the roadmap for ray tune?\nUseful features would be\n\u2022 to have a simple way to request different resources (computational resources (one model needs 2 gpu's, the other one only 1) and dependencies (e.g. one model needs tf 1, the other one tf2)) per Trainable ... \n\u2022 to be able to run multiple HPO run's in parallel that have different search spaces but share the same objective..."}
{"question": "Hello Team, I would like to get some guidance on how to interpret the Ray dashboard. I am using Ray on K8s. I have spawned up local Ray Cluster on a K8s pod. The K8s pod has 4 vcpus &amp; 64GB RAM. I invoked a lightgbm training job using\n```bst = ray.get(train_lgbm_remote.remote(upq_lgbm_params, upq_data_files,\n                                           RayParams(num_actors=2, cpus_per_actor=2)))```\nI understand 2 actors are consuming 2 cpus each i.e. they consume all the cpus. I still see couple of idle PIDs. What do they mean? Also I am getting this warning log, What do they mean? Also, the number of worker is 10, how is that calculated?\n```scheduler +4m42s) Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n(scheduler +5m18s) Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.```\nI could not find much documentation around dashboard. Could you point to if there is anything comprehensive."}
{"question": "Hello, \nIs there a way to prevent Ray from saving any logs to the server\u2019s filesystem that it\u2019s running on?\n\nI\u2019ve tried setting log_to_driver to false but logs are still flooding into /tmp/ray/sessionID/logs/workerID.out &amp; .err (the logs of a model that we use cannot be closed due to an issue so it accumulates high volumes of logs).\n\nCreating and scheduling a script to remove logs on a regular basis seems like a solution but not a right practice. \n\nThanks.\n"}
{"question": "Hello,\n\nI am running tune with `Asha scheduler` and `AX search algorithm` .  I would like to add some constraints on `AX` , a kind of stopping criteria if two metrics are jointly satisfied.  I did the following:\n\n`algo = AxSearch(outcome_constraints=[\"loss_validation&lt;= 0.005\"])`\n`algo = tune.suggest.ConcurrencyLimiter(algo, max_concurrent=4)`\n`Scheduler = ASHAScheduler( max_t=params[\"nb_epochs\"], reduction_factor=4, )` \n`analysis = tune.run( train_fn_with_parameters, metric=\"loss_validation\", mode=\"min\", config=config)`\n\nI tried the following but it doesn't work l get an error related to the format of `outcome_constraints`:\n\n1/ `algo = AxSearch(outcome_constraints=[\"loss_validation&lt;= 0.005\"])`\n2/ `algo = AxSearch(outcome_constraints=[\"metric&lt;= 0.005\"])`\n3/`algo = AxSearch(outcome_constraints=[\"loss_validation&lt;= 0.005\" and \"loss_train&lt;= 0.0001\"])`\n\nAny cue ?"}
{"question": "Hi,\n\nI am trying to connect to the dashboard of my ray cluster in GCP. I created a firewall rule for port 8265 and added the network tag to my head node:\n\n```ray_head_default:\n        node_config:\n            machineType: n1-highcpu-32\n            disks:\n              - boot: true\n                autoDelete: true\n                type: PERSISTENT\n                initializeParams:\n                  diskSizeGb: 500\n                  # See <https://cloud.google.com/compute/docs/images> for more images\n                  sourceImage: projects/&lt;my-project&gt;/global/images/common-cpu-ray-ml-head-v20220331\n            tags:\n              items:\n                - ray-dashboard\n&lt;snip&gt;\n# Command to start ray on the head node. You don't need to change this.\nhead_start_ray_commands:\n    - ray stop\n    - &gt;-\n      ulimit -n 65536;\n      ray start\n      --head\n      --port=6379\n      --object-manager-port=8076\n      --autoscaling-config=~/ray_bootstrap_config.yaml\n      --include-dashboard=true\n      --dashboard-port=8265 ```\nBut I get no response:\n\n```$ nmap -sV --reason -Pn  -p 8265 146.148.48.251                                                                                                                                                          (rllib) \nStarting Nmap 7.92 ( <https://nmap.org> ) at 2022-04-19 16:19 EEST\nNmap scan report for <http://251.48.148.146.bc.googleusercontent.com|251.48.148.146.bc.googleusercontent.com> (146.148.48.251)\nHost is up, received user-set.\nPORT     STATE    SERVICE REASON      VERSION\n8065/tcp filtered unknown no-response```\nDoes anyone what I am missing?\n\nMy current work around is to run ray dashboard cluster.yaml on my local machine and then open localhost:8265\n\nI don't want all our AI scientists to have to install ray and run this command locally to access the dashboard of our clusters :disappointed:"}
{"question": "Has anyone used Ray to solve production optimization use case using A* algorithm? We have a code that works on smaller data set and looking for ways to distribute and run it faster with all our products. It would be great to get some example code or hear about similar experiences and tips."}
{"question": "also if there are more concurrent methods than the concurrency limit, what happens? is there a queue of jobs for that actor until the threads free up?"}
{"question": "Hello, everyone.\nIs there any solution to control/track amount of resources used by specific user? Question closely related to authorization/authentication by ACL or anything close to that (OAuth?).\n\nThe only thing I\u2019ve found in docs is a TLS authentication, but it is only give/deny access, and I haven\u2019t found  any way to get additional fields from certificates (like user name or id) and then pass it to worker. Have I missed something?"}
{"question": "Hello Everyone,\nHow can I get deeper or more insights into the redis store to see the contents from the GCS server or the object store. We are running Ray on K8s with ray 1.10.0. I am able to `exec -it` into the Ray head pod.\n```root           1       0  0 07:48 ?        00:00:00 /bin/bash -c -- trap : TERM INT; touch /tmp/raylogs; tail -f /tmp/raylogs; sleep infinity &amp; wait;\nroot           8       1  0 07:48 ?        00:00:00 tail -f /tmp/raylogs\nroot          81       1  0 07:48 ?        00:00:00 /code/venvs/venv/lib/python3.8/site-packages/ray/core/src/ray/thirdparty/redis/src/redis-server *:6379\nroot          86       1  0 07:48 ?        00:00:00 /code/venvs/venv/lib/python3.8/site-packages/ray/core/src/ray/thirdparty/redis/src/redis-server *:46670\nroot          91       1  1 07:48 ?        00:00:01 /code/venvs/venv/lib/python3.8/site-packages/ray/core/src/ray/gcs/gcs_server --redis_address=10.46.124.154 --redis_port=6379 --log_dir\nroot         108       1  0 07:48 ?        00:00:01 /code/venvs/venv/bin/python3 -m ray.util.client.server --redis-address=10.46.124.154:6379 --host=0.0.0.0 --port=10001 --mode=proxy --r\nroot         109       1  1 07:48 ?        00:00:01 /code/venvs/venv/bin/python3 -u /code/venvs/venv/lib/python3.8/site-packages/ray/dashboard/dashboard.py --host=0.0.0.0 --port=8265 --p\nroot         130       1  0 07:48 ?        00:00:00 /code/venvs/venv/lib/python3.8/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2022-04-20_07-48-25_\nroot         131       1  0 07:48 ?        00:00:00 /code/venvs/venv/bin/python3 -u /code/venvs/venv/lib/python3.8/site-packages/ray/_private/log_monitor.py --redis-address=10.46.124.154\nroot         166     130  1 07:48 ?        00:00:01 /code/venvs/venv/bin/python3 -u /code/venvs/venv/lib/python3.8/site-packages/ray/dashboard/agent.py --node-ip-address=10.46.124.154 --```\nThen ran `apt-get update`;`apt-get install redis-cli` to install redis cli. But I am not able to authenticate.\n```root@kaggle-cluster-ray-head-r8b57:/opt/spark/work-dir# redis-cli\n127.0.0.1:6379&gt; ping\n(error) NOAUTH Authentication required.```\nWhat would be the user id and password ? What tool you use to investigate your cache ? For Ray on K8s would port forwarding to port 6379 and connecting to the redis using localhost:6379 would work?"}
{"question": "when i initialize an actor within another actor, is there a way to allocate resources outside of the parent actor? This can be limiting. I\u2019m guessing i could use a resource group? "}
{"question": "What happens when you set more jobs to an actor class than the max-concurrency limit? Is there a queue for the jobs in question?"}
{"question": "Hi there, I had a question on the Ray scheduler. How does it avoid deadlocking/creating a massive wave of worker processes in the following code?"}
{"question": "g() should claim one CPU, then attempt to create a lot more via f() - ray.get() releases the 1 CPU it's claimed for resources, I assume, but what I'd like details on scheduling wise is how it (or even whether it) avoids creating a ton of waiting g() processes  - is there code that prioritizes nested remote calls to prevent a worker process explosion? On my machine, there's a max of 24 workers made for my 12 CPU resources available, which I found interesting. I would have expected more variance."}
{"question": "Hello everyone. Have anyone tried to use the Pytorch profiler within a distributed ray application?"}
{"question": "Hi everyone, are there any tutorials or documentations providing examples to use Ray with SQLAlchemy?  I can\u2019t find much out there on the Internet."}
{"question": "Hi team,\nCurrently, I'm trying to use <https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#remote-uris|Remote URIs> to specify runtime_env. However, I can't make it work. Do you have an example using it? Example code and an example zipped env.\nThanks"}
{"question": "Hello everyone. I am new Ray user, I focus on Workflows module. At the beginning Workflow Managment section there is information to explicitly assign each workflow an id via .run(workflow_id=\"id\"). I've setted up in init \"local/path/to_folder\" and let's assume that I want to monitor job and get the logs from every run. Do you know is it possible to append logs instead of replacing it?  Thanks in advance"}
{"question": "What's the preferred way to set shared memory in a Docker container? I tried `-v /dev/shm:/dev/shm` to let it use the host's shared memory, but that ended up causing all my worker nodes to crash (having run out of RAM) before spilling to disk. It appears `--shm-size=$(expr $(grep MemTotal /proc/meminfo | awk '{print $2}') / 1024 / 3)mb` might work, but it seems very awkward? I'd rather not set to a static number as I'm experimenting with different VM sizes and I just want a sane default for everything."}
{"question": "Hey guys, I\u2019m running into some issues with launching clusters in AWS with certain ami\u2019s. With some AMIs, cluster launching gets stuck at\n```SSH still not available (SSH command failed.), retrying in 5 seconds.```\nand eventually times out. Any ideas on resolution strategies here?"}
{"question": "Hey guys, I am very new to the ray and trying to set up ray cluster to use it for distributed DL, using `RaySGD`.\nAfter cluster initialization, the training started using `TorchTrainer` . But when I checked GPU utilization on the head node, there were only 1 GPU utilized among others. Moreover, when I checked the same thing on the worker node, no GPUs were utilized at all. What could be the problem?\n\u2022 Version of ray: `2.0.0.dev0`\n\u2022 My `config.yaml` is next:\n```cluster_name: nikita-cluster\n\nmax_workers: 1\n\nupscaling_speed: 10000\n\nprovider:\n    type: gcp\n    region: asia-east1\n    availability_zone: asia-east1-a\n    project_id: &lt;PROJECT_ID&gt;\n\nauth:\n    ssh_user: ubuntu\n\navailable_node_types:\n    node:\n        min_workers: 1\n        max_workers: 1\n        resources: {\"CPU\": 32, \"GPU\": 2}\n        node_config:\n            machineType: n1-standard-32\n            disks:\n              - type: PERSISTENT\n                initializeParams:\n                   sourceImage: &lt;SOURCE_IMAGE&gt;\n                   diskSizeGb: 100\n                boot: True\n                autoDelete: true\n              - type: PERSISTENT\n                mode: READ_ONLY\n                source: &lt;SOURCE&gt;\n                boot: false\n                autoDelete: false\n            guestAccelerators:\n                - acceleratorType: projects/&lt;PROJECT_ID&gt;/zones/asia-east1-a/acceleratorTypes/nvidia-tesla-p100\n                  acceleratorCount: 2\n            scheduling:\n              - preemptible: true\n              - onHostMaintenance: TERMINATE\n            serviceAccounts:\n              - email: &lt;EMAIL&gt;\n                scope:\n                  - &lt;SCOPE&gt;\n\nhead_node_type: node\n\nsetup_commands:\n  - pip install timm tensorboard yacs\n  - ray install-nightly\n\n\nhead_start_ray_commands:\n    - ray stop\n    - &gt;-\n      ulimit -n 65536;\n      ray start\n      --head\n      --port=6379\n      --object-manager-port=8076\n      --autoscaling-config=~/ray_bootstrap_config.yaml\n\nworker_start_ray_commands:\n    - ray stop\n    - &gt;-\n      ulimit -n 65536;\n      ray start\n      --address=$RAY_HEAD_IP:6379\n      --object-manager-port=8076```\n\u2022 Trainer configuration:\n```TorchTrainer(training_operator_cls=RayTrainingOperator,\n                           num_workers=1,\n                           use_gpu=True,\n                           config=operator_config,\n                           backend=\"auto\",\n                           scheduler_step_freq=\"epoch\")```\n\u2022 Output from dashboard, `nvidia-smi` on head-node, `nvidia-smi` on worker-node:\n"}
{"question": "Does Raylet support graceful shutdown? Ideally, when Raylet catches *SIGTERM* signal, I hope it can evict tasks and actors on that node and proactively transfer workloads to other nodes instead of losing them via detection and then bring them up."}
{"question": "Can a `ray worker` communicate with python process? More specifically, I am dealing with an RL environment. The environment is unpickable thus cannot instantiate it in a `ray worker` . But it can run in a separate python process. Would it be possible that a `ray worker` receive the environment steps from a process (possibly through `pipe`), where the process itself takes care of the underlying environment? Is this theoretically sound?"}
{"question": "Has anyone tried reporting the prometheus metrics to datadog on an aws ray cluster?"}
{"question": "\u201cTo scale up, it is possible to instantiate more global schedulers which all share the same info because of the GCS.\u201d\n\nI am trying to understand if global scheduler can be extended in theory or in practice? Seems it uses sharding underneath? In that case, if one scheduler goes down, does it mean the nodes belong the corresponding partition will not get new tasks/actors?"}
{"question": "hi there, has anybody been having issues with apt keys when trying to apt-update on the following docker image? `<http://docker.io/rayproject/ray:1.11.0-py38-gpu|docker.io/rayproject/ray:1.11.0-py38-gpu>`"}
{"question": "Hi there, I was using ray to run some tasks and once there were not enough memory or cpu, just like the following snippets, I come up with the information to show the task cannot be scheduled. However I may get confused by the task id because I don\u2019t know *what exact function the task is related to,* is there any way that I can get kind of that relations by the ray cli?\n```import ray\n\n@ray.remote(memory = 10 * 1024 * 1024 * 1024)\ndef func():\n\timport time\n\ttime.sleep(5)\n\treturn 1\n\nray.init(address=\"auto\")\n\nresult = func.remote()\n\nprint(ray.get(result))```\nthe return seems like this:\n```2022-03-29 21:13:07,044\tINFO worker.py:842 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379\n2022-03-29 21:13:07,194\tWARNING worker.py:1245 -- The actor or task with ID 1a6b610341e28dd6a2d152273969d65555302d6301000000 cannot be scheduled right now. It requires {CPU: 1.000000}, {memory: 524288000.000000 GiB} for placement, however the cluster currently cannot provide the requested resources. The required resources may be added as autoscaling takes place or placement groups are scheduled. Otherwise, consider reducing the resource requirements of the task.```"}
{"question": "Is there any way to re-run `setup_commands`, when configuration is up to date? I need to perform `git pull` for all nodes, but the `config.yaml` wasn't changed itself, so `setup_commands` are being skipped and pulling is not performed."}
{"question": "What's the best way to see where my ray system hangs? I tried `ray stack` but it seems it doesn't print out information regarding the current execution point."}
{"question": "Hi, how do I force ray to update the files on a remote cluster after changing them locally? I'm running ray in a docker container and `ray up` or `ray submit` doesn't update the files automatically. Adding  `--no-cache-config` doesn't help, either."}
{"question": "I tried \"tune.with_parameters()\" because error message is saying about this. but it is still not working. Anyone knows how to solve this problem?? (I am using Googlecolab pro+ environment)"}
{"question": "Hi there, `ray submit cluster.yaml script.py -- --arg1 --arg2 --arg3` runs `\n```python ~/script.py --arg1 --arg2 --arg3```\nwhat if i want to run `python3` on the server? (how to use ray submit in this case)\nsay\n```python3 ~/script.py --arg1 --arg2 --arg3```"}
{"question": "Hi there, I am setting up `1 cpu head node + 2 gpu worker nodes`. I am using `ray.init(address=\"auto\")` and it uses all machines. Is there any way of running only on `2 gpus` not on the cpu head node?"}
{"question": "Hello. When I use `ray up` with a single head and single worker IP, and then open the dashboard, how many hosts should I see?"}
{"question": "At the moment I just see one (the head). Should IO see the worker too?"}
{"question": "Hi there, I just using ray images `rayproject/ray:1.11.0` and find there are several symlink is misconfig in the anaconda3 directory, I\u2019m wondering if it will affect any other behaviors? the incorrect symlink is\n\u2022 `/home/ray/anaconda3/lib/libstdc++.so.6.0.21`\n\u2022 `/home/ray/anaconda3/pkgs/libgcc-7.2.0-h69d50b8_2/lib/libstdc++.so.6.0.21`\n\u2022 `/home/ray/anaconda3/pkgs/python-3.7.7-hcff3b4d_5/compiler_compat/ld`\n"}
{"question": "Hi everyone, I want to ask a quick research question and understand how to improve search in <http://docs.ray.io|docs.ray.io>:\n\u2022 When you search, do you ever need to view all results? Or the top results are enough?"}
{"question": "If I have a disk with multiple zones on GCP, f.e. (asia-east1-a, asia-east1-c), how should I define a `zone` field in the config?\n```source: projects/&lt;PROJECT_ID&gt;/zones/&lt;???&gt;/disks/disk```\nIf I choose one between them, I get: `The referenced disk resource cannot be found.`"}
{"question": "I have another question, if i want to use custom machineType (GCP), what must be the value of `node_config.machineType` field?"}
{"question": "Anyone know about what happens when multiple actors are calling another actor's method with where the number of simulations calls &gt; max_concurrency? Are the threads queued?"}
{"question": "We just published a Medium blog post about Ludwig\u2019s AutoML capabilities for text classification, which use RayTune under the hood. The capabilities were developed with heavy involvement from the community. Hope it\u2019s interesting: <https://medium.com/ludwig-ai/ludwig-automl-for-text-classification-7c1759f3b150>\n\n<https://www.linkedin.com/feed/update/urn:li:activity:6927292495370305536/>\n\n<https://twitter.com/w4nderlus7/status/1521525379516669952?s=20&amp;t=FrvvVBdQmXlOdXY_9avCng>"}
{"question": "Suppose I have set up a local cluster. I would like a colleague to use it as well. I used a config.yaml file to start it, but it contains my ssh_user and reference to private key in it.\nWhat would be the recommended way of sharing the cluster with my colleague?"}
{"question": "Hello team,\n\nI am new to Ray and trying to setup distributed training on an on-prem cluster. First of all Ray is amazing and greatly simplifies the setup for to run distributed training. I am able to run the training job for the first time. However, subsequent jobs fail with this error. Can someone help me debug this?  <@U02GYSVBK29>"}
{"question": "Hi - just a quick q \u2014 is there a way to specify task invocation-specific timeouts ?"}
{"question": "Hello everyone.  Anyone have used ray with MiG enabled GPU's (A100)?"}
{"question": "Hello, when autoscaling fast, with upscaling_speed: 99999, from 0 to 40 worker nodes, our head node raylet process crashes without logging the failure anywhere. The head node is not running out of resources at any point: 32 cpu + 128GB ram\nCan someone recommend how to debug it?"}
{"question": "Anyone have experience running Ray inside Docker-Compose environments?"}
{"question": "Is there a programatic way to determine the current Ray session ID?"}
{"question": "This message really means trouble ?\n\n```[2022-05-12 12:34:24,246 W 106650 106719] <http://reference_count.cc:1413|reference_count.cc:1413>: Object locations requested for 0087611caa22031cffffffffffffffffffffffff0100000002000000, but ref already removed. This may be a bug in the distributed reference counting protocol.```"}
{"question": "General question about ray core:\n\u2022 I push some object to the ray cluster\nFirst question: can this object be stored on any node?\nIf so:\n\u2022 I get a ref to that object from that put\n\u2022 I call a remote function with this ref\nSecond question: will the task be run on the node which stores this object?\nThanks! :pray:"}
{"question": "I might be missing something simple - is there any way to obtain a ray objref's address from `ray.put()` in one python script, then use `ray.get()` to obtain it from another python interpreter? Something similar to the builtin <https://docs.python.org/3/library/multiprocessing.shared_memory.html>"}
{"question": "I'm  performing an evaluation of Ray. We currently use multiprocessing.Pool and would like to also use ray.util.multiprocessing.Pool so it can be run across multiple nodes. It works exactly as I'd hope for about 19 of every 20 jobs, but on the 1 job where it doesn't our Python script hangs at the pool.map step. When I look at running processes I can see that, along with my hung Python script, there is what appears to be a hung Ray PoolActor. I have not been able to find anything in Ray's logs which helps me understand what I'm seeing. Does anybody have any ideas?\n\nRay Version: 1.12.0\nPython Version: 3.6.8\nOS: Oracle Linux 7.6\n\nIt's worth mentioning that my script runs thousands of processes which each complete within less than a second. I split those processes into batches (one per node) and then use Ray to send those batches to the nodes. On the nodes I use multiprocessing.Pool to actually run the processes. I found that using ray.util.multiprocessing.Pool to manage all of the processes across the nodes was slower than just using multiprocessing.Pool on a single node."}
{"question": "Question about Ray Summit. If I get a bundle pass does that get me into all three training sessions?"}
{"question": "Conceptual question here from a student- Does ray tune make sense within data parallel and/or model parallel training? Can anyone give an example/use case where ray tune would be used in both of these scenaraios?"}
{"question": "Hi team, when reading the code of ReporterAgent (<https://github.com/ray-project/ray/blob/6d09244a7ea0580125306723ed1b6446af572bbc/dashboard/modules/reporter/reporter_agent.py#L22>), I find it calls some functions in in core.generated as follows but I cannot find the corresponding codes. I was wondering if there could be some illustration about that part, i.e., the implementation logic and overall workflow? Thanks very much!\n```from ray.core.generated import reporter_pb2\nfrom ray.core.generated import reporter_pb2_grpc```\n"}
{"question": "Is it possible to have \"conv1Pad\" depend on \"conv1Ker\"? I want to do `\"conv1Pad\": tune.randint(0, (conv1Ker/2))`\n```config = {\n    \"conv1Out\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n    \"conv1Ker\": tune.sample_from(lambda _: randrange(1, 9, 2)),\n    \"conv1Str\": tune.randint(1, 5),\n    \"conv1Pad\": tune.randint(0, 5),\n    \"conv2Out\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n    \"conv2Ker\": tune.sample_from(lambda _: randrange(1, 9, 2)),\n    \"conv2Str\": tune.randint(1, 5),\n    \"conv2Pad\": tune.randint(0, 5),\n    \"poolKer\": tune.sample_from(lambda _: randrange(1, 9, 2)),\n    \"poolStr\": tune.randint(1, 5),\n    \"poolPad\": tune.randint(0, 5),\n    \"fc1Out\": tune.randint(2, 9),\n    \"fc2Out\": tune.randint(2, 9),\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"batch_size\": tune.choice([2, 4, 8, 16, 32]),\n    \"momentum\": tune.sample_from(lambda _: uniform(0.1, 0.9))\n  }```"}
{"question": "Hi, has anyone ever used `psutils` inside a ray to collect data from a remote function call?"}
{"question": "Is there a way to do something like this with ray tune?\n```# Build a model by implementing define-by-run design from Optuna\ndef build_model_custom(trial):\n    \n    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n    layers = []\n\n    in_features = 20\n    \n    for i in range(n_layers):\n        \n        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 18)\n        \n        layers.append(nn.Linear(in_features, out_features))\n        layers.append(nn.LeakyReLU())\n\n        in_features = out_features\n        \n    layers.append(nn.Linear(in_features, 2))\n    layers.append(nn.LeakyReLU())\n    \n    return nn.Sequential(*layers)```"}
{"question": "Hi, is Ray workflow stable?"}
{"question": "What's the best way to 'migrate' an ObjectRef from one process to another so it will survive when the task which called the remote function dies?"}
{"question": "Will autoscaler remove idle worker, if `min_workers` == `max_workers` == 1? By the way, how does exactly ray determine, that worker node is idle? Sorry for stupid question."}
{"question": "Hi! I recently saw an anyscale swe intern job posting on my school's job board. I am a big fan of ray and really wanna work there, and is there anyone that can connect me with the recruiters?"}
{"question": "Hi!\n\nI have a question regarding ray debugging. The context I\u2019m facing problem in is the following.\n\n&gt; Sorry I cannot provide a lesser TL;DR but I\u2019ll do my best to be as brief as possible.\n\nI have a computation directed acyclic graph (which is conceptually similar to ray\u2019s workflows) with a single source node that represents the input data. Each node is a ray decorated function that takes results from the previous steps as its parameters and, once completed, stores its result for later utilization by the coherent nodes.\n\nThere\u2019re some nodes that require GPU computations, and some nodes are CPU-only.\n\nGraph\u2019s used inside a generator that yields some intermediate results outwards (using `ray.get`).\n\nAlso, graph may work in ray and non-ray modes, and in the later case it uses the underlying function wrapped by ray decorator.\n\nNon-ray mode always works as expected and each node\u2019s computation time is upper-bounded by 90 seconds by a great margin.\n\nThe problem appears when I start using ray. Most nodes (both GPU/CPU-required) work fine but the last yielded nodes may freeze so that I have to kill the process (or, which I tried, the computation\u2019s stopped by a timeout of 600 seconds).\n\nAmong the last nodes there\u2019re two distinct sequential groups:\n- the first one consists of quite fast similar computations of rather small volumetric images;\n- the second one (a single node) is something heavier \u2014 inference with NN over big 2D image.\n\nSo, if the first group utilizes GPU \u2014 it freezes, but if it doesn\u2019t there\u2019s the second group that freezes.\n\nAlso, sometimes everything\u2019s computed normally in ray mode, but more frequently \u2014 it doesn\u2019t!\n\nAnd this is something I couldn\u2019t explain and fix as of now.\n\nSo, I\u2019d really appreciate if you may enlighten me on the following issues:\n- Could you suggest some of the reasons that may explain such behaviour?\n- To me it looks like there might be a problem in how ray releases GPU resources required for a node. Does it sound reasonable? Can I control the scheduler? What can be done to control the scheduler?"}
{"question": "Hi All,\n\nI'm sure this has been discussed somewhere but its really frustrating that I'm unable to find simple things on Ray documentation. I have created a cluster using a yaml file. Now I want to submit a job to run python script. I used to just  do this:\n```ray submit cluster.yaml myscript.py```\nAnd it works. But, now I want to pass on an argument to python script. So, this doesn't work. The new API is this:\n<https://docs.ray.io/en/latest/cluster/jobs-package-ref.html>\n\nThe very first example says:\n```ray job submit -- python my_script.py --arg=val```\nBut this doesn't work either because it asks for `address`. I don't find any explanation what is this address is? Or how to get this address. Or how to tell this command the cluster name? Or specify the yaml file as I used to? Its the same sentence repeated again and again:\n```--address &lt;address&gt;\nAddress of the Ray cluster to connect to. Can also be specified using the RAY_ADDRESS environment variable.```\nI hate to say this but it shouldn't take more than 1 min to find the answer to this very very basic questions? How to pass argument to a python scritp?\n\nBTW, I'm running everything on AWS. (edited)"}
{"question": "Hi,\nIs there any way for pip installs performed within the actor to be performed across the system and be kept after actor shutdown, or to trigger a system wide pip install from the head script after already calling init?"}
{"question": "anyway to speculate available resources on your local node and alocate resources accordingly?"}
{"question": "when spawning a process of some executable (which might require n GBs of memory) within a task \u2014 will that have any impact or (negative) side effect on  ray\u2019s resource allocation behavior?"}
{"question": "Hi, I am recently trying to tune a pytorch NN model by Ray tune. The dataset is super large and therefore loading it on each training trail process can consume a huge amount of RAM memory. Can I share the training data for each trail with only one piece of the data loaded? I am thinking of using making the dataloader a <@UUWBZMU76>.remote object, but I am not sure how to implement this with ray tune. Any suggestions?"}
{"question": "I've been looking into using <https://github.com/containerd/stargz-snapshotter> for faster pulls of images (these machine learning containers are massive). Is anyone using this already that has suggestions for combining it with Ray cluster? The Ray cluster using Docker does an explicit image pull which would mean I can't rely on it to lazy load - is there a better alternative (k8s) which would make it possible to throw this in?"}
{"question": "hey, i am trying to retrieve an object reference and actor from the hex hash, anyone know how to do this?"}
{"question": "Hi all, I\u2019m trying to understand the worker management in ray, and there is a prestart function for  starting a worker per cpu,\n```/// Try to prestart a number of workers suitable the given task spec. Prestarting\n  /// is needed since core workers request one lease at a time, if starting is slow,\n  /// then it means it takes a long time to scale up.\n  ///\n  /// \\param task_spec The returned worker must be able to execute this task.\n  /// \\param backlog_size The number of tasks in the client backlog of this shape.\n  /// \\param num_available_cpus The number of CPUs that are currently unused.\n  /// We aim to prestart 1 worker per CPU, up to the the backlog size.\n  void PrestartWorkers(const TaskSpecification &amp;task_spec,\n                       int64_t backlog_size,\n                       int64_t num_available_cpus);```\nI may feel a little confused that why it would take a long time for scaling up?"}
{"question": "Trying to run a docker image in docker-compose: I get this error:\n```ERROR: for logging_sdk_1  Cannot start service sdk: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \"ray\": executable file not found in $PATH: unknown\n\nERROR: for sdk  Cannot start service sdk: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \"ray\": executable file not found in $PATH: unknown```\nI\u2019m running ray and python from within a USER folder .. Anyone familiar with this one?&gt;"}
{"question": "Is there a concept in ray similar to specifying worker in dask?\n```# dask\nclient.submit(f, x, workers='127.0.0.1')\nclient.submit(f, x, workers='127.0.0.1:55852')\nclient.submit(f, x, workers=['192.168.1.101', '192.168.1.100'])```\nI need the assigned task to be done on the specified worker.\nAny help, thanks!"}
{"question": "ray==1.12.1\nI ran scripts from tutorial with old and new version of RaySGD and got different GPU memory utilization patterns.\n\n1. <https://medium.com/distributed-computing-with-ray/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-27175a1b4f25|Getting Started with Distributed Machine Learning with PyTorch and Ray | by Michael Galarnyk | Distributed Computing with Ray | Medium> (RaySDGv1)\n*<https://aws1.discourse-cdn.com/business7/uploads/ray/original/2X/6/67fa771905f80b2329753aea83f8cfb6e9cb4061.png|image 1545\u00d7383 106 KB>*\n 2. <https://docs.ray.io/en/latest/train/examples/train_fashion_mnist_example.html|train_fashion_mnist_example \u2014 Ray 1.12.1> (RaySGDv2)\n*<https://aws1.discourse-cdn.com/business7/uploads/ray/original/2X/b/bb96271588227423c54043afd25bb0cdd97cddeb.png|image 1536\u00d7331 76.8 KB>*\nSo I have two questions:\n\u2022 Am I right, that RaySGDv1 uses DataParallel? Because we can see that separate GPU on head node utilizes more memory, than other GPU-workers;\n\u2022 Why, when using RaySGDv2, we don\u2019t have uniform memory distribution between all worker-GPUs across cluster? Is it normal behaviour?"}
{"question": "Running a ray cluster with workers on spot instances in EC2 we're observing a lot of ObjectLostError and OwnerDiedError for some long-running computations. What are the best practices we could employ to make the objects more resilient to node failures?"}
{"question": "Couls someone help me please? I am following the example of the Documentation"}
{"question": "Hi guys, I'm new here. I have <https://discuss.ray.io/t/parallel-detectron2-pytorch-inference-with-gpu/6267?u=james811223|question>, and wondering if anyone knows a solution for it?"}
{"question": "Hi everyone, I am trying to use ray for a distributed hyper parameter tuning job on gcp. The cluster creation with `ray up` is successful, but when I submit the job with `ray submit config.yaml script.py -- --arg1=...`   I got the following error, any ideas?\n```2022-05-26 08:43:53,738 INFO util.py:335 -- setting max workers for head node type to 0\nChecking GCP environment settings\n2022-05-26 08:43:58,984 INFO config.py:485 -- _configure_key_pair: Private key not specified in config, using/home/nqin/.ssh/ray-autoscaler_gcp_us-central1_radmontecarlo_ubuntu_0.pem\nFetched IP: 35.184.218.65\nShared connection to 35.184.218.65 closed.\nShared connection to 35.184.218.65 closed.\nError: SSH command failed.```"}
{"question": "Another problem is sometimes the trial stopped with the following error\n```Failure # 1 (occurred at 2022-05-25_23-00-03)\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 901, in get_next_executor_event\n    future_result = ray.get(ready_future)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1809, in get\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError: [36mray::WrappedDistributedTorchTrainable.train()[39m (pid=5470, ip=10.128.0.5, repr=&lt;ray.tune.integration.torch.WrappedDistributedTorchTrainable object at 0x7fdeebbf2be0&gt;)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/tune/trainable.py\", line 349, in train\n    result = self.step()\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/tune/integration/torch.py\", line 128, in step\n    result = ray.get([w.step.remote() for w in self.workers])[0]\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.```\nor\n```Failure # 1 (occurred at 2022-05-25_23-00-03)\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 901, in get_next_executor_event\n    future_result = ray.get(ready_future)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1811, in get\n    raise value\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.```\nsame question was asked here <https://discuss.ray.io/t/rayactorerror-the-actor-died-unexpectedly-before-finishing-this-task/3149> but no answers yet. Any ideas?"}
{"question": "Hi, I am trying to build my own custom ray worker disk image on Google Cloud. The issue I am currently facing is that the cluster can't recognize these nodes once they are launched. So essentially the creation of the VM happens successfully (by the auto scaler), but the nodes never are registered on the ray dashboard, and jobs can't run on them.\n\nAs far as ray dependencies this is what I have installed on the worker nodes:\n\npip freeze | grep ray\nlightgbm-ray==0.0.2\nray==1.11.0\nray-cpp==1.11.0\nray-lightning==0.2.0\nxgboost-ray==0.1.9\n\nI should mention that my intention at the moment is to run ray processes directly on the worker node (as opposed to inside a container). So I have removed the 'docker:' section of the cluster configuration part.\n\n\nSome relevant information:\n\ncloud provider: Google Cloud\nzone: us-central1-b\nworker node machine type: n1-highmem-8\nThe global base image I used: pytorch-1-11-cu113-notebooks-v20220516-ubuntu-2004\n\nAny thoughts on what I am missing? Happy to provide more information."}
{"question": "Hi all, I\u2019m reading the <https://docs.ray.io/en/master/ray-core/objects/memory-management.html#concepts> and I feel a little confused about the image, because 1) in the following instructions there may be some overlap in worker heap and object store because in the following part I see \u201c*if an object is already present on the node, this does not cause additional allocations.\u201d* 2) I feel like that the available memory is the blank part, not including the work heap. Is my conclusion correct or is there any misunderstanding?"}
{"question": "Hi all, I\u2019m doing some test in my local environment, I\u2019ve set a limit for the memory usage of the container: `docker run -it *-m 3g* --cpus=\"2\" -p 38265:8265 -p 30080:8080 -e RAY_enable_worker_prestart=false rayproject/ray:1.12.1 ray start --head --dashboard-host=0.0.0.0 --object-store-memory=2147741824 --block` but the dashboard  still get the ram and disk limit of the whole machine: I would like to know if it\u2019s better that when we run ray on container the dashboard would record the metrics by the limit of the container? If necessary, I would like to contribute for this feature."}
{"question": "is ray autoscaler not removing idle workers (terminated/errored tune trials) by default?"}
{"question": "Hi all,\n\nCan anyone point me to a roadmap for Ray development and where the functionality and features in upcoming releases are captured?\n\nCheers,"}
{"question": "Hello everyone!\nI want to implement the new inference service function by rewriting Ray serve, what is the easiest way to do it? Is it possible to extract the python package of ray serve and implement it separately?"}
{"question": "Are there any examples for <https://docs.ray.io/en/latest/train/examples/train_fashion_mnist_example.html> but using Ray Cluster to launch it distributed on AWS?"}
{"question": "I'm still getting an error initializing ray, even when I use the ignore_reinit_error=True in the ray.init() statement. It even informs me that it passes that parameter to the ray server, but still have to restart the kernel (doing this from inside a Jupyter Notebook and ray is running on a different server than the notebook)\n\n`ray.init(address='<ray://10.112.80.167:10001>', ignore_reinit_error=True)`\n\n```2022-05-31 13:44:41,762\tINFO client_builder.py:225 -- Passing the following kwargs to ray.init() on the server: ignore_reinit_error\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;timed eval&gt; in &lt;module&gt;\n\n/tmp/ipykernel_119078/1514682196.py in main(args)\n     97     ray_str = {'address':f'ray://{args.ray_head}:10001'} if args.ray_head else {'address':f'ray://{get_ray_head()}:10001'}\n     98     <http://logger.info|logger.info>(f'Starting RAY with this address: {ray_str}')\n---&gt; 99     ray.init(**ray_str,ignore_reinit_error=True)\n    100 \n    101     file_prefix = args.results_path if args.results_path else os.getcwd()\n\n/usr/local/lib/python3.7/site-packages/ray/_private/client_mode_hook.py in wrapper(*args, **kwargs)\n    103             if func.__name__ != \"init\" or is_client_mode_enabled_by_default:\n    104                 return getattr(ray, func.__name__)(*args, **kwargs)\n--&gt; 105         return func(*args, **kwargs)\n    106 \n    107     return wrapper\n\n/usr/local/lib/python3.7/site-packages/ray/worker.py in init(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, _enable_object_reconstruction, _redis_max_memory, _plasma_directory, _node_ip_address, _driver_object_store_memory, _memory, _redis_password, _temp_dir, _metrics_export_port, _system_config, _tracing_startup_hook, **kwargs)\n    880         passed_kwargs.update(kwargs)\n    881         builder._init_args(**passed_kwargs)\n--&gt; 882         return builder.connect()\n    883 \n    884     if kwargs:\n\n/usr/local/lib/python3.7/site-packages/ray/client_builder.py in connect(self)\n    162             job_config=self._job_config,\n    163             _credentials=self._credentials,\n--&gt; 164             ray_init_kwargs=self._remote_init_kwargs,\n    165         )\n    166         get_dashboard_url = ray.remote(ray.worker.get_dashboard_url)\n\n/usr/local/lib/python3.7/site-packages/ray/util/client_connect.py in connect(conn_str, secure, metadata, connection_retries, job_config, namespace, ignore_version, _credentials, ray_init_kwargs)\n     23     if ray.is_connected():\n     24         raise RuntimeError(\n---&gt; 25             \"Ray Client is already connected. Maybe you called \"\n     26             'ray.init(\"ray://&lt;address&gt;\") twice by accident?'\n     27         )\n\nRuntimeError: Ray Client is already connected. Maybe you called ray.init(\"ray://&lt;address&gt;\") twice by accident?```\n"}
{"question": "Hi all, I am trying to programmatically query the resources currently available on a Ray cluster (total - in use). So far I have the following code:\n```ray.state.state._check_connected()\nmessages = (ray.state.state.global_state_accessor.\n            get_all_available_resources())\nresources = map(\n    lambda message: dict(\n        ray._private.gcs_utils.AvailableResources.FromString(message).\n        resources_available.items()), messages)```\nIs there a way to do this without relying on private APIs? Is there an API to query the available resources for the current node only?"}
{"question": "In my config.yaml file, I am using docker, and the image is hosted locally.\n\nIn order to pull from the local docker repo, I need to update my daemon.json file, to include my local repo's IP as an insecure registry, as described <https://docs.docker.com/registry/insecure/|here>\n\nIs there a section of the config.yaml that can automatically copy over a suitable daemon.json file to each worker node and restart the docker service? From what I see, the setup commands will execute within the each docker container, rather than directly on the worker nodes."}
{"question": "I want to make a workflow system for network configuration. The main functions are as follows:\n1. Supports serial, parallel, serial-parallel combination of tasks\n2. Support sub-workflow\n3. With UI, drag and drop to generate workflow\n4. Distributed deployment, agent/worker as close to the device as possible\n\nShould I use *ray.core* or *ray.workflows*, or other options? thank you for your advice."}
{"question": "Are there instructions somewhere for how to install ray in \"developer mode\"? I want to make some changes to the python and try them out locally before submitting a PR."}
{"question": "Hi, I am new to rllib and just started training agents using rllib recently. I have a question regarding deployment of trained agents: is there any way to take trained agent using latest version of rllib and convert into TensorFlow model file so that it be deployed on any TensorFlow capable system but without rllib installation where it' not possible? is not does rllib is available for embedded systems?"}
{"question": "I'm working with Ray Workflows, and I'm trying to find an example of a conditional workflow, does anyone have an example they are willing to share? Is that type of functionality available with Ray workflows?"}
{"question": "Hello, I am working with rllib. I train policy in multiagent env and want to load the policy for test. Is there any suggestion for model loading in multiagent env? Thanks!"}
{"question": "I have an even more general question than <@U03AYNL0MEH> above with regards to the `ray job submission` api.\n\nIs there a channel required for `ray job submission` questions, or are we just posting to this genera; (or indeed another) channel?"}
{"question": "what is the relationship between Ray placement groups and AWS/EC2 Placement Groups?"}
{"question": "Hi all, just got introduced to Ray and I\u2019m excited to be using it. I have a question for using Ray Tune on multiple clusters of nodes via a manager like Slurm. I was able to find some examples in the docs that mention how to do this for training, but I haven\u2019t found any examples of this for parameter tuning. My workflow right now is using Pytorch Lightning. Can the Ray Tune library be distributed as well? If so, can someone point me in the right direction? Thanks!"}
{"question": "hey, is it possible to install nightly ray releases with pip?\n\nI tried with `pip install git+<https://github.com/ray-project/ray.git@c1afbcb6f4ef3b4217be33704f8f5135472da674#subdirectory=python>` but get some error about bazel missing.\n\nI'm assuming there will be more dependencies like this and may be more trouble than it's worth on that approach.\n\nalso tried with `SKIP_BAZEL_BUILD=1 pip install git+<https://github.com/ray-project/ray.git@c1afbcb6f4ef3b4217be33704f8f5135472da674#subdirectory=python>`  but get the same error"}
{"question": "Hello,\n\nl use `tune.run()`   to find the best width and depth of my network. However, some configurations lead to `out of memory`   and l get the following ray tune error:\n\n`trial_runner.py:1318 -- Blocking for next trial...`\n I have about 300 configurations to run. I would to continue running all the configurations even if we encounter configurations that lead to error. Is there any parameter in `tune.run()`  to do that ?\nI set `max_concurrent_trials=1`\nThank you"}
{"question": "Can ray be used in PyTorch 1.10.10 and Python 3.10.x? I keep finding that it does not have right version or something.\nOn Ray website, there is not wheel for 3.10.x either."}
{"question": "Hello everyone, how to install ray nightly version locally? I tried this command but it didn't work\n```pip install -U 'ray[default] @ ray-2.0.0.dev0-cp37-cp37m-macosx_10_15_intel.whl'```\nAnother question: how to config the timestamp of logs in ray dashboard? It shows the different time zone than mine: e.g. \"2022-06-07 12:53:49,756\" my time is \"2022-06-07 21:53:49\""}
{"question": "Hello everyone, now I just build ray from source code using the following script.\n```cd ray\\python\npip install -e . --verbose```\nEach time I rebuild the ray project, it will compile source code repeatedly though I change nothing. Is there any way to avoid repeatable compiling of the unchangable source code?"}
{"question": "Hello, I just start using Ray to do some image embeddings remotely. Im using FastAPI for API and three machines A, B and C where A is the API, B is the head node and C is a worker node. I have connected from A to B using `ray_init()` via python, and connected C do B with `ray start --address='xxx'`. On my dashboard I can see my head node and one worker (C machine). Now, when I was using only machines A and B (i didnt connect the C worker yet) all was working very well. I had my remote functions and I did some tasks on head node. My API uses file `tools.py` which is just a python file I wrote. I used `runtime_env` with `work_dir` and `tools.py` is in the same folder as my `main.py` file. All working good, my head node recieves the task and has no problems with imports from `tools.py`  but when i connected worker node (machine C) it just throws `ModuleNotFound` . I've already tried `runtime_env` with workdir, with py_modules, and tried to do it in a 2137 different ways :smile: But none of it was successfull. What can I do to achieve my goal? I dont want to copy my `tools.py` to every worker machine i intend to use."}
{"question": "I'm trying to get my ray service to show up in the anyscale service tab. I'm able to pretty easily spin up a cluster, but I would like the ability for the DNS not to change when I restart the service. Could anyone point to me documentation on how to do this I can't seem to find any? Here is my example code that works well\n\n```import ray\nfrom ray import serve\nfrom fastapi import FastAPI\n\nray.init(address = '<anyscale://cluster-name>', autosuspend=-1)\nserve.start(detached=True)\napp = FastAPI()\n\n@app.get(\"/\")\ndef get():\n    return {\"count\": 1}\n\n@app.get(\"/incr\")\ndef incr():\n    return {\"count\": 2}\n\n@app.get(\"/decr\")\ndef decr():\n    return {\"count\": 3}\n\n@serve.deployment(route_prefix=\"/\")\n@serve.ingress(app)\nclass FastAPIWrapper:\n    pass\n\nFastAPIWrapper.deploy()```"}
{"question": "Hello community, i am wondering if there are any good practices on using ray to perform a map operation with super big file?  for some reason we have to process files with size can go up to 1-2 Tb.  We were using the actor pool pattern to map on files, which clearly won\u2019t finish processing big files in reasonable time. Not sure about the internals of ray dataset. would it try to load the file in memory once before some shuffling ?"}
{"question": "Hey all, what's the best channel (or forum) to ask questions about Ray Workflows?"}
{"question": "Is there a progress bar for ray core?\n\nI started a bunch of futures, and would like to see the progress on them\n```futures = [do_something.remote(i) for i in range(1000000)]\nresults = ray.get(futures)```\nFor now, I can do something like this, but I am not sure if this is equivalent, or less efficient than the previous approach:\n```futures = [do_something.remote(i) for i in range(1000000)]\nresults = [ray.get(f) for f in tqdm(futures)]```"}
{"question": "It looks like Ray 1.13.0 just released about half an hour ago (:tada: ), will the docs be available later today?"}
{"question": "this might be a long shot but has anyone gotten ray to work on compute canada resources? It seems installing ray in a virtualenv gives me this error\n```+ srun --nodes=1 --ntasks=1 -w gra1157 ray start --address 10.29.96.9:6379 --num-cpus 6 --num-gpus '' --block\n\nslurmstepd: error: execve(): ray: No such file or directory```"}
{"question": "I have setup ray on an SGE cluster and some of the jobs I have run on the workers cause the SGE cluster to kill the nodes that are running the jobs and I see these messages:\n\n```The node with node id: 2239113ac858d18f418ee58c24eb6862098d5d5b771c0a42b6c0d507 and ip: 10.112.80.196 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a raylet crashes unexpectedly or has lagging heartbeats.```\nBut the jobs that are running on those nodes are not flagged as failed, so ray is still seems to wait for the jobs to finish, so it is waiting for ever. Is there a timeout I can set that will ensure that the jobs on these dead nodes are flagged as failed so I can intercept the Exception?"}
{"question": "I have another question about usage. Suppose I have a list of Pandas dataframes, and I want to send these dataframes to remote function for computation:\n\n```dfs = [...]\nfutures = [process.remote(df) for df in dfs]\nresults = ray.get(futures)```\nWould it be more efficient to do this, or would it be the same?\n```dfs = [...]\ndfs_future = [ray.put(df) for df in dfs]\nfutures = [process.remote(df_f) for df_f in dfs_future]\nresults = ray.get(futures)```"}
{"question": "Hello everyone. I have a Ray cluster v1.12.1 running on Openshift. Using the Ray lib (submitting jobs) remotely works fine. Executing CLI \u201cray status\u201d on head/workers works fine, but running \u201cray status\u201d remotely with \u201c--address\u201d argument had me puzzled. The value \u201c<ray://headNode:10001>\u201d gives me  \u201cDNS resolution failed for service\u201d error. The value \u201cheadNode:10001\" gives me \u201c\u201dgrpc_message\u201d:\u201cMethod not found!\u201c,\u201dgrpc_status\u201d:12}\u201c. Anyone knows what the --address value should be?"}
{"question": "Hi guys,\nAnyone knows how I would be able to configure ray serve to log at `logging.ERROR` logging level?\nHere is my script"}
{"question": "Hi guys, now I want to learn the details of ray by reading the source code. Is there anyone know how could I *use gdb to debug* core_worker or raylet?"}
{"question": "Good morning. When I try to submit a ray job, it tells me:\n\nRuntimeError: The Ray jobs CLI &amp; SDK require the ray[default] installation: `pip install 'ray[default']``\n\nHowever, I cannot run that (I'm on Linux), as it freezes while installing gpustat.\nThe comment in the setup.py file is that gpustat is for Windows.\n\nHow can I workaround this?"}
{"question": "Hi! Is it possible to use variables in the cluster config yaml file? (Something like {{ var }} in Ansible)"}
{"question": "Hi. I want to run multiple instances of ray-tune for optimization. (I have different datasets hence, essentially I have to start different runs entirely). In such a case, the tune.checkpoint_dir is causing issues since the models seem to be overwriting each other and thereby failing these multiple instances. What would be the way to ensure these checkpoint_dir are completely distinguished from each other?"}
{"question": "Hi, I am blocked by a problem again:face_with_head_bandage:. Is there anyone run java application under a ray cluster? I read the docs and find that we need provide *ray.job.code-search-path* when we run java app under ray cluster, but *I am not sure what jars the ray.job.code-search-path should contains.* Is there anyone could explain? Thanks."}
{"question": "Is there a way to do file_mounts with host-only, instead of with the docker? the use case is an Azure cluster that needs to connect to AWS, thus needing a credentials file, for AWS ECR to login to work it needs to be put in initialization_commands and file mounts are not available"}
{"question": "I am looking at the horvod and pytorch elastic blogs and they seem to claim that they are able to leverage ray and run elastic distributed training jobs. does this mean that you can start a raycluster with min/max workers, the cluster can start with min workers. then you can launch the horovod/torchx job with min/max number of nodes for those jobs and then they can scale up/down as and when ray scales the cluster up and down. is that a fair understanding?"}
{"question": "Hi! Could anyone take a look at this topic? <https://discuss.ray.io/t/timestamps-of-logs-on-dashboard-is-wrong/6481> Thanks!"}
{"question": "Hi! Could anyone take a look at this topic?\n<https://discuss.ray.io/t/using-ray-core-actors-on-multiple-cpu-core-at-same-time/6511>"}
{"question": "Hi there! As I am new to Ray, can it be that the whole ray-Api has gone through a fundamental change (I am using 1.13 now), e.g. I wanted to check on the XGBoost Example, but it seems that the examples in the upcoming book \"Learning Ray\" and on the website are outdated. See here the example <https://docs.ray.io/en/master/ray-air/examples/xgboost_example.html>\nIt seems that XGBoost has been extracted out of the ray.train branch to  the xgboost_ray library , is that true ? Furthermore I would like to contribute, how and where can I contribute to PR and Issues ?"}
{"question": "While using Ray on Slurm, it refuses to fully utilize my available CPUs. It never goes above ~250 cpu cores out of 2,000. Am I missing *any hidden settings about max concurrency* or similar?\n\nI can replicate this low CPU utilization \u2018bug\u2019 even when each @ray.remote tasks does simple pure-python math calculations before exiting. Thanks!"}
{"question": "Does each ray worker run its own Python interpreter? If so, then Python\u2019s GIL shouldn\u2019t be an issue, but then it\u2019s a little unclear the purpose of <https://docs.ray.io/en/master/ray-core/actors/async_api.html#asyncio-concurrency-for-actors|AsyncIO for Actors>\u2026"}
{"question": "Anyone experienced this when creating a cluster in GCP?\n```  [5/7] Initalizing command runner\nShared connection to 34.83.27.133 closed.\nlatest-cpu: Pulling from rayproject/ray\nd5fd17ec1767: Pull complete\n341eeba1871f: Pull complete\n913d7f86391e: Pull complete\n2107148d3c4b: Pull complete\nd0a777325523: Pull complete\n1dc2e272e6b0: Pull complete\ndf3e0297f017: Pull complete\nda37cde4b300: Pull complete\n9d617ad85b1c: Pull complete\nDigest: sha256:7831c923acc3b761f52ac9cfa8d449d3b8ad02d611cf4615795c08982bc41c9d\nStatus: Downloaded newer image for rayproject/ray:latest-cpu\n<http://docker.io/rayproject/ray:latest-cpu|docker.io/rayproject/ray:latest-cpu>\nShared connection to 34.83.27.133 closed.\nShared connection to 34.83.27.133 closed.\nShared connection to 34.83.27.133 closed.\nShared connection to 34.83.27.133 closed.\nShared connection to 34.83.27.133 closed.\nec362cc862f1df9528957c52c02bc03aeb2c2124f6b93f96abbae7c316802f58\nShared connection to 34.83.27.133 closed.\nShared connection to 34.83.27.133 closed.\nprotocol version mismatch -- is your shell clean?\n(see the rsync man page for an explanation)\nrsync error: protocol incompatibility (code 2) at compat.c(178) [sender=3.1.3]\nShared connection to 34.83.27.133 closed.\n2022-06-15 23:55:47,351 INFO node.py:311 -- wait_for_compute_zone_operation: Waiting for operation operation-1655355350012-5e189739d51a2-082ecc50-ad0ff17c to finish...\n2022-06-15 23:55:52,677 INFO node.py:330 -- wait_for_compute_zone_operation: Operation operation-1655355350012-5e189739d51a2-082ecc50-ad0ff17c finished.\n  New status: update-failed\n  !!!\n  SSH command failed.\n  !!!\n\n  Failed to setup head node.```\nIt was working well two weeks ago. Maybe some changes on GCP side causes the error?"}
{"question": "Is there a python API equivalent to `ray start --head` or is the recommended way to start a single-node cluster from python still something like `subprocess.run([\"ray\", \"start\", \"--head\"])`?"}
{"question": "Hello, I am looking to contribute a big fix to Ray. The contributing instructions don\u2019t state branch naming conventions. Additionally is there any permissions I need granted to push a local branch? Thanks!"}
{"question": "Hi all, I\u2019m noticing the following two log lines being emitted often in `raylet.out` on my Ray cluster when running large Ray Tune runs or many concurrently. My Ray Tune objective functions spin up a between 5\u201350 remote child tasks. Are these something to be worried about? And if so, is there a workaround to fix this? I found <https://ray-distributed.slack.com/archives/C01DLHZHRBJ/p1637523126013500|this related thread> which maybe suggests that the first log line might have something to do with placement groups? If it\u2019s helpful, I\u2019m using a `PlacementGroupFactory` which 1 bundle for each of the remote child tasks (i.e. 5\u201350 1 CPU bundles depending on the # of remote child tasks).\n```[2022-06-19 13:29:40,896 W 58 58] (raylet) <http://task_spec.cc:50|task_spec.cc:50>: More than 228 types of tasks seen, this may reduce performance.\n[2022-06-19 13:29:41,076 I 58 58] (raylet) <http://object_buffer_pool.cc:153|object_buffer_pool.cc:153>: Not enough memory to create requested object 00ffffffffffffffffffffffffffffffffffffff0600000002000000, aborting```\nIs the second log line at all related to the Plasma store? The Plasma store is hardly used from looking at the Ray dashboard (&lt;5% utilization at any given time) and I\u2019m not seeing any object spilling being triggered. Thanks in advance for your time!"}
{"question": "Hi Everyone, i\u2019m presently working to introduce ray as a replacement of our internal orchestration system for ETL and MLflows, i found equivalent replacement would be \u2018ray workflows\u2019 , how ever its still in alpha stage , do we have any stable working solution already available to port existing code with minimal changes ?  Thanks in advance"}
{"question": "Hey, anyone used ray to accelerate dagster pipelines?\n\nI'm thinking that i can use ray in my asset definitions and then use the plasma store as an iomanager. I'm not sure if this will work though since I think that after materialisation the process will finish and the ref counter will get decremented and this will result in garbage collection of the object.\n\nI am not super confident in my Ray or Dagster knowledge here so this may just work.\n\nOtherwise maybe an actor could be initialised to maintain the ref I guess? Any thoughts about this?"}
{"question": "Is there a maximum size to an object that can be `ray.put` when connected to a kubernetes cluster?\n\nI have a relatively large list that I need to access from a number of different tasks. When I try to run this using Ray locally I have no issues, but when I'm connected to a k8s cluster I get:\n```2022-06-20 18:44:10,223\tWARNING dataclient.py:363 -- Encountered connection issues in the data channel. Attempting to reconnect.\nLog channel is reconnecting. Logs produced while the connection was down can be found on the head node of the cluster in `ray_client_server_[port].out`\n2022-06-20 18:45:53,081\tWARNING dataclient.py:363 -- Encountered connection issues in the data channel. Attempting to reconnect.\n2022-06-20 18:46:30,931\tWARNING dataclient.py:363 -- Encountered connection issues in the data channel. Attempting to reconnect.\n2022-06-20 18:47:09,174\tWARNING dataclient.py:363 -- Encountered connection issues in the data channel. Attempting to reconnect.```\nAnd I'm unsure if it's ever going to finish uploading. The dashboard shows 0 usage of object store memory."}
{"question": "Hello, me and my team are trying to run a set of Ray jobs on Kubernetes cluster using rayDP and lgbm on ray. While running spark job, we are facing an issue stating\n```2022-06-22 05:12:47,443 WARN TaskSchedulerImpl [task-starvation-timer]: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources```\nCan someone help with what are we missing?\nAs such from the Kube cluster and Ray cluster, we have enough reso\\urces present.\nThe config of the rayDP :\n```spark = raydp.init_spark(\n  app_name = \"Ray_LGBM_Collab\",\n  num_executors = 2,\n  executor_cores = 2,\n  executor_memory = \"2GB\",\n)```\nCluster configs\n```c5.4xlarge : 6 nodes working (16 cores and 32GB RAM per node)```"}
{"question": "hey! x-posting <https://mlops-community.slack.com/archives/C015J2Y9RLM/p1655819530968389|from the MLOps community slack>\n\n<@UMQSXB6TD> can you please share to me some of the early insights about performance for batch inference jobs with Ray datasets? thanks, much appreciated :bow::skin-tone-2:"}
{"question": "Hello, what is the turnaround time for getting review usually after submitting a PR? "}
{"question": "Hey, any good pattersn to implement a generator ray function?\n\nthe naive solution\n```ray.init()\n@ray.remote\ndef func():\n    yield from [1,2,3]\n\nfor result in func.remote():\n    print(result)```\nresults in\n```2022-06-24 10:39:53,036\tERROR worker.py:92 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::func() (pid=1432811, ip=192.168.94.75)\n  File \"/home/oliver/miniconda3/envs/nvflare/lib/python3.8/site-packages/ray/serialization.py\", line 413, in serialize\n    return self._serialize_to_msgpack(value)\n  File \"/home/oliver/miniconda3/envs/nvflare/lib/python3.8/site-packages/ray/serialization.py\", line 391, in _serialize_to_msgpack\n    pickle5_serialized_object = self._serialize_to_pickle5(\n  File \"/home/oliver/miniconda3/envs/nvflare/lib/python3.8/site-packages/ray/serialization.py\", line 353, in _serialize_to_pickle5\n    raise e\n  File \"/home/oliver/miniconda3/envs/nvflare/lib/python3.8/site-packages/ray/serialization.py\", line 348, in _serialize_to_pickle5\n    inband = pickle.dumps(\n  File \"/home/oliver/miniconda3/envs/nvflare/lib/python3.8/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n    cp.dump(obj)\n  File \"/home/oliver/miniconda3/envs/nvflare/lib/python3.8/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 620, in dump\n    return Pickler.dump(self, obj)\nTypeError: cannot pickle 'generator' object```"}
{"question": "In case people here are interested --- Alpa (<https://github.com/alpa-projects/alpa>) is a Ray-derived project targeting large ML model (like GPT-3) training and serving; it was co-developed by several Ray developers! It can automatically parallelize the computation of large models, and it parallelizes it on top of a Ray cluster!\n\nFollow <https://alpa-projects.github.io/tutorials/opt_serving.html|this guide> to use Alpa to serve the largest open-source available language model OPT-175B. See a <https://www.linkedin.com/posts/a-gholami_response-from-facebooks-opt-30b-model-activity-6935960927045386240-nZkN?utm_source=linkedin_share&amp;utm_medium=member_desktop_web|fun post> about it!\n\nIf you find the project interesting, don't hesitate to star or fork our project :wink:. Please also help us upvote our <https://news.ycombinator.com/item?id=31852875|HN post>!"}
{"question": "<https://stackoverflow.com/questions/69613739/how-to-kill-ray-tasks-when-the-driver-is-dead#comment123050259_69613855>\n\nin this situation does the ray task get cancelled with the calling process?"}
{"question": "We've been seeing this same error:\n<https://discuss.ray.io/t/rllib-evaluation-rollout-socket-gaierror-errno-2-name-or-service-not-known/5766>\n\nFirst experienced this with Ray 1.11 on Ubuntu.  We tried upgrading to Ray 1.13, same error.\n\nHas there been any discussion about this error?  Is the only workaround to disable the Ray dashboard?"}
{"question": "Hi all, I've been checking out `placement_groups` , and I am wondering if there is a way for the tasks that run inside a single worker (of a PG) to know the metadata regarding the PG ex: rank, group size, etc?\n```ray.init(num_cpus=4)\n\npg = placement_group([{\"CPU\": 4}])\nray.get(pg.ready())\n\n@ray.remote(num_cpus=4)\ndef f():\n    rank = ...\n    pg_size = ...\n    return (rank, pg_size)\n\nprint(ray.get(f.options(placement_group=pg).remote())) # expected [(0, 4),(1, 4),(2, 4),(3, 4)]```"}
{"question": "anyone have a good pattern for finding what is causing unserialisable exceptions?\n\n```ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\ntraceback: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/ray/exceptions.py\", line 38, in from_ray_exception\n    return pickle.loads(ray_exception.serialized_exception)\nTypeError: __init__() missing 2 required positional arguments: 'errors' and 'config_value'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/ray/serialization.py\", line 332, in deserialize_objects\n    obj = self._deserialize_object(data, metadata, object_ref)\n  File \"/opt/conda/lib/python3.8/site-packages/ray/serialization.py\", line 258, in _deserialize_object\n    return RayError.from_bytes(obj)\n  File \"/opt/conda/lib/python3.8/site-packages/ray/exceptions.py\", line 32, in from_bytes\n    return RayError.from_ray_exception(ray_exception)\n  File \"/opt/conda/lib/python3.8/site-packages/ray/exceptions.py\", line 41, in from_ray_exception\n    raise RuntimeError(msg) from e\nRuntimeError: Failed to unpickle serialized exception```"}
{"question": "Hi all,\nI am running into an issue scaling ray on a server. I specify max_concurrent_trials and even resources_per_trial but my ray tune.run is using all of the processors on the server anyway.\n\nWhat I am doing wrong? Below is a small example of parameters that are passed into tune.run(objective, ...)\n\n```'parameters':{\n    \"num_samples\": 100,\n    \"search_alg\": HyperOptSearch(metric=\"compute_RPS\", mode=\"min\"),\n    \"scheduler\": ASHAScheduler(metric=\"compute_RPS\", mode=\"min\"),\n    \"resources_per_trial\": {\"cpu\": 1, \"gpu\": 0},\n    \"max_concurrent_trials\": 30\n}```"}
{"question": "Hi all! I'm new here, and I am being unable to handle the \"rayoutofmemory\" error. If there anyone knowing how to tackle it? Thanks a lot in advance! :slightly_smiling_face:"}
{"question": "Hi :wave: team, we have met the same issue several times.  here is the error logs, what does this mean?\n```[[/tmp/ray/session_latest/logs/raylet.out]] [state-dump] - num chunks received failed / plasma error: 0```"}
{"question": "Hello,\nI'm trying to solve regression problem with offline Reinforcement learning.\nI successfully approached classification with episodes = 1, and rewards = -1 when fail a guess and 0 when a right prediction take place. Also offline (database based as supervised problem)\nIm trying to approach regression where prediction is a real number, also offline (based on dataset) using PPO, with reward as euclidian distance between real and predicted. But when I train my agent it gets to a point where only propose two actions: 0 / 1.\nSomebody have some guidelines on how can i face this problem (regression)?\nI'd really appreciate any help\nBest regards\nAlberto"}
{"question": "Hi everyone,\n\nAt the moment I am using ray-core to run several experiments in parallel. The\ngoal is to submit the script to a GCP instance and let the autoscaler create and\nremove workers as needed.\n\nThe folder structure in my local machine looks something like this:\n\n```github_repository/\n\u251c\u2500 my_module/\n\u2502  \u251c\u2500 __init__.py\n\u2502  \u251c\u2500 some_useful_functions.py\n\u2502  \u251c\u2500 script_to_run.py```\nThe file `script_to_run.py` has the python code that I want to run and looks\nsomething like this:\n\n```python\nimport ray\n\nimport tensorflow\nimport numpy as np\n\nimport my_module\nfrom my_module.some_useful_functions import useful_function\n\n@ray.remote()\ndef run_experiment_with_ray(args):\n    # run python code\n    pass\n\ndef main():\n    ray.init(address='auto',\n             runtime_env={'py_modules': [my_module]})\n    for experiment in all_experiments_ro_run:\n        run_ids = run_experiment_with_ray.remote(experiment)\n\n    ray.get(run_ids)```\nRunning locally (in my laptop) everything works. But problems start to arise\nwhen trying to run on google cloud.\n\nAt first I had problems importing `my_module` and solve this by adding\n\n```git clone my_repo &amp;&amp; cd my_repo &amp;&amp; pip install -e &amp;&amp; cd ..```\nTo the `head_setup_commands` section of the .yaml file. This solved the import\nerrors but, at the moment, the cluster is not working as expected and I have no\nidea why.\n\nSince the experiments are quite heavy I would expect to see workers starting to\nbe launched. However everything is still being ran in the head node and the only\nerror messages I get are this:\n\n```(scheduler +2m31s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n(scheduler +2m31s) Removing 1 nodes of type ray_worker_small (launch failed).\n(scheduler +2m31s) Adding 1 nodes of type ray_worker_small.\n(scheduler +7m44s) Removing 1 nodes of type ray_worker_small (launch failed).\n(scheduler +7m44s) Adding 1 nodes of type ray_worker_small.\n(scheduler +12m55s) Removing 1 nodes of type ray_worker_small (launch failed).\n(scheduler +12m55s) Adding 1 nodes of type ray_worker_small.\n(scheduler +17m55s) Removing 1 nodes of type ray_worker_small (launch failed).\n(scheduler +17m55s) Adding 1 nodes of type ray_worker_small.\n(scheduler +22m54s) Removing 1 nodes of type ray_worker_small (launch failed).\n(scheduler +22m54s) Adding 1 nodes of type ray_worker_small.```\nI know that the script is running, all the plots and other files that result from\nthe experiments are saved to disk. However no experiments run in worker nodes.\nEverything runs in the head node.\n\n\nMy .yaml file is as follows:\n\n```cluster_name: default\n\nmax_workers: 5\n\nupscaling_speed: 1.0\n\ndocker:\n  image: \"rayproject/ray-ml:latest-gpu\" # You can change this to latest-cpu if you don't need GPU support and want a faster startup\n  container_name: \"ray_container\"\n  pull_before_run: True\n  run_options:  # Extra options to pass into \"docker run\"\n    - --ulimit nofile=65536:65536\n\nidle_timeout_minutes: 5\n\nprovider:\n    type: gcp\n    region: europe-west1\n    availability_zone: europe-west1-c\n    project_id: ray-cluster-352608 # Globally unique project id\n\nauth:\n    ssh_user: ubuntu\n\navailable_node_types:\n    ray_head_default:\n        resources: {\"CPU\": 2}\n        node_config:\n            machineType: n1-standard-2\n            disks:\n              - boot: true\n                autoDelete: true\n                type: PERSISTENT\n                initializeParams:\n                  diskSizeGb: 50\n                  sourceImage: projects/deeplearning-platform-release/global/images/family/common-cpu\n\n    ray_worker_small:\n        min_workers: 1\n        max_workers: 5\n        resources: {\"CPU\": 2}\n        node_config:\n            machineType: n1-standard-2\n            disks:\n              - boot: true\n                autoDelete: true\n                type: PERSISTENT\n                initializeParams:\n                  diskSizeGb: 50\n                  sourceImage: projects/deeplearning-platform-release/global/images/family/common-cpu\n            scheduling:\n              - preemptible: true\n\n\nhead_node_type: ray_head_default\n\nfile_mounts: {}\n\ncluster_synced_files: []\n\nfile_mounts_sync_continuously: False\n\nrsync_exclude:\n    - \"**/.git\"\n    - \"**/.git/**\"\n\nrsync_filter:\n    - \".gitignore\"\n\ninitialization_commands: []\n\nsetup_commands: []\n\n\nhead_setup_commands:\n  - sudo apt-get install libpython3.7\n  - pip install --upgrade pip\n  - pip install google-api-python-client==1.7.8\n  - cd my_repository &amp;&amp; pip -e . &amp;&amp; cd ..\n\nworker_setup_commands: []\n\nhead_start_ray_commands:\n    - ray stop\n    - &gt;-\n      ray start\n      --head\n      --port=6379\n      --object-manager-port=8076\n      --autoscaling-config=~/ray_bootstrap_config.yaml\n\nworker_start_ray_commands:\n    - ray stop\n    - &gt;-\n      ray start\n      --address=$RAY_HEAD_IP:6379\n      --object-manager-port=8076\n\nhead_node: {}\nworker_nodes: {}```\nHere we have the last lines of the file `tmp/ray/session_latest/monitor.err`:\n\n```Warning: Permanently added '10.132.0.36' (ECDSA) to the list of known hosts.\nShared connection to 10.132.0.36 closed.\nWarning: Permanently added '10.132.0.36' (ECDSA) to the list of known hosts.\nShared connection to 10.132.0.36 closed.\nShared connection to 10.132.0.36 closed.\nssh: connect to host 10.132.0.37 port 22: Connection refused\nssh: connect to host 10.132.0.37 port 22: Connection refused\nssh: connect to host 10.132.0.37 port 22: Connection refused\nssh: connect to host 10.132.0.37 port 22: Connection refused\nssh: connect to host 10.132.0.37 port 22: Connection refused\nWarning: Permanently added '10.132.0.37' (ECDSA) to the list of known hosts.\nShared connection to 10.132.0.37 closed.\nWarning: Permanently added '10.132.0.37' (ECDSA) to the list of known hosts.\nShared connection to 10.132.0.37 closed.\nShared connection to 10.132.0.37 closed.\nssh: connect to host 10.132.0.38 port 22: Connection refused\nssh: connect to host 10.132.0.38 port 22: Connection refused\nssh: connect to host 10.132.0.38 port 22: Connection refused\nssh: connect to host 10.132.0.38 port 22: Connection refused\nssh: connect to host 10.132.0.38 port 22: Connection refused\nssh: connect to host 10.132.0.38 port 22: Connection refused\nWarning: Permanently added '10.132.0.38' (ECDSA) to the list of known hosts.\nShared connection to 10.132.0.38 closed.\nWarning: Permanently added '10.132.0.38' (ECDSA) to the list of known hosts.\nShared connection to 10.132.0.38 closed.\nShared connection to 10.132.0.38 closed.\nssh: connect to host 10.132.0.39 port 22: Connection refused\nssh: connect to host 10.132.0.39 port 22: Connection refused\nssh: connect to host 10.132.0.39 port 22: Connection refused```\nand the last lines of the file `tmp/ray/session_latest/monitor.log`:\n\n```======== Autoscaler status: 2022-06-30 07:52:22.062281 ========\nNode status\n---------------------------------------------------------------\nHealthy:\n 1 ray_head_default\nPending:\n 10.132.0.39: ray_worker_small, setting-up\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 2.0/2.0 CPU\n 0.00/3.928 GiB memory\n 0.00/1.964 GiB object_store_memory\n\nDemands:\n {'CPU': 1.0}: 1+ pending tasks/actors\n2022-06-30 07:52:27,369 INFO discovery.py:873 -- URL being requested: GET <https://compute.googleapis.com/compute/v1/projects/ray-cluster-352608/zones/europe-west1-c/instances?filter=%28%28status+%3D+RUNNING%29+OR+%28status+%3D+PROVISIONING%29+OR+%28status+%3D+STAGING%29%29+AND+%28labels.ray-cluster-name+%3D+rayhadamard%29&amp;alt=json>\n2022-06-30 07:52:27,609 INFO autoscaler.py:330 --\n======== Autoscaler status: 2022-06-30 07:52:27.609383 ========\nNode status\n---------------------------------------------------------------\nHealthy:\n 1 ray_head_default\nPending:\n 10.132.0.39: ray_worker_small, setting-up\nRecent failures:\n (no failures)```"}
{"question": "I am setting up a ray cluster on an HPC with SGE (old school, no SLURM for me :disappointed: ) and we have very large nodes, so when I create a node with ray start, should I try to AVOID starting TWO worker nodes on the same physical machine? Or will they happily work on the same physical machine? I can avoid hosts with qsub, but it limits the number of worker nodes I could have and I cannot \"hog\" the 256GB, 56 core machines all for one ray node!"}
{"question": "Edit: I figured out the problem with my code, but please see my still open question in the reply. I\u2019m still not clear on what the correct way to use Ray is. Thanks.\n\nHi Ray folks, I\u2019m just getting started using Ray and could use some help. For starters I\u2019m trying to add a column to a dataset, but it\u2019s not working. The code seems to run, but the data set does not have the new column. Here is my current code:\n```import polars as pl\n\ndef addIDColumn(table):\n  df = pl.from_arrow(table)\n  df.with_row_count(name='docId')\n  return df.to_arrow()\n\nds = ray.data.read_parquet(fileName)\nds = ds.repartition(1)\nds = ds = ds.map_batches(addIDColumn, batch_format=\"pyarrow\")```\nWhat is the correct way to add a column? Also, will the repartition(1) call prevent the row numbers from starting at zero for each partition?"}
{"question": "Hi Ray folks, I observed a weird behavior of `ray start` :\n<https://docs.ray.io/en/latest/ray-core/starting-ray.html#starting-ray-via-the-cli-ray-start|Starting Ray via the CLI (ray start)> seems to imply, `ray start` should not be launched on the same node twice.\nI tried, but it could run normally and create two nodes with the exactly same Node, IP settings:\nWhat I am not sure is, whether it's a feature, or an undefined behavior?"}
{"question": "there are often discussions about scalability, including the max number of nodes per cluster. For this particular metric \u2014 which component is the bottleneck?"}
{"question": "and are there significant differences when \u2018just\u2019 running a cluster at scale (eg min and max instances=10000) vs letting the autoscaler decide how many instances would be needed?"}
{"question": "Hi all :slightly_smiling_face:\n\nif i specify pip packages in the runtime_env when/where does this get built?\n\nI want to preinstall some larger packages and then allow users to bring their own for smaller ones, this doesn't seem to work as the preinstalled packages get a `module not found`\n\nthen when adding these packages to pip I get a long hang, assuming this is the pip packages being built but hard to know."}
{"question": "Dear Ray community,\nI am new to Ray and having trouble autoscaling RLlib applications on AWS Ray Clusters.\nI submitted an issue here: <https://github.com/ray-project/ray/issues/26172>\nWould someone please help me out? Many thanks :smile:"}
{"question": "Hi all,\nI am running an application that uses ray as an execution back-end. This requires a Redis instance. Is it possible to reuse ray's internal Redis instance for this?"}
{"question": "I'm trying to submit a job to a cluster, from outside of the cluster:\n\n`export RAY_ADDRESS=\"<ray://192.168.xx.xx:10001>\"`\n`runtime_env=...`\n`script=...`\n`ray job submit --runtime-env \"$runtime_env\" python3.9 \"$script\"`\n\nHowever, it gives me this error:\n\n`ConnectionError: Failed to connect to Ray at address: <http://192.168.xx.xx:10001>.`\n\nMy machine can access that address. If I open it in a browser, it gives me some text:\n\n`\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd?\u044f\u044f\ufffd\ufffd?\u044f\u044f\ufffd\ufffd\ufffd \ufffd\u044e\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd?\ufffd\ufffd`\n\nThe padlock next to the address bar tells me the connection is not secure ( I don't know if this is a problem or not).\n\nI can submit the job successfully from the head node.\n\nWhat could I be doing wrong?"}
{"question": "Is the next release of Ray going to be 1.14 or are we heading towards the 2.0 already?"}
{"question": "hi where can I find the code for this talk by anyscale : <https://www.youtube.com/watch?v=UtH-CMpmxvI&amp;ab_channel=Anyscale|Productionizing ML at scale with Ray Serve> ?"}
{"question": "Hi, I failed to start ray 1.13.0 on my macbook with just `ray.init()`.  Does any one know why? Here is the error log:\n```2022-07-06 22:03:23,390\tERROR services.py:1488 -- Failed to start the dashboard: Failed to start the dashboard, return code 1\nFailed to read dashboard log: [Errno 2] No such file or directory: '/tmp/ray/session_2022-07-06_22-03-22_173275_24726/logs/dashboard.log'\n2022-07-06 22:03:23,390\tERROR services.py:1489 -- Failed to start the dashboard, return code 1\nFailed to read dashboard log: [Errno 2] No such file or directory: '/tmp/ray/session_2022-07-06_22-03-22_173275_24726/logs/dashboard.log'\nTraceback (most recent call last):\n  File \"/Users/ruian/miniforge3/envs/ray-sql/lib/python3.9/site-packages/ray/_private/services.py\", line 1451, in start_dashboard\n    with open(dashboard_log, \"rb\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/ray/session_2022-07-06_22-03-22_173275_24726/logs/dashboard.log'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/ruian/miniforge3/envs/ray-sql/lib/python3.9/site-packages/ray/_private/services.py\", line 1462, in start_dashboard\n    raise Exception(err_msg + f\"\\nFailed to read dashboard log: {e}\")\nException: Failed to start the dashboard, return code 1\nFailed to read dashboard log: [Errno 2] No such file or directory: '/tmp/ray/session_2022-07-06_22-03-22_173275_24726/logs/dashboard.log'\n[2022-07-06 22:03:32,875 E 24726 14443730] <http://core_worker_process.cc:223|core_worker_process.cc:223>: Failed to get the system config from raylet because it is dead. Worker will terminate. Status: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details:  .Please see `raylet.out` for more details.\n\nProcess finished with exit code 1```"}
{"question": "Hi <@U01TCGYN8CA> and <@UNCRYAV9N>,\n\nI\u2019m back on this and experiencing instability with actor placement groups. Do you see a workaround for any of these 3 errors?\n\nEven using Ray on a single-node my workers are periodically dying:\n```A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 69aa703871773bbfca1de7cc4d43e816c887a68a02000000 Worker ID: 8bdb61da7c9318d96978375b214f80c2f92135a2f29fa8d779ac260f Node ID: 3a3d714d76e7a20500e94edfb40106bbd0e8647e5ede2cd9e55d7bed Worker IP address: 141.142.144.157 Worker port: 10173 Worker PID: 486772```\nI tried a single node with 120 of 128 cores in the placement group and that failed. (I deleted the logs, but the error is something like \u201cerror: cpu core 5 has been killed\u201d).\n\nAnd in multi-node settings (with 100 gb networking), my workers are timing out:\n```<http://worker_pool.cc:342|worker_pool.cc:342>: Some workers of the worker process(29490) have not registered to raylet within timeout.```\nI\u2019m also getting errors from the GCS about \u2018too many open files\u2019\n```(pid=gcs_server) E0704 <http://tcp_server_posix.cc:213]|tcp_server_posix.cc:213]>    Failed accept4: Too many open files```\n*What does work:*\n\u2022 Single node, using 100 of 128 cores appears stable. \n\u2022 I\u2019m using 100 submission actors with 1 CPU each that all read next jobs from a shared Ray `Queue()`.\n\u2022 It helped stability to start the \u201csubmission actors\u201d slowly, taking a 1 second break between their initial deployment to reduce concurrent I/O on initial startup. \n*Kinda works:* \n\u2022 Multi-node, up to 6 nodes but I can only use about half the available CPU cores in the placement group before fully crashes (I think due to overwhelming the GCS). Still, in this setup I get a continuous trickle of `worker not registered to raylet within timeout` errors. "}
{"question": "Hello all!\n\nI am getting familiar with cluster auto-scaling. When I submit a task which would require a new node, the launching fails (stack trace in a reply to this message).\n\nHow can I go about getting more information about why these instances failed to join the cluster (I see the instances in the AWS console, cycling through different states)?"}
{"question": "Hello everyone! I've been getting familiar with ray and have been trying to run a pretty simple workflow. I have a pretrained ML model that I have been putting into the object store via `ray.put()`  then retrieving on remote tasks via `ray.get()`. I then put the model/data on gpu, run the data through the model, and save the output to s3. The problem is, every time a remote task completes, it does not seem to be releasing gpu memory (even if I explicitly `del` the model and data). This runs me out of GPU memory pretty fast. Any fixes for this? The output of `nvidia-smi` on a node in question is shown below"}
{"question": "Hi all! I want to send a signal to the ray actor to gracefully shutdown it. However, the actor is not main thread, so it cannot have its own signal handler.\nWhen a ray actor dies unexpectedly due to SIGTERM, etc., is there a way to gracefully shutdown it by creating a handler?"}
{"question": "Hi Ray Team! I'm exploring Ray Workflows in combination with a FastAPI application. I have a long-running (&gt;20min) Ray Workflow defined and would like to invoke it from a FastAPI endpoint, which should just trigger the workflow. I'm not using Ray Serve. What would be the appropriate way to do it? I was reading about <https://docs.ray.io/en/latest/cluster/job-submission.html#ray-job-sdk|Job Submissions> but I don't think this is (yet) appropriate for my use case, but perhaps I am wrong. Thanks for any pointers!"}
{"question": "Hi \u2014 where can i find a link to the usage data being collected here? <https://docs.ray.io/en/latest/cluster/usage-stats.html>"}
{"question": "Hello\nHow would you concatenate two ray datasets together?\nI'm looking for an equivalent in `pandas` of `pd.concat([df_a, df_b])`\nI've already tried to use `modin` to do so, but the performance is terrible :smile:\n\nAny advice is appreciated :hugging_face:"}
{"question": "Hi everyone,\nIs there some API that allows to temporary release [resources](<https://docs.ray.io/en/master/ray-core/tasks/resources.html>) during the execution of a ray task, wait/sleep for some time and the start once the resources are available again? I have some tasks that can't all run concurrently but should run more or less in sync. Think of training mutliple models on a limited number of GPU's in parallel. First you train model 1-3 for 5 epochs, they release the GPU memory and the corresponding ray resources, then the other 2 models train for 5 epochs and then release the resources so that the models 1-3 can continue training, and so on...."}
{"question": "Is there a built-in behavior, that if the cluster nodes were set up successfully, but they execute no scripts, the ray will shut down these nodes automatically, or call `ray down`  ?"}
{"question": "Hello All, is there any possibilities to add a new worker node to existing ray cluster with GPU ? note: existing cluster is only on cpu."}
{"question": "I use my head node to submit tasks. I would prefer it not to do any work. Is there a way to edit my `config.yaml` file such that it will not do any work? I tried using `available_node_types`, but it gives me error:\n\n`The field available_node_types is not supported for on-premise clusters.`"}
{"question": "Hi <@UNCRYAV9N> and <@U01TCGYN8CA>,\n\nRE our <https://ray-distributed.slack.com/archives/C01DLHZHRBJ/p1657153897060669?thread_ts=1655337242.713989&amp;cid=C01DLHZHRBJ|ongoing discussion here>. Update: your suggestions worked great!\n1. Address \u2018too many open files\u2019 error in GCS. Fix: `ulimit -n unlimited` (on head node &amp; workers)\n2. Address \u2019(raylet) <http://worker_pool.cc:518|worker_pool.cc:518>: Some workers of the worker process(1830280) have not registered within the timeout.\u2019. Fix: `export RAY_worker_register_timeout_seconds=120`.\n3. I refactored to read input from network file storage, and write output to a node\u2019s local SSD. This is much faster than using network file storage for everything.\nNow everything is extremely stable, until all of a sudden a worker node dies!\n\n```The node with node id: a30f89fada6c37637a96a7e1f54aa5d8299a1cdaa4902382dc7467ea and ip: 172.28.22.98 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a raylet crashes unexpectedly or has lagging heartbeats.```\nHow could I *prevent workers from being marked dead* after missed heartbeats? It seems like everything is all fine when this happens out of nowhere."}
{"question": "I am current trying the hook up my Java codes to the python codes. The cross-language documentation is still not clear, Could anyone help with a clear guide?"}
{"question": "Hi everyone! I wonder what tool do you use to read ray's source code, especially the c++ part?\nI use VSCode but sometimes its \"Go to definition\" is really slow. And CLion doesn't seem to work well with Bazel project."}
{"question": "I am trying to setup cluster with 1 head and 1 worker nodes each has resources: {\"CPU\": 16, \"GPU\": 2}.\nI have a next situation:\n```cluster_name: cluster\n\nupscaling_speed: 10000\nidle_timeout_minutes: 2\n\nmin_workers: 0\ninitial_workers: 1\nmax_workers: 1```\n```available_node_types:\n    node:\n        min_workers: 0\n        max_workers: 1\n        resources: {\"CPU\": 16, \"GPU\": 2}```\nAfter head node setup, I expected of starting of 1 worker node set up, but autoscaler logs shows me, that this process didn't start. Further, I ran a training script, which firstly initializes ray cluster in  a next way:\n```@ray.remote(num_cpus=1)\ndef sleep_func(t: int):\n    \"\"\"Helper function for cluster initialization.\"\"\"\n\n    time.sleep(t)\n\n\ndef multi_node_setup(args: Namespace):\n    \"\"\"Function to set up ray cluster.\n\n    @param args: Namespace with init arguments.\n    @return: Nothing.\n    \"\"\"\n\n    ray.init(address='auto')\n\n    # Ray workers setup.\n    device = \"GPU\" if parse_bool(args.gpu) else \"CPU\"\n    print(f\"Device type: {device}\")\n\n    while ray.cluster_resources()[device] &lt; args.num_workers:\n        print(f\"Cluster available workers: {ray.cluster_resources()[device]}, Needed workers: {args.num_workers}\")\n        time.sleep(10)\n\n    # <http://github.com/ray-project/ray/issues/8326|github.com/ray-project/ray/issues/8326>\n    num_cpus = int(ray.cluster_resources()[\"CPU\"] - 1)\n\n    for i in range(num_cpus):\n        sleep_func.remote(args.sleep_time)\n\n    time.sleep(args.sleep_time * 2)```\nAnd also worker node's set up didn't start. The logs from autoscaler are next:\n```---------------------------------------------------------------\nHealthy:\n 1 node\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/16.0 CPU\n 0.0/2.0 GPU\n 0.0/1.0 accelerator_type:V100\n 0.00/34.778 GiB memory\n 0.00/17.389 GiB object_store_memory\n\nDemands:\n (no resource demands)```\nI suppose, that this is unexpected behavior. What could be the problem?"}
{"question": "What is the behavior of the autoscaler if the head node failed? Will be the worker nodes also shut down? If no, is there any workaround for this situation?"}
{"question": "I'm seeing a bunch of '(raylet): Spilled [32000ish] Mib, 647 objects, write throughput [...]' messages.  is there a suggested way to troubleshoot what is causing these writes?"}
{"question": "Hi, I am trying to use ray to run a distributed rl algorithm, I wanted to run the algorithm on a local system and a google tpu VM. To do this I ran a cluster with the tpu VM as the head node and the local system as a client node, but for some reason every time I run the program I get the following error:\n```\":\"src/core/lib/surface/call.cc\",\"file_line\":1074,\"grpc_message\":\"Attempted to reconnect a session that has already been cleaned up\",```\nor\n```(gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details:```\nWhat can I do to solve this?"}
{"question": "Hello everyone is there any restriction to use ABC class with ray actors ? The inherited methods will be made remote automatically ?"}
{"question": "I'm wondering how to deploy on Ray for each of our projects its own python environment, without having pip to re-install everything from zero at each job submit on the cluster...\n\ncurrently I'm using:\n`ray job submit --runtime-en-json={\"pip\": all_my_libs}` but from what I understand is it's installing everything at each submit.\n\nSo I was considering using Docker to set up the python environment as described here: <https://docs.ray.io/en/latest/ray-core/handling-dependencies.html>\nHowever, it just set a global env on the cluster, which is not what I'm looking for.\n\nAny ideas? Thanks :)"}
{"question": "<@U03N4S179A9> This may be by design since it's only required for that job, and its runtime environment is transient as opposed to permanent when adding dependencies as you would with a cluster launcher. \n\nCan someone confirm this? cc: <@U02P5FKRT9U> <@U01855CHF9T> <@U02CA1F7BB8> "}
{"question": "Hi, I am trying to understand the deployment of a Ray Cluster on Kubernetes using the Helm Chart <https://docs.ray.io/en/latest/cluster/kubernetes.html#installing-the-ray-operator-with-helm|here>. How do I deploy the dashboard? I usually have a CI/CD pipeline that will deploy the Helm chart to a DNS within my VPC. I understand that this will then deploy the Ray Cluster to my DNS which I can access and then run my jobs on.\n\nBut it seems that I will always have to use port-forwarding to access the dashboard based on <https://docs.ray.io/en/latest/cluster/kubernetes.html#observability|this>. Is there anyway other than giving users direct access to the cluster to run the kubectl command to perform port forwarding?"}
{"question": "Hi how can I know which ray docker image is stable?"}
{"question": "Hi I am trying to run one simple airflow example, <https://github.com/anyscale/airflow-provider-ray/blob/main/ray_provider/example_dags/demo_fault_tolerance.py|demo_fault_tolerance.py>(<https://github.com/anyscale/airflow-provider-ray/tree/main/ray_provider/example_dags>).  First two tasks, load_data1() and load_data2() were successful. But the third task, transform_data(data1, data2), was failed. Looked at the log file in Airflow, it seems that ray cannot retrieve the data returned by the first two tasks.\n```return data1 * data2 * 100\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'; 47841)```\nAlso I found some warning messages in Airflow as follows;\n```2022-07-22, 16:23:57 UTC] {ray_backend.py:116} INFO - Failed to look up actor with name 'ray_kv_store'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.\n[2022-07-22, 16:23:57 UTC] {ray_backend.py:118} INFO - Creating new Actor with identifier Failed to look up actor with name 'ray_kv_store'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.\n[2022-07-22, 16:23:57 UTC] {logging_mixin.py:115} WARNING - Caught schedule exception\n[2022-07-22, 16:23:57 UTC] {ray_decorators.py:19} INFO - [wrapper] Launching task (with ('ObjectRef(e369509ba5853d89ffffffffffffffffffffffff0100000002000000)', 'ObjectRef(b72fac3ebbacb4feffffffffffffffffffffffff0200000002000000)'), {}.\n[2022-07-22, 16:23:57 UTC] {logging_mixin.py:115} WARNING - It looks like you're creating a detached actor in an anonymous namespace. In order to access this actor in the future, you will need to explicitly connect to this namespace with ray.init(namespace=\"74d8f28e-a642-434e-994b-3c2e88024ea8\", ...)```\nis this because anonymous namespace used? Airflow version is 2.3.2 and RAY version is 1.13.0. Could someone give me a clue how to solve this? Thanks."}
{"question": "Hi all!\n\nThe _*Ray 2.0.0 release candidate*_ will be released next week! The release candidate will include the following highlighted features, and more:\n\u2022 <https://docs.ray.io/en/master/ray-air/getting-started.html|Ray AIR> [Beta]\n\u2022 <https://docs.google.com/document/d/1RPdqMgQcIHFEB-ZXswCqr7LhNvWManKqpui0tJumabQ/edit?usp=sharing|Ray State Observability> [Alpha]\n\u2022 <https://docs.ray.io/en/master/rllib/core-concepts.html#execution-plans|RLlib training step function> [Alpha]\n\u2022 <https://arxiv.org/pdf/2203.05072v3.pdf|Scalable Dataset Shuffle> [Beta]\n\u2022 <https://docs.ray.io/en/master/serve/deployment-graph/deployment-graph-e2e-tutorial.html|Serve Deployment graph> [Beta]\n\u2022 <https://ray-project.github.io/kuberay/guidance/gcs-ha/|Serve HA> [Alpha]\nWe strongly encourage you to try out this release and let us know if you have any issues. You will be able to try out the release candidate by running:\n\n  `pip install --pre ray==2.0.0rc0`\n\nPlease post any issues or questions on <#C03QLKAGWSH|ray-2-0-feedbacks> slack channel!\n\nThanks!"}
{"question": "Can i use Affinity for ray cluster in kubenetes?"}
{"question": "Hi, guys. I'm learning Rllib for training agent along with a outside simulator. I found that the *`PolicyClient` and `PolicyServerInput`*is suitable for this. but I'm not sure where the *`PolicyServerInput`*  would be running when the `num_works`  &gt; 0.\nIIUC, if the *`PolicyServerInput`*  running in rollout worker in a distributed ray cluster, then *`PolicyServerInput`*  would listen on different address, that would be not easy for the policyClient to connect, Because it don't know the rollout worker's address.\n\nDo I misunderstand it ?"}
{"question": "Hi I am trying to run this demo code to train a torch model on gpu.\n```import torch\nimport torch.nn as nn\n\n\nnum_samples = 20\ninput_size = 10\nlayer_size = 15\noutput_size = 5\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(input_size, layer_size)\n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(layer_size, output_size)\n\n    def forward(self, input):\n        return self.layer2(self.relu(self.layer1(input)))\n\n# In this example we use a randomly generated dataset.\ninput = torch.randn(num_samples, input_size)\nlabels = torch.randn(num_samples, output_size)\n\n\nimport torch.optim as optim\n\ndef train_func():\n    num_epochs = 3\n    model = NeuralNetwork()\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.1)\n\n    for epoch in range(num_epochs):\n        output = model(input)\n        loss = loss_fn(output, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(f\"epoch: {epoch}, loss: {loss.item()}\")\n\n# train_func()\n\nfrom ray import train\nimport ray.train.torch\n\ndef train_func_distributed():\n    num_epochs = 3\n    model = NeuralNetwork()\n    model = train.torch.prepare_model(model)\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.1)\n\n    for epoch in range(num_epochs):\n        output = model(input)\n        loss = loss_fn(output, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(f\"epoch: {epoch}, loss: {loss.item()}\")\n    \nimport ray\nfrom ray.train import Trainer\nray.init(\"auto\")\n\n\n# trainer = Trainer(backend=\"torch\", num_workers=2)\n\n# For GPU Training, set `use_gpu` to True.\ntrainer = Trainer(backend=\"torch\", num_workers=1, use_gpu=True)\n\ntrainer.start()\nresults = trainer.run(train_func_distributed)\ntrainer.shutdown()```\nThe code is follow <https://docs.ray.io/en/latest/train/train.html> , but I got this error\n```---------------------------------------\nJob 'raysubmit_s9wgX4mWJDK327GN' failed\n---------------------------------------\n\nStatus message: Job failed due to an application error, last available logs:\n2022-07-26 02:20:38,732 INFO trainer.py:223 -- Trainer logs will be logged in: /home/ray/ray_results/train_2022-07-26_02-20-38\n(BaseWorkerMixin pid=493, ip=10.59.113.55) 2022-07-26 02:20:44,907      INFO torch.py:335 -- Setting up process group for: env:// [rank=0, world_size=1]\n2022-07-26 02:20:44,955 INFO trainer.py:229 -- Run results will be logged in: /home/ray/ray_results/train_2022-07-26_02-20-38/run_001\n(BaseWorkerMixin pid=493, ip=10.59.113.55) 2022-07-26 02:20:44,968      INFO torch.py:92 -- Moving model to device: cuda:0\nTraceback (most recent call last):\n  File \"train.py\", line 72, in &lt;module&gt;\n    results = trainer.run(train_func_distributed)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/trainer.py\", line 334, in run\n    for intermediate_result in iterator:\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/trainer.py\", line 720, in __next__\n    self._finish_training\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/trainer.py\", line 687, in _run_with_error_handling\n    return func()\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/trainer.py\", line 791, in _finish_training\n    return self._backend_executor.finish_training()\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/utils.py\", line 173, in &lt;lambda&gt;\n    return lambda *args, **kwargs: ray.get(actor_method.remote(*args, **kwargs))\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1809, in get\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(RuntimeError): ray::BackendExecutor.finish_training() (pid=418, ip=10.59.113.55, repr=&lt;ray.train.backend.BackendExecutor object at 0x7fa644f44750&gt;)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/backend.py\", line 544, in finish_training\n    results = self.get_with_failure_handling(futures)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/backend.py\", line 563, in get_with_failure_handling\n    success, failed_worker_indexes = check_for_failure(remote_values)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/utils.py\", line 53, in check_for_failure\n    ray.get(object_ref)\nray.exceptions.RayTaskError(RuntimeError): ray::BaseWorkerMixin._BaseWorkerMixin__execute() (pid=493, ip=10.59.113.55, repr=&lt;ray.train.worker_group.BaseWorkerMixin object at 0x7fac42ec5810&gt;)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/worker_group.py\", line 26, in __execute\n    return func(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/backend.py\", line 535, in end_training\n    output = session.finish()\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/session.py\", line 117, in finish\n    func_output = self.training_thread.join()\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/utils.py\", line 101, in join\n    raise self.exc\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/train/utils.py\", line 94, in run\n    self.ret = self._target(*self._args, **self._kwargs)\n  File \"train.py\", line 54, in train_func_distributed\n    output = model(input)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"train.py\", line 18, in forward\n    return self.layer2(self.relu(self.layer1(input)))\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\", line 96, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"/home/ray/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\", line 1847, in linear\n    return torch._C._nn.linear(input, weight, bias)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument mat1 in method wrapper_addmm)```\nAny one know how to fix it?"}
{"question": "How can I disable regular logging output from raylet in a local cluster?\n```[(raylet) [2022-07-26 16:23:04,661 E 285124 285159] (raylet) <http://file_system_monitor.cc:105|file_system_monitor.cc:105>: /tmp/ray/session_2022-07-26_16-05-39_932514_2850\n48 is over 95% full, available space: 2189856768; capacity: 250375106560. Object creation will fail if spilling is required.```\nStarting the client with\n```import ray\nimport logging\nray.init('auto', logging_level=logging.ERROR)```\ndid not help. Thanks!"}
{"question": "could someone tell me where I can learn more about the `runtime_env` behavior?  I\u2019m curious how it works.\n\u2022 does it just install at the beginning of each run on each worker? So if there are ten replicas, it\u2019ll get installed ten times? Or does Ray come with a fancy image building/caching service and the image is actually saved and re-used?  Assuming that exists, how does it work?\n\u2022 Has anyone run into any dependency issues wrt reproducibility?  Like if you just specify the pip packages that you want (even if you specify the version) it may pull in arbitrary other dependencies (because their setup.py is not pinned)"}
{"question": "Hey Ray Team,\n\nDoes anyone know how we can specify data types for columns when using the Ray Dataset `read_csv` library?\n\nFor context, the code I am using is the following:\n```import ray\nimport gcsfs\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport time\nimport glob\n\n@ray.remote\ndef load_data(project: str, gcs_dir: str):\n  fs = gcsfs.GCSFileSystem()\n  ds = ray.data.read_csv(gcs_dir, filesystem=fs)\n  return ds\nproject='PROJECT NAME'\ngcs_dir = \"GCS DIRECTORY\"\nds = ray.get(load_data.remote(project, gcs_dir))```\nUpon running it, I get an error due to a conversion issue,\n\n```2022-07-26 22:11:25,607\tWARNING utils.py:533 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 9.6 to 9.\n(_execute_read_task pid=11711) 2022-07-26 22:11:28,174\tINFO worker.py:451 -- Task failed with retryable exception: TaskID(43476cf6705748a9ffffffffffffffffffffffff01000000).\n(_execute_read_task pid=11711) Traceback (most recent call last):\n(_execute_read_task pid=11711)   File \"python/ray/_raylet.pyx\", line 665, in ray._raylet.execute_task\n(_execute_read_task pid=11711)   File \"python/ray/_raylet.pyx\", line 669, in ray._raylet.execute_task\n(_execute_read_task pid=11711)   File \"/home/sandbox/.local/share/jupyter/kernels/pip/pip_venv/lib/python3.7/site-packages/ray/data/impl/lazy_block_list.py\", line 451, in _execute_read_task\n(_execute_read_task pid=11711)     block = task()\n(_execute_read_task pid=11711)   File \"/home/sandbox/.local/share/jupyter/kernels/pip/pip_venv/lib/python3.7/site-packages/ray/data/datasource/datasource.py\", line 157, in __call__\n(_execute_read_task pid=11711)     for block in result:\n(_execute_read_task pid=11711)   File \"/home/sandbox/.local/share/jupyter/kernels/pip/pip_venv/lib/python3.7/site-packages/ray/data/datasource/file_based_datasource.py\", line 209, in read_files\n(_execute_read_task pid=11711)     for data in read_stream(f, read_path, **reader_args):\n(_execute_read_task pid=11711)   File \"/home/sandbox/.local/share/jupyter/kernels/pip/pip_venv/lib/python3.7/site-packages/ray/data/datasource/csv_datasource.py\", line 38, in _read_stream\n(_execute_read_task pid=11711)     batch = reader.read_next_batch()\n(_execute_read_task pid=11711)   File \"pyarrow/ipc.pxi\", line 682, in pyarrow.lib.RecordBatchReader.read_next_batch\n(_execute_read_task pid=11711)   File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\n(_execute_read_task pid=11711) pyarrow.lib.ArrowInvalid: In CSV column #3: Row #91520: CSV conversion error to int64: invalid value '41.968069'```\n*Question:*\nIs there any way to explicitly specify the column types to the `read_csv` function? Right now, its trying to infer as int64 for certain columns when it should be float32.\n\nI see that a similar problem has been described in a github issue <https://github.com/ray-project/ray/issues/23133#issuecomment-1106619322|here>. Is there any examples on how we can pass the set <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions.column_types|ConvertOptions.column_types> to explicitly specify the type of a column (PyArrow then won't try to infer the type) through the `ray.dataset.read_csv` function?"}
{"question": "Hi, has anybody been able to run Ray in scala language? Let me know what worked for you"}
{"question": "Hi all, there is any way to limit the total number of tasks from the same driver program? Even though there are idle workers, I don't want all workers are occupied by one job and others have to wait.\nI don't want to limit nodes it can use, but the number of executing tasks from the same job at the same time.\nThanks.\n"}
{"question": "Does Ray support multithreads in one process?"}
{"question": "how does a scheduler like PB2 and a search alg like HyperOptSearch interact in ray.tune? It seems like both of them would set the the hyperparameters for a new trial. Does one take precedence over the other? Are they invoked under different conditions? Should the set of hyperparameters assigned to PB2 and HyperOptSearch be mutually exclusive?"}
{"question": "why is is starting 6 workers? I figured it would start 2 worker per the --num-cpus argument"}
{"question": "or does that --num-cpus=2 set the number of vCPUs for each worker to 2?"}
{"question": "If so, how do I cap the number of workers to a set number?"}
{"question": "i have an issue viewing the dashboard when initiating ray in a container, is this normal? This is after i start a 1 node cluster in the docker image"}
{"question": "Hi all!\n\nI\u2019m running a process on a ray cluster - let\u2019s say I have 1 machine with 16 cores. The process is ML prediction, on a large dataset, which means I cannot run 16 workers in parallel with 1 CPU each, because RAM explodes. I would have to define workers with 4 CPUs each, meaning this would add up to 4 workers in total.\n\nI\u2019m having issues defining such worker-based resources, the cluster.yaml file provides the option to set the resources on the machine level, so if I limit the resources there, I\u2019m decreasing CPU utilization. Is there some setting here how to define the resources for each worker?"}
{"question": "Hi all! Can anyone please take a look at this <https://github.com/ray-project/ray/issues/27076|issue>?\n\nThis issue is about the fault tolerance of the placement group:\n\u2022 Once a node failed, the placement groups on that dead node will be rescheduled by GCS automatically by default, however, there isn\u2019t any notification about the failure and rescheduling. So it\u2019s very hard to reuse those placement groups which have been rescheduled, because we don\u2019t know if or when they have been rescheduled. Since we can\u2019t reuse them, and they have a higher priority than new placement groups creations, it will eat the computation resources.\nThere are probably two ways to solve this issue:\n1. Add a callback to the placement group, so we can be notified if something goes wrong.\n2. Add an argument to disable the fault tolerance, so when a node dead, we can abandon the old placement groups on the dead node and create new groups."}
{"question": "Anyone ever seen this?\n```ModuleNotFoundError: No module named 'ray.runtime_env.runtime_env'; 'ray.runtime_env' is not a package```"}
{"question": "```@ray.remote(num_cpus=2)\nclass Actor(object):\n    pass\n\ncounters = [Counter.remote() for _ in range(2)]```\nIf I have a CPU with 8 cores and want each Actor to have access to 4 CPUs, does this code snippet use all the 8 cores?"}
{"question": "Is it possible that multiple remote functions have access to and modify a shared object?"}
{"question": "Hi all!\n\nThe _*Ray 2.0.0 rc0 has been released*_! The release candidate will include the following highlighted features, and more:\n\u2022 <https://docs.ray.io/en/master/ray-air/getting-started.html|Ray AIR> [Beta]\n\u2022 <https://docs.google.com/document/d/1RPdqMgQcIHFEB-ZXswCqr7LhNvWManKqpui0tJumabQ/edit?usp=sharing|Ray State Observability> [Alpha]\n\u2022 <https://docs.ray.io/en/master/rllib/core-concepts.html#execution-plans|RLlib training step function> [Alpha]\n\u2022 <https://arxiv.org/pdf/2203.05072v3.pdf|Scalable Dataset Shuffle> [Beta]\n\u2022 <https://docs.ray.io/en/master/serve/deployment-graph/deployment-graph-e2e-tutorial.html|Serve Deployment graph> [Beta]\n\u2022 <https://ray-project.github.io/kuberay/guidance/gcs-ha/|Serve HA> [Alpha]\nWe strongly encourage you to try out this release and let us know if you have any issues. You will be able to try out the release candidate by running:\n\n  `pip install ray==2.0.0rc0`\n\nPlease post any issues or questions on <#C03QLKAGWSH|ray-2-0-feedbacks> slack channel!\n\nThanks!"}
{"question": "hi all, are there any prerequisites to list a framework under the <https://docs.ray.io/en/latest/ray-overview/ray-libraries.html|Ray Ecosystem>? I am managing a project where we use Ray as an execution provider. We would like to be listed under the ecosystem and we were wondering what the procedure there would be?"}
{"question": "Hi folks,\n\nWe are seeing occasional crashes of the ray cluster and am looking for help in diagnosing the issue. To date, I've reviewed the logs for the session in /tmp/ray but did not find a root issue.\n\nIs there additional logging I can turn on or other ways to retrieve additional logs ?\n\nThanks in advance !"}
{"question": "Hey, just spent awhile trying to run Ray in a docker container on my M1 mac. It looks like there is no wheel for that, even though I can download ray on my M1 without a container.\n\nAnyone else experience this? Are there any workarounds besides building the docker image on a different platform?"}
{"question": "Hey I'm thinking of working on improving the observability of ray workflows. Is there currently a way to retrieve all the steps in a workflow? Thanks for any help"}
{"question": "Hello, I recently upgraded ray to 1.13 and I notice that the `_name` attribute of trainers is not there anyone, e.g. `DQNTrainer._name`. What can I use instead? I need it to be passed as argument to `tune.registry.get_trainable_cls()`"}
{"question": "Has anyone seen an issue where placement group scheduling seems to hang even though resources are available? Seeing this issue when running Horovod on Ray."}
{"question": "First time trying ray, deploying with helm, and running into exact same issue as <https://github.com/ray-project/ray/issues/18276> , but the issue has no solution :disappointed:\n\nIs there a simple solution? Should I reopen the issue on github?"}
{"question": "hey folks,\nis there a Ray docker image for linux/arm64 platform?"}
{"question": "Is it possible having 3 Actors sending asynchronous request to a ray Server that executes the requests in a batch when 3 requests are collected and then send the individual requests back to the corresponding Actor?"}
{"question": "Is there a way to configure Ray to try adding a different worker node type if the default worker node type has no spot capacity? Did a quick search of the slack/the internet but couldn't find an answer"}
{"question": "I am using Ray==1.12.0 with RLLIB and ran a cluster job with ray. When I sync the experiment results down and try to create the ExperimentAnalysis object on my local machine to the local path of the experiment folder, I get an error of incorrect file path. The directory structure during training of the cluster head persists and causes a failure on loading the ExperimentAnalysis object as it attempts to load the checkpoints with the cluster filepath rather than the relative local file paths.\n\nIs there a way to fix this? Is it required that I have the same absolute directory structure on the remote head as I do for the local machine meant for analyzing the results.  I tried searching for github issues, but did not see anyone discussing this."}
{"question": "What\u2019s new in ray 2.0? Any advertisements/changelogs I can read?"}
{"question": "How can I config Ray cluster such that the new added nodes automatically run the task when join?"}
{"question": "Hi! I am getting the error \u201cfail to register worker to raylet\u201d because of \u201cno available ports\u201d from ray tune, although I have set the range of`[min-worker-port, max-worker-port]` to be larger than the number of CPUs on each compute node. Is there a document that explains what a \u201cworker\u201d is and how it bind to a port? Thanks!"}
{"question": "Hey Ray experts! For the life of me, I cannot find documentation on:\n\n*How Ray handles the case where the resources requested for all jobs exceeds currently available resources.*\n\nCan someone point me to the docs that discuss the behavior when this happens?"}
{"question": "Hey team, I want to use the runtime_env with Ray Serve, but got the following error. Does anyone know how to resolve this?\n```ray._private.runtime_env.utils.SubprocessCalledProcessError: Run cmd[3] failed with the following details.\nCommand '['/Users/yiqingwang/.pyenv/versions/3.7.3/bin/python3.7', '-m', 'virtualenv', '--app-data', '/tmp/ray/session_2022-08-11_18-53-15_713807_17153/runtime_resources/pip/4a81b1a231b50edc8deb2b5a7fcfbd346b31f270/virtualenv_app_data', '--reset-app-data', '--no-periodic-update', '--system-site-packages', '--no-download', '/tmp/ray/session_2022-08-11_18-53-15_713807_17153/runtime_resources/pip/4a81b1a231b50edc8deb2b5a7fcfbd346b31f270/virtualenv']' returned non-zero exit status 2.\nLast 50 lines of stdout:\n    Usage: virtualenv.py [OPTIONS] DEST_DIR\n\n    virtualenv.py: error: no such option: --app-data```"}
{"question": "Hello, I am new to Ray. A quick question: Does the Ray driver program talk to Ray workers directly?  Is it possible delegating all the communications between the driver program through the head node?"}
{"question": "Hey Ray experts! I\u2019d love to know:\n\n*Can I see available_resources on a PlacementGroup?*"}
{"question": "Is the documentation for versions of ray older than 1.11.0 hosted anywhere? I was using 0.8.4 documentation last week and can no longer find it"}
{"question": "Hi All,\n\nWould like to know how ray.wait() internally works. Like does it make REST call's several times until the values are available in store? Wanted to understand this to know on how ray can handle long running tasks.\n\nThank You"}
{"question": "Hi All,\n\nI just got started with Ray for my distributed ml project. I have 2 containers running on 2 different machines. The containers are built with the same docker image where Ray is installed. The communication between the 2 nodes via ssh works well. Then I manually start Ray by running \"ray start --head\" on one machine and \"ray start --address='10.165.9.54:6379'\" on the other. Everything seems normal, but when I run \"ray.init(address='auto')\" in my python script, I got a lot of these errors:\n\n``` (pid=gcs_server) [2022-08-13 16:49:02,724 E 5632 5632] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details:\n(pid=gcs_server) [2022-08-13 16:49:03,724 E 5632 5632] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details:```\nCan anyone in the group help me out on this issue? I couldn't find any answer on the web except <https://discuss.ray.io/t/actor-died-unexpectedly-grpcunavailable-failed-to-connect-to-all-addresses/6640|this one>, but I am using Linux machines and I do not use any Python IDE.... Thanks in advance!!"}
{"question": "<@UNCRYAV9N>: We had a discussion about logs a while back. Has there been any changes in the timeline for structured logging via /tmp/ray/* ? Our team is  very much interested in JSON log output for everything if possible."}
{"question": "Hi, does anyone have this grpc \"Exception iterating requests!\" error when connecting the ray head through a proxy? <https://github.com/ray-project/ray/issues/23865>"}
{"question": ":wave: Hi everyone! Are you interested in helping us and :fire: earning a reward at the same time? :star-struck:\nWe\u2019re looking for some volunteers(Incentive provided) to show us how they complete some tasks using Ray 2.0 so we can improve the user experience.\n:point_right: All you need to do is <https://airtable.com/shrpEdgk3D59lCmdH|register here> and provide us with some basic information on your background and use of Ray. Then, we\u2019ll reach out to schedule a session with you and Ray team members.\nWe\u2019ll send a gift card to everyone who completes the session!\nFeel free to reach out here with any questions, and thanks in advance for all your feedback! :pray:\nJoin <#C03TRK8CL11|collabrate> for future opportunities like this."}
{"question": "Hi, I read a paper named \" Rearchitecting In-Memory Object Stores for Low Latency\", in which a in-memory object store named lighting is proposed. In any case, do you know that if lighting is used in Ray or not?"}
{"question": "Hi Ray community,\n\nI'm wondering how Ray manages the data sessions storage. I'm using Ray cluster on premise, and the tmp folder where Ray is storing the sessions data is getting bigger and bigger, resulting in a few errors due to no space left on the disks (As an example, I have one session of 25 Go, mostly some spilled objects :boom:).\nWhy Ray is not cleaning the session data after the job has finished?\nIs there a way to tell Ray to do so, or should I do it myself?\n\nThank you your great job :slightly_smiling_face:"}
{"question": "Is <https://github.com/ray-project/enhancements/blob/main/reps/2022-04-21-state-observability-apis.md#apis> / <https://github.com/ray-project/enhancements/pull/8> still open for comment?"}
{"question": "Hi, is this the best place to get help with Ray configuration? I\u2019m trying to get a basic dev environment working but can\u2019t get actor logs to show up and also can\u2019t get environment variables to be available in actors. I\u2019m using docker compose to run things in a container. For the logging issue, I\u2019ve looked through all logs in the dashboard and can\u2019t find anything. For the environment variables I\u2019ve tried adding them as a `runtime_env` parameter to `ray.init()`. I\u2019m wondering if there\u2019s something basic I\u2019m missing?\nFor background I\u2019m using serve to launch a workflow that uses data. Could the problem be actors spawning other actors?"}
{"question": "Hi folks, I am currently using xgboost-ray for my project. I have set up a ray cluster with 2 nodes where I set the number of actors to 2 and CPU numbers per actor to 96 (since my machine has 96 cores) and my xgboost-ray training job run through. My data are all parquet files and sit on the head node. However, compared to the original xgboost library on one single machine, the performance with the distributed one is 4x downgrade. Does anyone have an idea what could be the cause of this issue? For any tip or suggestion or comment, I would be very grateful!!"}
{"question": "Hi,  We are deploying Ray 2.0 as part of our infrastructure and we were first making sure everything passed security tests/scanning, so we have run a dockle scan on the Ray image and it has prompted us about issue with sudo usage.\nIs this something you're aware of and planning to address part of your image security hardening?"}
{"question": "Hi team, can the data that ray stores on an external redis be used to restore the headnode state after a crash? If no, then what would be some use cases for using external redis?"}
{"question": "Hello All - We have been banging our heads against building Ray for linux on arm64 (specifically via Docker on Apple Silicon) for weeks without any luck. As I understand it, there is a build script for building Ray from source on MacOS/ARM64 but there isn\u2019t one (nor a wheel) for using Ray on Linux/ARM64 (which Docker requires).\n\nNote that we can use an emulation layer for AMD64 which allows us to install Ray in a container running on top of MacOS/ARM64 but it runs *VERY* slow.\n\nHas anyone had success building Ray for Linux/ARM64?  We would be willing to contribute a PR if we could have a quick call with someone who is familiar with Ray\u2019s build processes."}
{"question": "Hi team, does *@ray.remote* have any plans to support *retry_delay*?"}
{"question": "*Aww man* :sob: *did you all remove the `ray.objects()` API?* I can\u2019t invoke it anymore.\n\nI\u2019m trying to figure out if an object `ref` is on my current node so I can know whether to read a file from disk instead of trying to read from Plasma on a different node, since that will incur a massive IPC cost.\n\nI basically just want to know which object `ref`s are located on which node IDs."}
{"question": "What is the expected maximum scale for running Ray? I\u2019m starting to get various issues (workers dying, head node perf issues, dashboard hanging, ray.wait slowness) with around 500 nodes (5000 workers) and 50k tasks. (I\u2019m using Ray Core API.)"}
{"question": "How does one run <http://Ray.io|Ray.io>\u2019s docker image in a small test system? For example <https://hub.docker.com/r/rayproject/ray/tags?page=1&amp;name=1.13.1-py38-cpu> ? There is no CMD or ENDPOINT in those images.  ?"}
{"question": "Is there any example of using Ray on SageMaker for non-RL use cases?\nFor example, there is one for Dask (and I found one for pyspark as well): <https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_processing/feature_transformation_with_sagemaker_processing_dask/feature_transformation_with_sagemaker_processing_dask.ipynb|https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker_processing/fe[\u2026]ask/feature_transformation_with_sagemaker_processing_dask.ipynb>\n\nIn  the example above there are two instances and a dask scheduler handing tasks. It will be good to see something similar with Ray as well..."}
{"question": "Is it normal to have bunch of dead nodes seen in the dashboard? The k8s worker containers have no restarts."}
{"question": "Hi, how do I use my Ray Cluster within ray.init()?\nI have deployed it to my AWS EKS with a HOST url to access it with port forwarding for the applications set through Ingress like so:\nHOST -&gt; Ray Dashboard(8265)\nHOST/client -&gt; Ray Client(10001)\nHOST/serve -&gt; Ray Serve(8000)\n\nWhen I try to init the cluster locally using:\n```ray.init(address=\"<https://HOST/serve>\", namespace=\"test\")```\nBut I get the following error `RuntimeError: Module: https does not exist.`\nI can submit a job using a POST request to the endpoint <https://HOST/api/jobs> and it is successfully run."}
{"question": "Running a simple ray head and worker node in docker-compose is proving to be a major headache.  Does anyone have a known good minimalist configuration?"}
{"question": "When attempting to schedule an actor over the resource limit the requesting process simply hangs and prints information about missing resources. Is there a way to force Ray to raise an exception and return control to the requesting process when this happens?\n\nedit - if this is the wrong channel for this type of question, let me know and I\u2019ll happily move this"}
{"question": "anyone know how to run ray on minikube?"}
{"question": "How do you serialize and deserialize an ObjectRef? Calling `str(ref)` on it gives e.g. \u201cObjectRef(9509a174e60608d3ffffffffffffffffffffffff0100000001000000)\u201c. Is there a way to turn that back into a usable ObjectRef? Is there another way?"}
{"question": "Congrats on Ray Summit, that was outstanding!\nI'm curious, has there been any support in Ray for handling storage-class memory (SCM) ?  For example, could we schedule _placement groups_ with respect to different tiers of memory storage?"}
{"question": "I\u2019ve being combing through the docs, and still couple of basic questions remain unclear:\n1. Does ray-head do any work? Because example in `ray-cluster.autoscaler.large.yaml` gives it large amount of resource, and I thought head shouldn\u2019t be doing any heavy workload.\n2. What\u2019s the difference between `task` and `job` in ray? Conceptually"}
{"question": "hello, I want to use the ray workflow, but I found in the version2.0 the new API .bind. In your example the output of last node must be the input of next node to build the DAG, how I build the DAG without that?for example, the first step in the workflow write something to a file, and the next step will read the file"}
{"question": "Hello, I wanna ask a question. I previous using ray.remote API and I am happy with it. Now there is a workflow API, I am confused, they are APIs almost same, except workflow not running eager mode. What's the pros and cons?"}
{"question": "Also, anybody could let me know, how to make the Actor support dictionary? I really need this feature, currently it not supported.\n\n```\n@ray.remote\nclass GatherAll:\n\n    def __init__(self) -&gt; None:\n        pass\n\n    # def infer(self, hands, kpts):\n    #     if hands.shape[0] &gt; kpts.shape[0]:\n    #         out = hands.copy()\n    #         out[:kpts.shape[0], :] += kpts[..., :4]\n    #         return out\n    #     else:\n    #         out = kpts.copy()[..., :4]\n    #         out[:hands.shape[0], :] += hands\n    #         return out\n\n    def infer(self, datas):\n        hands = datas['a']\n        kpts = datas['b']\n        if hands.shape[0] &gt; kpts.shape[0]:\n            out = hands.copy()\n            out[:kpts.shape[0], :] += kpts[..., :4]\n            return out\n        else:\n            out = kpts.copy()[..., :4]\n            out[:hands.shape[0], :] += hands\n            return out\n\n\nP = PersonDetector.remote()\nK = KptsDetector.remote()\nH = HandDetector.remote()\nG = GatherAll.remote()\n\nimg = []\nboxes = P.infer.remote(img)\nkpts = K.infer.remote(img, boxes)\nhands = H.infer.remote(img)\n\n# out = gather.remote(hands, kpts)\n# out = G.infer.remote(hands, kpts)\nout = G.infer.remote({'a': hands, 'b': kpts})```"}
{"question": "Hello, I have a question about monitoring ray. What is the recommended way to monitor if ray is up and healthy?"}
{"question": "Hello, I wanna ask a question. I previous using ray.remote API and I am happy with it. Now there is a workflow API, I am confused, they are APIs almost same, except workflow not running eager mode. What's the pros and cons?"}
{"question": "May I ask why ray has so much overhead? 400ms!\n\nthis program indicates it:\n\n```import ray\nimport numpy as np\nimport time\n\n@ray.remote\nclass PersonDetector:\n    \n    def __init__(self) -&gt; None:\n        self.model = self._init_model()\n    \n    def _init_model(self):\n        s = np.random.random([100, 4])\n        return s\n\n    def infer(self, img):\n        b = self.model[0:np.random.randint(100), :]\n        # random batch boxes\n        print(b.shape)\n        time.sleep(4)\n        return b\n\n@ray.remote\nclass KptsDetector:\n\n    def __init__(self) -&gt; None:\n        self.model = self._init_model()\n    \n    def _init_model(self):\n        s = np.random.random([100, 17])\n        return s\n    \n    def infer(self, img, boxes):\n        sh1 = boxes.shape[0]\n        kpts = self.model[0: sh1, :]\n        time.sleep(2)\n        return kpts\n\n\n@ray.remote\nclass HandDetector:\n    \n    def __init__(self) -&gt; None:\n        self.model = self._init_model()\n    \n    def _init_model(self):\n        s = np.random.random([100, 4])\n        return s\n\n    def infer(self, img):\n        b = self.model[0:np.random.randint(100), :]\n        # random batch boxes\n        data = {}\n        data['hands'] = b\n        time.sleep(3)\n        return data\n\n@ray.remote\ndef gather_all(hands, kpts):\n    t0 = time.time()\n    if isinstance(hands, dict):\n        print(f'in hands info: {hands.keys()}')\n        hands = hands['hands']\n    if hands.shape[0] &gt; kpts.shape[0]:\n        out = hands.copy()\n        out[:kpts.shape[0], :] += kpts[..., :4]\n    else:\n        out = kpts.copy()[..., :4]\n        out[:hands.shape[0], :] += hands\n    print(f'[gather time] {time.time() - t0}')\n    return out\n\n# how to written DAG in classes?\nP = PersonDetector.remote()\nK = KptsDetector.remote()\nH = HandDetector.remote()\n\nt0 = time.time()\nimg = []\nboxes = P.infer.remote(img)\nhands = H.infer.remote(img)\nkpts = K.infer.remote(img, boxes)\n\n# out = gather.remote(hands, kpts)\nout = gather_all.remote(hands, kpts)\nt1 = time.time()\nprint(t1 - t0)\n\nout = ray.get(out)\nt2 = time.time()\nprint(t2 - t0)\nprint(t2 - t1)\nprint(out.shape)```\nI want HandDetector time running semutenously along with Person + Kpts, it should expected cost 6s (Person + Kpts, Hand running meanwhile), but I got 400ms overhead in ray.\n\nHow to reduce this overhead ?"}
{"question": "Any body can see this issue??? Tooooo much overhead in ray!! <https://github.com/ray-project/ray/issues/7511#issuecomment-1229114440>"}
{"question": "Hi.\n\nIs there a way to store some objects in Plasma store at cluster creation and then retrieve them inside jobs that are submitted to this cluster?"}
{"question": "Hi, Has anyone tried using memray <https://github.com/bloomberg/memray> with ray? I see in ray documentation some steps to profile - <https://docs.ray.io/en/latest/ray-contribute/profiling.html> but I'm looking for simple solution with memray.\n\nI encountered this error when starting ray with memray (`memray run file.py`)\n\n`Unable to connect to GCS at 10.0.2.15:51744. Check that (1) Ray GCS with matching version started successfully at the specified address, and (2) there is no firewall setting preventing access.`"}
{"question": "Hi again.\n\nHow one can fix the following warning?\n\n```(raylet) [2022-08-29 15:02:25,150 E 1207555 1207585] (raylet) <http://file_system_monitor.cc:105|file_system_monitor.cc:105>: /tmp/ray/session_2022-08-29_14-56-13_079159_1207502 is over 95% full, available space: 0; capacity: 844438069248. Object creation will fail if spilling is required.```\nThis seems have no connection with Plasma storage as increasing `--object-store-memory` seems to have no effect on warning fixing."}
{"question": "Hello. I\u2019m trying to run a Ray cluster with an external Redis instance. It works fine with a single master node in Redis, but not with a Redis cluster that has multiple master nodes. Is this possible at all in the current version of Ray?"}
{"question": "Hi all,\nI am trying to use Ray to find hidden layer size with keras but I got this error\n\nTune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override Trainable.default_resource_request if using the Trainable API. 2022-08-30 04:14:49,732 WARNING trial_runner.py:1575 -- You are trying to access _search_alg interface of TrialRunner in TrialScheduler, which is being restricted. If you believe it is reasonable for your scheduler to access this TrialRunner API, please reach out to Ray team on GitHub. A more strict API access pattern would be enforced starting 1.12s.0 (BroadModel pid=17115) 2022-08-30 04:14:54.026559: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n\n\nI use this code:\n\nif __name__ == \"__main__\":\n    import ray\n    from ray.tune.schedulers import PopulationBasedTraining\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\"\n    )\n    parser.add_argument(\n        \"--server-address\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"The address of server to connect to if using Ray Client.\",\n    )\n    args, _ = parser.parse_known_args()\n\n    if args.smoke_test:\n        ray.init(num_gpus=1)\n        print(num_gpus)\n    elif args.server_address:\n        ray.init(f\"ray://{args.server_address}\")\n\n    pbt = PopulationBasedTraining(\n        perturbation_interval=2,\n        hyperparam_mutations={\n            \"dropout\": lambda: np.random.uniform(0, 1),\n            \"lr\": lambda: 10 ** np.random.randint(-10, 0),\n        },\n    )\n\n    tuner = tune.Tuner(\n    BroadModel,\n        run_config=air.RunConfig(\n            name=\"pbt_babi_memnn\",\n            stop={\"training_iteration\": 4 if args.smoke_test else 100},\n        ),\n        tune_config=tune.TuneConfig(\n            scheduler=pbt,\n            metric=\"mean_accuracy\",\n            mode=\"max\",\n            num_samples=2,\n        ),\n        param_space={\n            \"finish_fast\": args.smoke_test,\n            \"batch_size\": 32,\n            \"epochs\": 1,\n            \"dropout\": 0.3,\n            \"lr\": 0.01,\n        },\n    )\n\n    tuner.fit()\n    \nWhat is my mistake? How can I force ray to use GPU?"}
{"question": "I have tried the ray serve sample code but I am getting this error:\nCaught schedule exception\n2022-08-30 16:51:21,788\tINFO dis.py:448 -- Exception from actor creation is ignored in destructor. To receive this exception in application code, call a method on the actor reference before its destructor is run.\n\nany idea how to debug this? or what is the problem?"}
{"question": "I want to use Ray to find the best hidden layer size for regression problem. I found your example more related to classifications. How could I configure tunConfige for regression problem?\n\ntune_config=tune.TuneConfig(\n            scheduler=pbt,\n            metric=\"mean_accuracy\",\n            mode=\"max\",\n            num_samples=2,\n        ),\n\nI think this is not related to regression problem, because of \"Mean_Accuracy\" but I am not sure about that.\nWhat other metrics I could chose? Can I chose a custom function for metric?\nThanks"}
{"question": "~Can anyone think of a reason that I wouldn't be able to connect to my cluster when the `storage` initArg is set? I'm trying to use the Workflows API which requires `ray[data]`.~\n\n~When I try to run a workflow I get an error that the `storage` init arg isn't set. So, I restarted my cluster with storage set to an existing S3 URI which the cluster has access to. I know this works because a `_valid` object appears at that S3 location. However, once the cluster is up, I can no longer connect to it (my connections time out). I can access the dashboard, but the dashboard is just spinning, unable to fetch any information about the worker nodes. Any ideas? I'm on Ray 2.0.0 on both client and server, and my cluster is configured using the KubeRay helm chart~\n\nFalse alarm: Things seem to have started working after waiting for a while. Possibly some certificate fetching issues."}
{"question": "Hello, I have a question about a server connecting to ray. We have a server that will submit jobs to ray. How can we make sure the server always have a connection to ray?  after calling ray.init once does not work? What is the ray cluster has to be deployed after the server call ray.init?"}
{"question": "Hi all,\ndoes anyone know if there are authentication options for working_dir and py_module URLs in runtime-environments? I try to load zip-files from a private GitLab package registry when using a runtime-environment within a Kuberay RayService yaml definition."}
{"question": "hi all, when creating a actor pool of a ray job with both min_workers and max_workers specified, would actor pool be created as soon as min workers are available or max workers ?"}
{"question": "Hi all, I've a technical question regarding the use of a RAY cluster behind a web server.\nOur architecture is composed of a FastAPI server that for some routes may need the help of specific actors.\nThese routes may be called in concurrency, and the algorithm is able to establish how many actors are required to finish in the optimal time.\nThe idea that we are exploring is:\n\u2022 At startup event of the web server we register the Actors in a specific namespace and with a specific name.\n\u2022 When the route is activated and some actors are required it connect to the ray cluster\n\u2022 claim some Actor\n\u2022 Do the computation\n\u2022 release the Actors and they remain detached for the next call.\nIs this a suitable way?\nthere are problems in connecting and asking Actors for all single call?\nDoes someone have some experience in a similar configuration and can confirm the performance of this solution?\n\nThank you in advice"}
{"question": "Is there anything resembling an idiomatic seeding for RLlib and gym combination?\n\nHere are stuff I'm thinking about:\n\nI'm aware that tune algorithm has to be seeded, and that each worker and each env in each worker should get its own seed. Also, I know that in theory both `np.random` and Python's `random` have to be seeded. Gym's observation also has a capability to do seeding so it can be sampled reproducibly.\n\nObservation sets internal ._np_random variable but I'm not sure how/when does it use it.\nIn new gym versions, `env.reset` has seed as the argument and now that's a recommended way to do seeding. I'm not sure if RLlib uses that functionality and if yes - how.\n\nAdditional questions occur when env is wrapped with something that changes action space (wrapper extends `gym.ActionWrapper`). If sampling is done from the wrapper than wrapper's observations aren't seeded whatever was done inside the env. Should the logic for determining worker/env seeding be replicated in the wrapper? That seems as a bad practice.\n\n<https://numpy.org/doc/stable/reference/random/generator.html|Numpy> has moved on from making global seeding to creating rng objects and using them explicitly for random value generation.\nContrary to that, setting global `np.random` and `random` seed in each `env.__init(...)___` inside vectorized env will create a conflict. In the end, the seed of latest environment will be a common seed for all environments on the same worker (inside a vectorized env). Thus, each of the environments will still not be reproducible on its own, even with the correct seed.\n\nHow do people tackle seeding in a systematic way? Is there an idiomatic way to do this?"}
{"question": "Hello, everyone!\nI ran into a problem using ray.remote() tasks:\nOne of the our modules use `inspect.getsource()` function to build up proper query from Python lambda/function passed as an argument. But remote run broke, as it isn\u2019t possible to get source code without having a source file locally, which of course stays on a client machine.\nIs there any possible solution for such a problem? Could I somehow customize serialization to prevent this issue?\nThanks in advance!"}
{"question": "When working in `MyCallback(DefaultCallbacks)`, is it possible to get info from all environments in vectorized environemnt in `.on_episode_step` ?\nSo despite `len(base_env.vector_env.envs)` &gt; 0, I get only one object returned from `[episode.last_info_for(agent_id) for agent_id in base_env.get_agent_ids()]` . I would expect to get `len(base_env.vector_env.envs)`  info's in that case. Is that functionality possible in any way?"}
{"question": "Does anyone have a workaround, or additional info on <https://github.com/ray-project/ray/issues/19834> ?\n\nThis is a blocker for us"}
{"question": "Hi all,\nI am trying to use PBT for optimizing the size of hidden layer but I got this warning:\n`WARNING trial_runner.py:1575 -- You are trying to access _search_alg interface of TrialRunner in TrialScheduler, which is being restricted. If you believe it is reasonable for your scheduler to access this TrialRunner API, please reach out to Ray team on GitHub. A more strict API access pattern would be enforced starting 1.12s.0`\nWhat does it mean?"}
{"question": "Hi! when does the videos from Ray Summit 22 will be available?"}
{"question": "Hi guys,\nWe are trying to port some processing pipeline to a ray cluster and among things to do, we had to implement dedicated serializer/deserializer for complex objects passed on with `ray.util.register_serializer`.\n\nThis is working fine for a single task but when the pipeline get more advanced (basically a task calling 2 other tasks in sequence) then it seems the registration is lost somewhere...\n```TypeError: Could not serialize the argument &lt;object```\n(basically the same error we had before the implementation of serializer/deserializer).\n\nAny pointer here? Should we register the serializer/deserializer in any sub tasks?"}
{"question": "What's the best way to compare models across datasets?\n\nHi all,\nI have a work project where I have 10000 datasets, each dataset consists of Training, Validating, Testing data.\nThe data-mining/machine learning problem is a binary classification problem. And of course there are a bunch of models to compare and select, e.g. Logistic Regression, xgboost, SVC, Deep Learning etc.\nI have to compare classification performance metrics across all datasets and all models.\nSo the work-flow is:\n\n\nfor model in models:\n```  for data in datasets:\n\n         train the model, and collect testing metrics.\n\n  collect metric numbers for all data and calculate their mean.```\ncompare the means across models and make a table of the metrics for all model.\n\nIs there an easy-to-use framework already present for the above model-compare/selection process?\nSince these are common models, I would imagine there are already out-of-box solutions for such model comparison.\nCould anybody give me some pointers?\nThanks a lot!"}
{"question": "Help: Ray 2.0.0 on Windows 10 runs slower and slower and then hangs!\n--------------------------------------------------------------\nOS: Win 10\nCPU: AMD 64-Core\nMemory: 128GB\nEnv: Anaconda (the latest version). \u201cbase\u201d env\nPython: 3.9.12\nRay: 2.0.0 for cp39. I installed via downloading the whl version and then pip install.\nHarddrive: both C and D drives has more than 128GB available.\nI kept the num_cpus small. And I also made sure (by eye-watching) at any time the Windows task manager resource viewer\u2019s Memory Usage is &lt; 50%.\nMy code snippet:\n------------------------------------------------------\n\nray.init(num_cpus=20)\n# all the huge_arrays are numpy arrays. And they total around 15-20GB.\nobjref1 = ray.put(huge_array1)\nobjref2 = ray.put(huge_array2)\nobjref3 = ray.put(huge_array3)\nobjref4 = ray.put(huge_array4)\nfutures = [ ]\nfor p in range(10000):\n      futures.append(processing_one_point_at_a_time.remote(p, objref1, objref2, objref3, objref4 ))\nresults = ray.get(futures)\n\n\nObservations:\n\n1. Without any error, it runs slower and slower and eventually reached freezing mode when i increases to 1000. At that time, the Windows Task Manager resource viewer shows CPU usage is around 20% and memory usage is around 30%.\n2. Inside the loop, there is Scikit-Learn training function, which has a parallel mechanism itself(multithreading?), in which I assigned n_jobs = 40 to it.\n3. So my estimation was: Ray uses num_cpus=20, and Scikit-Learn training uses n_jobs=40, together they sum to 60, which is less than 64.\n4. I have timed it and found that one iteration executes around 70 seconds. So initially Ray runs very fast with issuing 20 task in parallel. And then it gets slower and slower, and finally it freezes at 1000.\nIt seems that Ray does not allow dispatching all 10000 tasks all at the same time?\nIt looks it might be better to send out jobs in batches, for example, every 100 jobs in one batch?\nBecause the job stalled, now it is even worse than single process, which defeats the initial goal of using Ray, I am beating my head against wall.\nPlease help me and shed some lights on me!\nThanks a lot!"}
{"question": "Hi all,\nI am trying to tune a Keras model by PBT in ray tune.\nBut I got some weird errors when I am increasing the number of iterations such as\n```convA2 = Conv1D(config[f'convAn_{i}'],11,padding='same',activation='relu')(convA1)\nKeyError: 'convAn_0'```\nfor this part of my code\n```convA1 = Conv1D(config[\"Conv1DA\"],11,padding='same',activation='relu')(x1)\n        for i in range(config[\"Conv1DAn\"]):\n            if i &gt; 0: \n                convA2 = Conv1D(config[f'Conv1DAn_{i}'],11,padding='same',activation='relu')(convA1)\n        poolA = MaxPooling1D(3)(convA1)```\n:neutral_face::smiling_face_with_tear: if i &gt; 0 then it must execute the `convA2 = Conv1D(config[f'Conv1DAn_{i}'],11,padding='same',activation='relu')(convA1)`, so we don't have any `convAn_0`\nOr some times I get these errors:\n1\n```UnboundLocalError: local variable 'poolA' referenced before assignment```\n2\n```NameError: name 'poolB' is not defined```\n3\n```convA2 = Conv1D(config[f'convAn_{i+1}'],11,padding='same',activation='relu')(convA1)\nKeyError: 'convAn_1'```\n4\n```optimizer=Adam(model.parameters(),learning_rate= config.get(\"lr\",0.01)),\nAttributeError: 'Functional' object has no attribute 'parameters'\nresult = self.step()   File \"ray_test.py\", line 258, in step\nself.model.fit( AttributeError: 'BroadModel' object has no attribute 'model'```\nThe model\n```class BroadModel(tune.Trainable):\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n    def build_model(self, config):\n        global convB2, drop2, convA2, poolA, poolB \n        window_size = 200\n        self.x_gyro, self.x_acc, x_mag, q = load_data_train()\n        self.Att_quat = Att_q(q)\n        self.x_gyro_t, self.x_acc_t, x_mag_t, q_t = load_data_test()\n        self.Att_quat_t = Att_q(q_t)\n\n        self.x_gyro, self.x_acc, self.Att_quat = shuffle(self.x_gyro, self.x_acc, self.Att_quat)\n        x1 = Input((window_size, 3), name='x1')\n        x2 = Input((window_size, 3), name='x2')\n        convA1 = Conv1D(config[\"Conv1DA\"],11,padding='same',activation='relu')(x1)\n        for i in range(config[\"Conv1DAn\"]):\n            if i &gt; 0: \n                convA2 = Conv1D(config[f'Conv1DAn_{i}'],11,padding='same',activation='relu')(convA1)\n        poolA = MaxPooling1D(3)(convA1)\n        \n        \n        convB1 = Conv1D(config[\"Conv1DB\"],11,padding='same',activation='relu')(x2)\n        for i in range(config[\"Conv1DBn\"]):\n            if i &gt; 0:\n                convB2 = Conv1D(config[f'Conv1DBn_{i}'],11,padding='same',activation='relu')(convB1)\n        poolB = MaxPooling1D(3)(convB1)\n        AB = concatenate([poolA, poolB])\n        \n        lstm1 = Bidirectional(LSTM(config[\"LSTM1\"], return_sequences=True))(AB)\n        drop1 = Dropout(config['dropout'])(lstm1)\n        for i in range(config['LSTMn']):\n            if i &gt; 0:\n                lstm2 = Bidirectional(LSTM(config[f'LSTMn_{i}'], return_sequences=True))(drop1)\n                drop1 =  Dropout(config['dropout'])(lstm2)   \n        lstm2 = Bidirectional(LSTM(config['LSTMn_l']))(drop1)\n        drop2 =  Dropout(config['dropout'])(lstm2)\n        y1_pred = Dense(4,kernel_regularizer='l2')(drop2)\n        model = Model(inputs =[x1, x2], outputs = [y1_pred])\n        return model\n    def setup(self, config):\n        \n        model = self.build_model(config)\n    \n        model.compile(\n            optimizer=Adam(learning_rate=config['lr']),\n            loss=quaternion_mean_multiplicative_error,\n            metrics=[quaternion_mean_multiplicative_error],\n        )\n        self.model = model\n        return model```\nTuner\n```if __name__ == \"__main__\":\n    import ray\n    from ray.tune.schedulers import PopulationBasedTraining\n    \n    pbt = PopulationBasedTraining(\n        perturbation_interval=600,\n        hyperparam_mutations={\n            \"dropout\": tune.uniform(0.1,0.5),\n            \"lr\": tune.uniform(1e-5,1e-3),\n            \"Conv1DA\": tune.randint(10,15),\n            \"Conv1DAn\": tune.choice([0,1,2,3]),\n            \"Conv1DAn_1\": tune.randint(10,15),\n            \"Conv1DAn_2\": tune.randint(10,15),\n            \"Conv1DAn_3\": tune.randint(10,15),\n            \"Conv1DB\": tune.randint(10,15),\n            \"Conv1DBn\": tune.choice([0,1,2,3]),\n            \"Conv1DBn_1\": tune.randint(10,15),\n            \"Conv1DBn_2\": tune.randint(10,15),\n            \"Conv1DBn_3\": tune.randint(10,15),\n            \"LSTM1\": tune.randint(10,15),\n            \"LSTMn\": tune.choice([0,1,2,3]),\n            \"LSTMn_1\": tune.randint(10,15),\n            \"LSTMn_2\": tune.randint(10,15),\n            \"LSTMn_3\": tune.randint(10,15),\n            \"LSTMn_l\": tune.randint(10,15)},\n        \n    )\n    resources_per_trial = {\"cpu\": 10 , \"gpu\": 0}\n    tuner = tune.Tuner(\n         tune.with_resources(\n        BroadModel,\n        resources=resources_per_trial),\n        run_config=air.RunConfig(\n            name=\"BroadPBT\"+timestr,\n            stop={\"training_iteration\": 50}),\n        tune_config=tune.TuneConfig(\n            reuse_actors=True,\n            scheduler=pbt,\n            metric=\"loss\",\n            mode=\"min\",\n            num_samples=2),\n        param_space={\n            \"finish_fast\": False,\n            \"batch_size\": 1000,\n            \"epochs\": 200,\n            \"dropout\": tune.uniform(0.1,0.5),\n            \"lr\": tune.uniform(1e-5,1e-3),\n            \"Conv1DA\": tune.randint(10,15),\n            \"Conv1DAn\": tune.choice([0,1,2,3]),\n            \"Conv1DAn_1\": tune.randint(10,15),\n            \"Conv1DAn_2\": tune.randint(10,15),\n            \"Conv1DAn_3\": tune.randint(10,15),\n            \"Conv1DB\": tune.randint(10,15),\n            \"Conv1DBn\": tune.choice([0,1,2,3]),\n            \"Conv1DBn_1\": tune.randint(10,15),\n            \"Conv1DBn_2\": tune.randint(10,15),\n            \"Conv1DBn_3\": tune.randint(10,15),\n            \"LSTM1\": tune.randint(10,15),\n            \"LSTMn\": tune.choice([0,1,2,3]),\n            \"LSTMn_1\": tune.randint(10,15),\n            \"LSTMn_2\": tune.randint(10,15),\n            \"LSTMn_3\": tune.randint(10,15),\n            \"LSTMn_l\": tune.randint(10,15)},\n    )\n    tuner.fit()```\nIf I decrease the number of iterations the code will work fine.\nIs it a bug? How can I solve that?\n\nThanks"}
{"question": "Hi, everyone! Is it possible to prevent Object Spillage ?"}
{"question": "Hi all,\nis there a way to retrieve the URL of the dashboard via code?\nI found this `ray.worker._global_node.webui_url` but maybe not the right way.\nThank you"}
{"question": "hi guys,\nI\u2019m looking into `runtime_env`  option to manage multiple dependency contexts for different tasks. I\u2019ve been trying the `container`  option as it was discuss between <@U02T4STETE1> and <@U01F5TR6BT2> here <https://ray-distributed.slack.com/archives/C01DLHZHRBJ/p1643409879553469>\n\nI wasn\u2019t able to make it work so far with raylet on the tasks tagged with specific container failing to connect to the cluster (specifically on GCP cluster).\n\nSo how stable is that option? Should I continue to invest on it or would it be better to look for a simpler dependency management alternative (pip or all-in-one worker container)"}
{"question": "Looking for good AutoML to run with Ray or run under Ray?\n\nCould anybody please point me to good AutoML that can run with/under Ray?\n\nI plan to use Ray for the outer parallel loops and use AutoML in the inner part to find good trained models for each job of the outer loop.\n\nLets say I need to run the training-validation-testing 10000 times, I am thinking of to use Ray remote function to dispatch 10000 jobs, with each job is an AutoML run on different data-sets.\n\nMy headache problems are:\n\n1. I am using Win 10, and I found auto-sklearn for Win10 needs to run in Ubuntu batch shell mode or WSL Linux mode. (I cannot switch to Linux so have to stick with Win10 for now)\n<https://automl.github.io/auto-sklearn/master/installation.html#windows-macos-compatibility>\n<https://github.com/automl/auto-sklearn/issues/431>\n<https://github.com/automl/auto-sklearn/issues/860>\n<https://github.com/automl/auto-sklearn/issues/755>\n1. Can Ray be used together with Ubuntu batch shell or WSL Linux mode on Win10?\n2. If the above 2 is not a good idea, what about running the AutoML jobs each in Virtual Machines or Docker?\n3. All I want is a good AutoML together with Ray on Win10. Could anybody give some pointers?\nThanks a lot!"}
{"question": "I'm running into an issue when starting the head node using the latest nightly wheel inside docker.  The 2.0.0 tag works (`ray[default,serve]==2.0.0`), but the nightly builds fail with the following error:\n\n```t5-srv_1    | 2022-09-06 21:06:11,195\tERR scripts.py:937 -- Some Ray subprocesses exited unexpectedly:\nt5-srv_1    | 2022-09-06 21:06:11,195\tERR scripts.py:941 -- raylet [exit code=1]\nt5-srv_1    | 2022-09-06 21:06:11,196\tERR scripts.py:948 -- Remaining processes will be killed.\nt5-srv_1    | exit EXIT_CODE=1```\nIs there a way to specify the wheel for a specific build/commit in requirements.txt?\n\nrequirements.txt\n```ray[default,serve] @ <https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp310-cp310-manylinux2014_x86_64.whl>```"}
{"question": "What is the `aiohttp`  version required by Ray ?  We got this error:\n```module 'aiohttp' has no attribute 'web'```\nWe are at ray 1.3.0"}
{"question": "Hey everyone,\n\nI need a little assistance with my codebase. I am confused about autoscaling Ray on Kubernetes. I m trying to use Ray tune for tuning some hyperparameters. I have a running Kubernetes cluster, on which I have setup my RayCluster using Helm. Could anyone tell me how to scale-up automatically to run each configuration of the hyperparameter tuning on a separate pod? Will RayTune automatically do this when I run the task? Or do I have to configure this beforehand? If so, where? Thank you!"}
{"question": "Hello, I'm trying to use ray tune for hyperparameter fitting XGBoost models, and trying to do so from a DataBricks cluster following the instructions at <https://databricks.com/blog/2021/11/19/ray-on-databricks.html> .  So far no luck; has anyone been able to do so successfully?"}
{"question": "There\u2019s significant overhead (10 seconds) with spinning up a Ray Task/Actor in our system when it references user code that is globally scoped, even when it is not called. For example:\n```from rest_of_codebase import bar\n\n@ray.remote\ndef foo():\n    bar # bar is intentionally not called to illustrate overhead latency```\nThe overhead isn\u2019t better even if we do this:\n```@ray.remote\ndef foo():\n    from rest_of_codebase import bar\n    bar```\nThe latency of the actual code execution is tiny, but somehow, the Task/Actor spinup overhead is massive. I\u2019m not running into dependency or import errors. Is there any way to avoid this?\n\nThere is no overhead if we do this, or execute any code where any dependencies come from pip instead of from user code:\n```@ray.remote\ndef foo():\n    a = np.random.random((20000, 256))\n    b = np.random.random((256, 1))\n    return np.dot(a, b)```"}
{"question": "Hey! I\u2019m using ray\u2019s tune with pytorch lightning. Is there any way to disable the lightning prints? I\u2019ve tried `logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)` but it does not work when using `tune.Tune`  (If I just run pytorch lightning, that line works)"}
{"question": "I\u2019m having trouble with Ray finding the machine\u2019s GPU. I get the error `No CUDA GPUs are available` when running map_batches() on a data set with an actor pool of size (1,1) on a machine with a single GPU. When I inspect the state `ray.cluster_resources()['GPU']` returns 1, and `torch.cuda.is_available()` returns True, but `ray.get_gpu_ids()` returns an empty array. I\u2019m trying to containerize some code that ran fine in a Colab notebook. Any thoughts on what\u2019s going wrong?"}
{"question": "and this variable a will be deleted automatically by Ray if I don't use it any more. Am I right?"}
{"question": "Hi! About observation spaces. If I get it right, RLlib internally maps all given observation Spaces (especially Dict and Tuple) to Box. Question: Where can I find the method that does that?\n\nHere is a problem where this would be handy: <https://discuss.ray.io/t/how-to-flatten-space-when-action-masking/6070>"}
{"question": "Hey there! I'm one of the co-founders @ <https://proteinqure.com|ProteinQure> and we've build a large part of our cloud stack for protein drug design on Ray :rocket: thanks for building such an awesome tool!\n\nLately I've been experimenting with the new Ray Workflows feature and I've been running into a weird issue when trying to pass object references between tasks and was hoping you could help me with that. The following example is pretty much taken 1-to-1 from the documentation <https://docs.ray.io/en/latest/workflows/basics.html#passing-object-references-between-tasks|here>. When running on a local Ray instance it works perfectly fine. However, once I try to run it on a Ray cluster it ends up erroring out (see error message in thread).\n\n```import sys\n\nimport ray\nfrom ray import workflow\n\n\nLOCAL_RAY = len(sys.argv) == 2 and sys.argv[1] == \"--local\"\nif LOCAL_RAY:\n    # connect to local ray cluster\n    ray.init(storage=\"<s3://some-s3-path>\")\nelse:\n    # connect to remote ray cluster\n    ray.util.connect(\n        \"<http://some-ray-cluster-url.com:443|some-ray-cluster-url.com:443>\",\n        secure=True,\n    )\n\n@ray.remote\ndef do_add(a, b):\n    return a + b\n\n@ray.remote\ndef add(a, b):\n    return do_add.remote(a, b)\n\nworkflow.run(add.bind(ray.put(10), ray.put(20))) == 30```\nThis happens with ray 2.0.0. Has anyone seen this before? Am I doing something wrong that's obvious? Are object references not supposed to work with Workflows? Any help would be greatly appreciated!"}
{"question": "With an increasing amount of jax distributed projects using ray, have yall thought about adding native jax GPU collectives in ray? If so, what would the timeline be for it?\n<https://github.com/ray-project/ray/issues/28472>"}
{"question": "Hi all! Does Ray Core support prioritization of whole batches of jobs? I.e. if one developer submits a large batch of long-running jobs and another developer then submits a small batch of jobs while the former is still running, can it pre-empt execution of the large batch so that the small batch gets to finish quickly? That way, the cluster resources would be used more fairly (completion speed proportional to batch size)."}
{"question": "Hi all,\nI have an issue with Kuberay and hope somebody knows help.\nI start a RayService in K8s, which defines a pip runtime_env. It should need about 3 minutes to install.  But after about 2 and a half minutes the Ray cluster gets terminated and restarted.\nTo rule other issues out, I took the fruit-example from the docs (which runs fine for me) and only added the pip dependencies to the runtimeEnv in the RayService config. I\u2019m not even importing the dependencies in the python code.\nI increased the number of dependencies and it works, as long as the installation time is short enough.\nI watched the folder size increasing in /tmp/ray/session_latest/runtime_resources/pip/ and while the size is still increasing the pods go into status \u201cTerminating\u201d and the cluster get\u2019s recreated.\nI also set all timeout-properties in the RayService config to higher values:\n\u2022 serviceUnhealthySecondThreshold: 900 \n\u2022 deploymentUnhealthySecondThreshold: 900 \n\u2022 autoscalerOptions.idleTimeoutSeconds: 600\nand for each serve-deployment\n\u2022 gracefulShutdownWaitLoopS: 600.0\n\u2022 gracefulShutdownTimeoutS: 600.0\n\u2022 healthCheckPeriodS: 10.0\n\u2022 healthCheckTimeoutS: 600.0\nAre there any other config params or reasons which may lead to a restart of the Ray cluster if the serve-deployments are not running after a specific time?"}
{"question": "I\u2019m looking at using Terraform+Ansible to set up a Ray cluster on GCP. Are there any recipes published for this? Also, is this the recommended pattern or should I use the Ray Cluster Launcher?"}
{"question": "Hi everyone!\nSomebody knows or have and idea why when i run the `full-example.yaml` to run ray cluster it never gets connected?\nI would really appreciate any comment or help\nBest\n\n```(venv) beto@betosMac creative-ai-alpha % ray up example-ful.yaml\nCluster: default\n\n2022-09-14 13:15:35,196 WARNING util.py:234 -- Dropping the empty legacy field head_node. head_nodeis not supported for ray&gt;=2.0.0. It is recommended to removehead_node from the cluster config.\n2022-09-14 13:15:35,196 WARNING util.py:234 -- Dropping the empty legacy field worker_nodes. worker_nodesis not supported for ray&gt;=2.0.0. It is recommended to removeworker_nodes from the cluster config.\n2022-09-14 13:15:35,197 INFO util.py:357 -- setting max workers for head node type to 0\nLoaded cached provider configuration\nIf you experience issues with the cloud provider, try re-running the command with --no-config-cache.\nUpdating cluster configuration and running full setup.\nCluster Ray runtime will be restarted. Confirm [y/N]: Y\n\nUsage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See <https://docs.ray.io/en/master/cluster/usage-stats.html> for more details.\n\n&lt;1/1&gt; Setting up head node\n  Prepared bootstrap config\n2022-09-14 13:15:46,688 INFO node.py:311 -- wait_for_compute_zone_operation: Waiting for operation operation-1663179346355-5e8a71e232439-a742a6d1-645598ae to finish...\n2022-09-14 13:15:53,299 INFO node.py:330 -- wait_for_compute_zone_operation: Operation operation-1663179346355-5e8a71e232439-a742a6d1-645598ae finished.\n  New status: waiting-for-ssh\n  [1/7] Waiting for SSH to become available\n    Running `uptime` as a test.\n    Fetched IP: 10.10.10.4\nssh: connect to host 10.10.10.4 port 22: Operation timed out\n    SSH still not available (SSH command failed.), retrying in 5 seconds.```"}
{"question": "We\u2019re seeing that Ray Worker processes aren\u2019t reused by other clients, even if \u201c<https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/edit#heading=h.675q3tqxg0i7|if no work remains or if enough time has passed>\u201d on the original owner.\n\nI would expect that, after the owner is finished running Tasks, the Worker is released (but is still alive and remains on the same PID) and can be reused by other owners/drivers.\n\nThis is preventing us from sharing Workers in a cluster across drivers, requiring us to spin up new Workers every time a new driver wants to run a Task, even if all Workers are IDLE.\n\n*Why is this happening?*"}
{"question": "Just curious are there any updates on this arm64 builds (linux arm64, e.g., used in a docker on Apple's M1)\n\u2022 in addition to the list below, there is no Ray 2.0 wheel for macos 12 Monterey\n<https://ray-distributed.slack.com/archives/C01DLHZHRBJ/p1661441745582679?thread_ts=1661293763.594009&amp;cid=C01DLHZHRBJ>"}
{"question": "I am using AWS spot instances for scaling.\n\nI am curious to know what will happen if during computing the AWS instance go down.\n\nHow will I get notify and what will be action from ray side?\n\nCan someone please explain"}
{"question": "where i can get example code of pararell gpu use for tensorflow predicit code?"}
{"question": "Hi, I am having trouble with ray 2.0 due to a missing pem file:\nray up ray-aws-full-config.yaml\nCluster: aether\n\n2022-09-19 09:29:51,120\tINFO util.py:357 -- setting max workers for head node type to 0\nChecking AWS environment settings\nPrivate key file /Users/cwiklik/.ssh/ray-autoscaler_29_us-east-1.pem not found for ray-autoscaler_29_us-east-1\nShould this file be generated if missing? I did exist and (for whatever reason) it was deleted.\nAppreciate any help"}
{"question": "Hi All,\n\nI am working on some refactoring of the datasets unit tests. I am having trouble with a test, and appear to get contradictory results when attempting to debug it:\n\n*General Result* from running `python -m pytest -v -s python/ray/data/tests/test_dataset_formats.py`\n\nAll three tests pass with the following messages:\n\n```ray/data/tests/test_dataset_formats.py::test_image_folder_reader_estimate_data_size[64-RGB-30000-4] \u2713  \nray/data/tests/test_dataset_formats.py::test_image_folder_reader_estimate_data_size[32-L-3500-0.5] \u2713 \nray/data/tests/test_dataset_formats.py::test_image_folder_reader_estimate_data_size[256-RGBA-750000-85] \u2713  ```\n*Specific Result* from running `python -m pytest -v -s python/ray/data/tests/test_dataset_formats.py::test_image_folder_reader_estimate_data_size`\n\nAll three tests fail with a similar assertion error:\n\n```        root = \"<example://image-folders/different-sizes>\"\n        ds = ray.data.read_datasource(\n            ImageFolderDatasource(),\n            root=root,\n            size=(image_size, image_size),\n            mode=image_mode,\n        )\n\n        data_size = ds.size_bytes()\n&gt;       assert (\n            data_size &gt;= expected_size and data_size &lt;= expected_size * 1.5\n        ), \"estimated data size is out of expected bound\"\nE       AssertionError: estimated data size is out of expected bound\nE       assert (28043 &gt;= 30000)```\n*System information:*\n\u2022 Windows 11 parent system running wsl2 (ubuntu)\n\u2022 Ray running within WSL on Ubuntu 20.04\n\u2022 Ray version 3.0.0.dev0 (with development symlinks)\n\u2022 Python v3.8.10\n\u2022 pytest v7.0.1\nIt appears to have something to do with the size estimation used during a call to read_datasource, but before the query plan has been executed. (the subsequent result from `ds.fully_executed().size_bytes()` ends up being correct)\n\nMy first question is, does anyone know what could cause the difference in the estimated size between these two testing contexts?\n\nAdditionally, if the data_size is an estimate, is it knowable in advance that it will always be an over-estimate? (hence the [expected_size, expected_size*1.5] bound)\n\nAppreciate any thoughts or ideas anyone may have on this."}
{"question": "When running a Ray workflow via `workflow.run` all the head &amp; worker logs are being printed to stdout. However, when running `workflow.run_async` I can't seem to find a way to reattach to the logs and inspect my currently running workflow. I read the entire Ray Workflow documentation but can't seem to find anything. Is there a convenient way to attach to the logs of an asynchronously running Ray workflow?"}
{"question": "Hello!\nHow do I specify an endpoint url (and possibly credentials for s3) in\n`ray.tune.run(...,sync_config=SyncConfig(upload_dir='<s3://bucket_name>'))` ?\nIt doesn't seem to read correctly from `~/.aws/config` and  `~/.aws/credentials` files, but this may as well be expected behavior."}
{"question": "Hi, I am trying to load a large parquet table into ray dataset. It's ~12GB in size (snappy compressed) consisting of 2 int64 columns. I'm running a local ray cluster with 15 nodes and each set 108GB for `--object-store-memory`.\n```Resources\n---------------------------------------------------------------\nUsage:\n 1.0/520.0 CPU\n 0.00/2100.444 GiB memory\n 57.60/1508.743 GiB object_store_memory```\nIMO there's enough memory in each node to load  the table, but ray keeps on trying to spill 10+ GB objects into the disk. Unfortunately each node has a very small `/tmp` drive mounted (30GB). Why is it trying to spill data to the disk?\n```Map Progress: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [01:00&lt;00:00, 60.86s/it]\n(raylet, ip=149.165.146.210) Spilled 14126 MiB, 14 objects, write throughput 703 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n(raylet, ip=149.165.146.210) Spilled 14245 MiB, 18 objects, write throughput 704 MiB/s.\n...```\nI tried loading the data into pandas and call `Dataset.from_pandas` and I am getting the same issue. What could be going wrong here? My ray version 1.12.1\n```...\n(raylet, ip=149.165.146.210) Spilled 57581 MiB, 40 objects, write throughput 1147 MiB/s.\n2022-09-21 02:22:14,680 WARNING worker.py:1382 -- Traceback (most recent call last):\n  File \"python/ray/_raylet.pyx\", line 894, in ray._raylet.spill_objects_handler\n  File \"python/ray/_raylet.pyx\", line 897, in ray._raylet.spill_objects_handler\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 583, in spill_objects\n    return _external_storage.spill_objects(object_refs, owner_addresses)\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 303, in spill_objects\n    return self._write_multiple_objects(f, object_refs, owner_addresses, url)\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 151, in _write_multiple_objects\n    written_bytes = f.write(payload)\nOSError: [Errno 28] No space left on device\nAn unexpected internal error occurred while the IO worker was spilling objects: [Errno 28] No space left on device\n2022-09-21 02:22:14,682 WARNING worker.py:1382 -- Traceback (most recent call last):\n  File \"python/ray/_raylet.pyx\", line 894, in ray._raylet.spill_objects_handler\n  File \"python/ray/_raylet.pyx\", line 897, in ray._raylet.spill_objects_handler\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 583, in spill_objects\n    return _external_storage.spill_objects(object_refs, owner_addresses)\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 303, in spill_objects\n    return self._write_multiple_objects(f, object_refs, owner_addresses, url)\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 151, in _write_multiple_objects\n    written_bytes = f.write(payload)\nOSError: [Errno 28] No space left on device\nAn unexpected internal error occurred while the IO worker was spilling objects: [Errno 28] No space left on device\n2022-09-21 02:22:14,693 WARNING worker.py:1382 -- Traceback (most recent call last):\n  File \"python/ray/_raylet.pyx\", line 894, in ray._raylet.spill_objects_handler\n  File \"python/ray/_raylet.pyx\", line 897, in ray._raylet.spill_objects_handler\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 583, in spill_objects\n    return _external_storage.spill_objects(object_refs, owner_addresses)\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 303, in spill_objects\n    return self._write_multiple_objects(f, object_refs, owner_addresses, url)\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 151, in _write_multiple_objects\n    written_bytes = f.write(payload)\nOSError: [Errno 28] No space left on device\nAn unexpected internal error occurred while the IO worker was spilling objects: [Errno 28] No space left on device\n2022-09-21 02:22:14,694 WARNING worker.py:1382 -- Traceback (most recent call last):\n  File \"python/ray/_raylet.pyx\", line 894, in ray._raylet.spill_objects_handler\n  File \"python/ray/_raylet.pyx\", line 897, in ray._raylet.spill_objects_handler\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 583, in spill_objects\n    return _external_storage.spill_objects(object_refs, owner_addresses)\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 303, in spill_objects\n    return self._write_multiple_objects(f, object_refs, owner_addresses, url)\n  File \"/N/u/d/dnperera/.conda/envs/cylonflow/lib/python3.8/site-packages/ray/external_storage.py\", line 151, in _write_multiple_objects\n    written_bytes = f.write(payload)\nOSError: [Errno 28] No space left on device\nAn unexpected internal error occurred while the IO worker was spilling objects: [Errno 28] No space left on device\n(raylet, ip=149.165.146.210) Spilled 86700 MiB, 55 objects, write throughput 1727 MiB/s.\n...```"}
{"question": "*In Short: Has anyone tried Flink on Ray?* \n*In Detail:* I had been using Ray for batch jobs and ML workloads and realised that Spark - Ray transition is very smooth. Most of the ETL load is taken care by Spark while ML related aspects are executed over Ray. I wanted to check if anyone has tried Flink on Ray just like we run Spark on Ray. If yes, what were the result(perf wise) and why it is a good/bad choice to consider running Flink on Ray? Idea is trying to run Flink on ray actors with Flink\u2019s Job manager deployed on Ray Head, and Flink\u2019s Task manager running on Ray workers?"}
{"question": "If I set a breakpoint inside an actor does it also pause all other actors? Id think not right."}
{"question": "I recently published book about production-ready deep learning. It might be useful for some people who work on PyTorch and TensorFlow leveraging AWS tools.- <https://www.amazon.com/Production-Ready-Applied-Deep-Learning-TensorFlow/dp/180324366X/ref=asc_df_180324366X/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=598359496472&amp;hvpos=&amp;hvnetw=g&amp;hvrand=8397493717909696533&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9031999&amp;hvtargid=pla-1742211695190&amp;psc=1|https://www.amazon.com/Production-Ready-Applied-Deep-Learning-TensorFlow/dp/18032436[\u2026]=&amp;hvlocint=&amp;hvlocphy=9031999&amp;hvtargid=pla-1742211695190&amp;psc=1> If you would like to get some discount or even a free copy in exchange for Amazon review, please let me know (PM). Ray people if that's not the place please feel free to take it down. One of the chapters talks about Ray so I hope that won't be a problem that I posted here."}
{"question": ":bangbang:On a M1 mac, ray does not work in docker... This makes it very hard to develop even the main job is going to deploy/run somewhere....Can someone from Ray team take a look?\n\u2022 no linux/arm64 build\n\u2022 on standard x86_64 image (QEMU), I kept getting the something similar to the following:\n```Python 3.9.5 (default, Jun  4 2021, 12:28:51)\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.4.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import ray\n\nIn [2]: ray.init()\n2022-09-23 11:19:38,149\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265\n(raylet) [2022-09-23 11:19:45,921 E 706 755] (raylet) <http://agent_manager.cc:134|agent_manager.cc:134>: The raylet exited immediately because the Ray agent failed. The raylet fate shares with the agent. This can happen because the Ray agent was unexpectedly killed or failed. See `dashboard_agent.log` for the root cause.\nOut[2]: RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.5', ray_version='2.0.0', ray_commit='cba26cc83f6b5b8a2ff166594a65cb74c0ec8740', address_info={'node_ip_address': '172.17.0.2', 'raylet_ip_address': '172.17.0.2', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-09-23_11-19-28_019487_634/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-09-23_11-19-28_019487_634/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-09-23_11-19-28_019487_634', 'metrics_export_port': 56912, 'gcs_address': '172.17.0.2:56181', 'address': '172.17.0.2:56181', 'dashboard_agent_listen_port': 52365, 'node_id': '2262baa017ded791ebbd7f88049a9a61ab8ed79096de46d548f398b5'})\n\nIn [3]: 2022-09-23 11:20:14,409\tWARNING worker.py:1829 -- The node with node id: 2262baa017ded791ebbd7f88049a9a61ab8ed79096de46d548f398b5 and address: 172.17.0.2 and node name: 172.17.0.2 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a \t(1) raylet crashes unexpectedly (OOM, preempted node, etc.)\n\t(2) raylet has lagging heartbeats due to slow network or busy workload.```"}
{"question": "Is creating a ray actor with class.remote() synchronous? I get that function calls are async. Seems like I can't do Ray.get on an actor handle so I assume it's synchronous? If not how can I block it?"}
{"question": "Is this a docs bug, or is there something I don\u2019t understand: <https://docs.ray.io/en/master/ray-core/actors/patterns/concurrent-operations-async-actor.html>\nThe `___init___` for `LongPollingActorAsync` expects an argument, but in the snippet where it\u2019s used there\u2019s nothing passed in for `data_store_actor`?"}
{"question": "Also in the `run` method, it refers to `data_store_actor`, whereas presumably it should be `self.data_store_actor` ?"}
{"question": "My dataclass value become empty when using ray remote function. Does anyone run into same issue before?"}
{"question": "Hi team (apologies if this isn't the right place to ask these questions) - I just signed up for access to AnyScale Cloud platform and we would love to try it out as soon as possible for our production workloads. It seems like a great solution for us - just wondering how long wait time usually is to get access? Happy to help with reporting bugs etc."}
{"question": "With Ray - I would like to check also that it is absolutely required to download all data first and store it locally on computer? It doesn't work on databases at the moment right? I tried writing a custom datasource and was unfortunately left with a serializationerror saying it can't serialize my `<http://requests.post|requests.post>` function. Otherwise - let me know if there's an implementation with online databases somewhere I can read. (Note - this would be really useful with offline batch inference situations that often would run on a database)\n\n```Variable: \n\n\tFailTuple(get_read_tasks [obj=&lt;function _RelevanceDataSourceReader.get_read_tasks at 0x7f4299c3c560&gt;, parent=&lt;class '__main__._RelevanceDataSourceReader'&gt;])\n\nwas found to be non-serializable. There may be multiple other undetected variables that were non-serializable. ```"}
{"question": "I\u2019m experimenting with ray locally on my desktop (not deployed anywhere else). I see quite a bit of this sort of thing in the logs:\n``` (python-core-worker-81311d1ee8eb66486ab2acbd273a818361f89d643571cdd4c8312429) <http://reference_count.cc:1444|reference_count.cc:1444>: Object locations requested for a01a64375672ead0629ec04fee44ed9b3d1e155a0100000001000000, but ref already removed. This may be a bug in the distributed reference counting protocol.```\nShould I worry?"}
{"question": "Are there any rules of thumb for an upper bound on the number of `@ray.remote` tasks one should trigger at a time on an autoscaling kuberay cluster? A job on our job submission server is trying to create a large number of tasks, and eventually fails with a large number of errors (thousands) which look like this:\n```[2m[33m(raylet, ip=192.168.21.12)[0m 2022-09-26 20:22:07,433\tWARNING utils.py:1333 -- Unable to connect to GCS at my-cluster:6379. Check that (1) Ray GCS with matching version started successfully at the specified address, and (2) there is no firewall setting preventing access.```\nThe cluster seems to scale up successfully until the error occurs, but I'm just hoping to get a ballpark figure for recommended numbers of tasks."}
{"question": "Currently using local Ray client to launch actors in a remote cluster with large actor state is a blatant anti-design pattern since you upload this large state to each machine that needs it. Couod it be possible for Ray Core to just upload it to one of the machines and use the intra cluster bandwidth to get it to the other machines? This will dramatically speed up this process."}
{"question": "I think I found a bug when using both `with_parameters` and `with_resources`, can someone confirm if it\u2019s a bug? Or is this the wrong/non-intended way of doing things:\n```############\n# this works\n############\ntrainable=objective\n# 1st wrap\ntrainable = tune.with_parameters(trainable=trainable, int_data=666)\n# 2nd wrap \ntrainable = tune.with_resources(trainable, {\"cpu\": 10, \"gpu\": 0, \"memory\": 10 * 1024 * 1024 * 1024})\nprint(trainable._resources)\n\n############\n# this DOESNT work\n############\ntrainable=objective\n# with_resources first \ntrainable = tune.with_resources(trainable, {\"cpu\": 10, \"gpu\": 0, \"memory\": 10 * 1024 * 1024 * 1024})\n# with_parameters 2nd\ntrainable = tune.with_parameters(trainable=trainable, int_data=666)\nprint(trainable._resources) # INCORRECTLY prints None```"}
{"question": "Is `tune.Tuner.restore()`  working? I can\u2019t seem to get it to work in cluster setting"}
{"question": "Hi, When an actor is launching some other actors, an inter-blocking can happen if the cluster if already full of jobs (I,e the actor is waiting the sub-actors to finish, but these sub-actors are not executed because the cluster is full). Is it possible to temporary release the resources (CPUs) of the main actor until the sub actors have been executed ? Or to force the execution of the sub actors ?"}
{"question": "hey guys, is there any tutorial on making ray work with docker swarm?"}
{"question": "Hello all,\n\nHave a question about GPU-load scaling on K8s clusters using `ray==1.13`.\n\nI have a small K8s cluster deployed on Azure and the config has a few worker types defined (CPU-only and GPU types).\nWhen I submit a CPU-only job to Ray, it scales and creates new CPU workers as needed -- all is good.\nHowever, if I request any GPU resources (e.g. arg `resources_per_trial={\"cpu\": 1, \"gpu\": 1}` ), it doesn't spin up GPU workers automatically and, in this case, the client side sees:\n`(scheduler +15s)(run pid=17913) Error: No available node types can fulfill resource request {'GPU': 1.0, 'CPU': 1.0}. Add suitable node types to this cluster to resolve this issue.`\n\nThat said, if I use the autoscaler SDK to request a GPU worker manually (no other changes to the config or otherwise), say:\n`from ray.autoscaler.sdk import request_resources`\n`request_resources(bundles=[{\"GPU\": 1, \"CPU\": 1}])`\nit works fine and spins up a GPU worker as requested. It can also bring it down manually.\n\nI've looked though logs on ray/k8s but can't see why ray wouldn't autoscale.\n\nWould anyone have any pointers for where to look when trying to debug this issue?\nMany thanks!"}
{"question": "Hi all,\nFor permanent, long running, always alive application using ray, with continuous deployment, should we deploy it using `ray submit job` ? If so, how can we deploy a new application if an old application/\u201cjob\u201d is already running. I know that ray will take the new \u201cjob/application\u201d and start it in parallel as a subprocess in the head node. What I would like to have is something that mimics a rolling deployment, or at the very least, replacement.\n\n# My application does not use serve deployments."}
{"question": "When running Ray on lots of GPUs on AWS we sometimes get\n```uncorrectable ECC error encountered```\nAll coming from the same IP. Most likely it\u2019s a GPU gone bad, in some way. The worker node is obviously perfectly healthy, ie the CPU is working, but when we ask it to do any work, it\u2019ll just fail over and over. Do people have any experience fixing situations like this? The best we\u2019ve got so far is \u201cgo to AWS console and kill the failing EC2\u201d. This is both with Ray1 and Ray2, it doesn\u2019t really matter for us."}
{"question": "where can we get logs for launch-failed worker nodes?\n```(scheduler +9m3s) Removing 1 nodes of type ray_worker_gpu (launch failed).\n(scheduler +9m3s) Adding 1 nodes of type ray_worker_gpu.```"}
{"question": "Any suggestions on what would be the best way to register models on Ray Serve? During experimentation we are writing our deployment files individually for each model to be deployed. Now in production we want the model onboarding to be done to be done by another service. This service will collate the model information and create the deployment for ray serve by hitting either an exposed by ray serve or in some other manner . Are there some example design patterns or best practices which we can follow. Looking for some pointers. Thanks!"}
{"question": "Hi! Do we have any ETAs for when ray 2.0.1 gets released? Thanks!"}
{"question": "Hey I was trying to see how's it like to use ray+k8s in production, are there a lot of hurdles, what kind?  What makes people to switch from using ray to using anyscale.  Any insight would be greatly appreciated. Thanks:slightly_smiling_face:"}
{"question": "I deployed a ray cluster on Kubernetes using kuberay and I want to monitor the cluster using prometheus metrics. After reading ray document, I know that there is service discovery file is generated on the head node `/tmp/ray/prom_metrics_service_discovery.json`. Using the below Prometheus config, Prometheus will automatically update the addresses that it scrapes based on the contents of Ray\u2019s service discovery file.\n```# Prometheus config file\n\n# my global config\nglobal:\n  scrape_interval:     2s\n  evaluation_interval: 2s\n\n# Scrape from Ray.\nscrape_configs:\n- job_name: 'ray'\n  file_sd_configs:\n  - files:\n    - '/tmp/ray/prom_metrics_service_discovery.json'```\nBut since I am using Kubernetes, based on humble experience, I think the most convenient way to configure Prometheus to scape the ray metrics should be exposing metrics configuration on service annotations like this:\n```apiVersion: v1\nkind: Service\nmetadata:\n  name: xxx\n  annotations:\n    <http://prometheus.io/scrape|prometheus.io/scrape>: \"true\"\n    <http://prometheus.io/path|prometheus.io/path>: /metrics\n    <http://prometheus.io/port|prometheus.io/port>: \"8081\"```\nIs there any way to achieve this?"}
{"question": "One concept I can\u2019t quite wrap my head around with Ray is what if you want to trigger your actors, deployments, tasks, etc from within the Ray cluster? Like say I have an actor that I want to run every 30 seconds that downloads an image and sends it to a model that I am also serving within the cluster. All of the examples have `ray.get` or `ray.put` running in a loop where you\u2019re running locally and sending requests to your ray cluster. Is triggering Ray within a cluster an anti-pattern?"}
{"question": "Hey guys! Do u anyone have faced this issue? <https://discuss.ray.io/t/urgent-attributeerror-rayinternalkvstore-object-has-no-attribute-del-keys/7755>"}
{"question": "Hi, I'd like to use the Ray Plasma store to broadcast a very large variable and then use it in a `spark_df.groupby().applyInPandas()`  function.\nUnfortunately, it seems like the applyInPandas function starts a new local cluster in every evaluation.\nHow can I make it re-use the actors' cluster connection?\nSee also <https://github.com/oap-project/raydp/issues/275>"}
{"question": "Should `EpisodeV2.add_init_obs` be called before `to_eval, outputs = self._process_observations`  in env_runner_v2's  `.run()`?\nIn custom callback's function `.on_episode_start` , provided `episode.length` is `-1` instead of `0` .\n\nAlso, I see that github version is different than latest release I have in my env. <@UQMD93JSK> I think you understand the data flow in this feature."}
{"question": "Hey guys! I am trying to use Ray for scaling up computation to larger number of nodes on a cluster. What is happening that all the ray actors runs for a while, and after that one of the ray actor dies somehow, which causes the whole program to shut down. I am pretty sure the program is not running out of memory(As I don\u2019t see any OOMKiller log in dmesg).\n```(raylet, ip=172.29.58.146) [2022-10-05 02:57:21,559 E 279276 279315] (raylet) <http://agent_manager.cc:107|agent_manager.cc:107>: The raylet exited immediately because the Ray agent failed. The raylet fate shares with the agent. This can happen because the Ray agent was unexpectedly killed or failed. See `dashboard_agent.log` for the root cause.\n(pid=gcs_server) [2022-10-05 02:57:22,411 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:23,414 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:24,419 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:25,422 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:26,426 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:27,439 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:28,437 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:29,442 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:30,446 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:31,450 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:32,454 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:33,458 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:34,460 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:35,464 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:36,469 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:37,473 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:38,477 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:39,484 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:40,488 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:41,492 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:42,498 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:43,502 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:44,510 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:45,515 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:46,519 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:47,524 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:48,530 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n(pid=gcs_server) [2022-10-05 02:57:49,534 E 1570095 1570095] (gcs_server) <http://gcs_server.cc:283|gcs_server.cc:283>: Failed to get the resource load: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details: \n2022-10-05 02:57:49,972 WARNING worker.py:1404 -- The node with node id: 199d89f99d2e6c7d08bdbb19c9fa50ca3907926fac2687fc76f626ea and address: 172.29.58.146 and node name: 172.29.58.146 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a raylet crashes unexpectedly or has lagging heartbeats.\nTraceback (most recent call last):\n  File \"mlm_training.py\", line 12, in &lt;module&gt;\n    head.train()\n  File \"/usr/local/lib/python3.8/dist-packages/thirdai/_distributed_bolt/backend/distributed_bolt.py\", line 70, in train\n    trainer.train(epoch, batch_id, self.learning_rate)\n  File \"/usr/local/lib/python3.8/dist-packages/thirdai/_distributed_bolt/backend/trainer.py\", line 51, in train\n    self._calculate_gradients(batch_id)\n  File \"/usr/local/lib/python3.8/dist-packages/thirdai/_distributed_bolt/backend/trainer.py\", line 64, in _calculate_gradients\n    ray.get(\n  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/ray/worker.py\", line 1833, in get\n    raise value\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.```\nCould anyone point out the place to look for figuring out what\u2019s happening?"}
{"question": "Hello, we require help in using Ray to connect to 100s of cameras, any body did this before? we are willing to pay for prefessional services if needed"}
{"question": "Hi! I am trying to process a Ray dataset with a cluster. Looking at the docs, I see the following example:\n\n```workers = [Worker.remote(i) for i in range(4)]\n\nds = ray.data.range(10000)\n# -&gt; Dataset(num_blocks=200, num_rows=10000, schema=&lt;class 'int'&gt;)\n\nshards = ds.split(n=4)\n# -&gt; [Dataset(num_blocks=13, num_rows=2500, schema=&lt;class 'int'&gt;),\n#     Dataset(num_blocks=13, num_rows=2500, schema=&lt;class 'int'&gt;), ...]\n\nray.get([w.train.remote(s) for w, s in zip(workers, shards)])```\nIn this example, I see that there is a preset number of workers (4), but I just want to use every worker in my cluster. Is there a way to do this?\nForum post for reference: <https://discuss.ray.io/t/how-to-get-list-of-workers-in-ray-cluster/7817>"}
{"question": "And I guess the follow-up question here is, how do I handle a situation where I'm using spot instances and workers and come and go?"}
{"question": "The more I think about this, the more I'm wondering if explicitly specifying the # of workers is even the correct way to do this? Seems like specifying the number of workers explicitly would be bad if I'm using spot instances where nodes can come/go"}
{"question": "Final question, is there a way to get the node id a remote function is executing on? When logging things, it would be useful to know what the underlying AWS instance is, so I could, e.g, SSH in if there's an issue"}
{"question": "Can someone explain how Ray's function serialization works? Are functions pickled by reference or by value?"}
{"question": "when I run my ray server using `serve run module:deplpoyment_graph`  inside a docker container. The service starts running in the background and my container exits. Is there a way to start the deployment in the foreground so that my docker container doesn't shut down?"}
{"question": "Hi all. I'm getting an exception `ModuleNotFoundError: No module named 'starlette.middleware.exceptions'`  when I try to deploy with ray serve running on our kubernetes cluster if I use fastapi. It works fine without fastapi in`@serve.ingress` or if I deploy locally.\n\nWe're using the `rayproject/ray:2.0.0-py38` docker container with kuberay. Is there anything else I need to install on there to use fastapi or another container?"}
{"question": "Are file mounts synced inside/outside of the docker container?"}
{"question": "How do I reference the `ray.util.placement_group.PlacementGroup` type (for type-annotation purposes). If I make a placement group I get back a class of type `ray.util.placement_group.PlacementGroup`, but if I try and access that type after importing ray I get an error because `ray.util.placement_group` is a method..\n\n```(Pdb) pg = ray.util.placement_group([], lifetime=\"detached\", name=\"testpg\")\n(Pdb) type(pg)\n&lt;class 'ray.util.placement_group.PlacementGroup'&gt;\n(Pdb) ray.util.placement_group.PlacementGroup\n*** AttributeError: 'function' object has no attribute 'PlacementGroup'```\nSome sort of funkiness is going on where the module `ray.util.placement_group` is changed to a method for the public API. How do I reference the `PlacementGroup` type? This is `ray==1.13` FYI\n\nedit - turns out I can just import the relevant parts directly\n```&gt;&gt;&gt; from ray.util.placement_group import placement_group, PlacementGroup\n&gt;&gt;&gt; PlacementGroup\n&lt;class 'ray.util.placement_group.PlacementGroup'&gt;```"}
{"question": "Hello!\nI was wondering how I can get the PID of the worker which is running a given Task?"}
{"question": "Ray cluster could not be launched on my cluster\n\nHi, I'm trying to launch a cluster by using ray, and I took the scripts on <https://github.com/NERSC/slurm-ray-cluster> as a template.\nWhen I launched the head node, it seemed good:\n```IP Head: 10.142.4.26\nSTARTING HEAD\n2022-10-11 10:48:02,108 INFO usage_lib.py:478 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See <https://docs.ray.io/en/master/cluster/usage-stats.html> for more details.\n8.8.8.8:53\n2022-10-11 10:48:02,121 INFO scripts.py:719 -- Local node IP: 10.142.4.26\n6379 10.142.4.26:6379\n6379 10.142.4.26:6379\n2022-10-11 10:48:06,909 SUCC scripts.py:756 -- --------------------\n2022-10-11 10:48:06,909 SUCC scripts.py:757 -- Ray runtime started.\n2022-10-11 10:48:06,909 SUCC scripts.py:758 -- --------------------\n2022-10-11 10:48:06,909 INFO scripts.py:760 -- Next steps\n2022-10-11 10:48:06,909 INFO scripts.py:761 -- To connect to this Ray runtime from another node, run\n2022-10-11 10:48:06,909 INFO scripts.py:764 --   ray start --address='10.142.4.26:6379'\n2022-10-11 10:48:06,909 INFO scripts.py:780 -- Alternatively, use the following Python code:\n2022-10-11 10:48:06,910 INFO scripts.py:782 -- import ray\n2022-10-11 10:48:06,910 INFO scripts.py:786 -- ray.init(address='auto', _node_ip_address='10.142.4.26')\n2022-10-11 10:48:06,910 INFO scripts.py:798 -- To connect to this Ray runtime from outside of the cluster, for example to\n2022-10-11 10:48:06,910 INFO scripts.py:802 -- connect to a remote cluster from your laptop directly, use the following\n2022-10-11 10:48:06,910 INFO scripts.py:806 -- Python code:\n2022-10-11 10:48:06,910 INFO scripts.py:808 -- import ray\n2022-10-11 10:48:06,910 INFO scripts.py:809 -- ray.init(address='ray://&lt;head_node_ip_address&gt;:10001')\n2022-10-11 10:48:06,910 INFO scripts.py:818 -- If connection fails, check your firewall settings and network configuration.\n2022-10-11 10:48:06,910 INFO scripts.py:826 -- To terminate the Ray runtime, run\n2022-10-11 10:48:06,910 INFO scripts.py:827 --   ray stop```\nBut in monitor.log:\n```2022-10-11 10:49:03,886\tINFO autoscaler.py:386 -- \n======== Autoscaler status: 2022-10-11 10:49:03.882040 ========\nNode status\n---------------------------------------------------------------\nHealthy:\n 1 node_3f8546d770dc1ac4d8534aeedfcbe027884b9d787fbdc4ad09358040\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/10.0 CPU\n 0.0/2.0 GPU\n 0.0/1.0 accelerator_type:PG503\n 0.00/392.673 GiB memory\n 0.00/172.280 GiB object_store_memory\n\nDemands:\n (no resource demands)\n2022-10-11 10:49:08,894\tINFO load_metrics.py:160 -- LoadMetrics: Removed ip: 3f8546d770dc1ac4d8534aeedfcbe027884b9d787fbdc4ad09358040.\n2022-10-11 10:49:08,894\tINFO load_metrics.py:163 -- LoadMetrics: Removed 1 stale ip mappings: {'3f8546d770dc1ac4d8534aeedfcbe027884b9d787fbdc4ad09358040'} not in set()\n2022-10-11 10:49:08,894\tINFO autoscaler.py:386 -- \n======== Autoscaler status: 2022-10-11 10:49:08.894473 ========\nNode status\n---------------------------------------------------------------\nHealthy:\n\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n\n\nDemands:\n (no resource demands)\n2022-10-11 10:49:13,903\tINFO monitor.py:353 -- Autoscaler has not yet received load metrics. Waiting.\n2022-10-11 10:49:18,912\tINFO monitor.py:353 -- Autoscaler has not yet received load metrics. Waiting.\n2022-10-11 10:49:23,920\tINFO monitor.py:353 -- Autoscaler has not yet received load metrics. Waiting.```\nThe node ip was removed and then the node info could not be found in ray status anymore.\nI found the timestamp was after the ray agent failed (which I found in raylet.out):\n```[2022-10-11 10:48:33,814 D 140104 140104] (raylet) gcs_rpc_client.h:514: GCS channel status: 2\n[2022-10-11 10:48:33,814 D 140104 140149] (raylet) gcs_rpc_client.h:514: GCS channel status: 2\n[2022-10-11 10:48:34,814 D 140104 140149] (raylet) gcs_rpc_client.h:514: GCS channel status: 2\n[2022-10-11 10:48:34,814 D 140104 140104] (raylet) gcs_rpc_client.h:514: GCS channel status: 2\n[2022-10-11 10:48:35,814 D 140104 140149] (raylet) gcs_rpc_client.h:514: GCS channel status: 2\n[2022-10-11 10:48:35,814 D 140104 140104] (raylet) gcs_rpc_client.h:514: GCS channel status: 2\n[2022-10-11 10:48:36,807 W 140104 140104] (raylet) <http://agent_manager.cc:115|agent_manager.cc:115>: Agent process expected id 424238335 timed out before registering. ip , id 0\n[2022-10-11 10:48:36,815 D 140104 140104] (raylet) gcs_rpc_client.h:514: GCS channel status: 2\n[2022-10-11 10:48:36,815 D 140104 140149] (raylet) gcs_rpc_client.h:514: GCS channel status: 2\n[2022-10-11 10:48:36,822 W 140104 140146] (raylet) <http://agent_manager.cc:131|agent_manager.cc:131>: Agent process with id 424238335 exited, return value 0. ip . id 0\n[2022-10-11 10:48:36,822 E 140104 140146] (raylet) <http://agent_manager.cc:134|agent_manager.cc:134>: The raylet exited immediately because the Ray agent failed. The raylet fate shares with the agent. This can happen because the Ray agent was unexpectedly killed or failed. See `dashboard_agent.log` for the root cause.\n[2022-10-11 10:48:36,842 D 140104 140146] (raylet) <http://logging.cc:323|logging.cc:323>: Uninstall signal handlers.```\nI wonder why 'Agent process expected id 424238335 timed out before registering. ip , id 0' happened?\nBesides, although no node was shown in ray status, I could still run ray stop to exit ray processes on the ip address.\nIf I launch a worker on the same machine of the head (totally 8 GPUs, for this test, 2 GPUs for head, 2 GPUs for worker), I could find a node in ray status. If I launch a worker from another machine, no node was shown in ray status.\n\nMy scripts are shown below:\nstart_ray_head.sh:\n```head_node=SH-IDC1-10-142-4-26\npartition=toolchain\nredis_password=$(uuidgen)\nexport redis_password\nip=$(srun -p $partition --nodes=1 --ntasks=1 -w $head_node hostname --ip-address)\nsrun --nodes=1 --ntasks=1 -p $partition -w $head_node --gpus-per-task=2 --cpus-per-task=10 bash start_head.sh $ip $redis_password```\nstart_head.sh:\n```export PATH=/mnt/lustre/huangxinchi/.local/bin:$PATH\nexport RAY_BACKEND_LOG_LEVEL=debug\npartition=toolchain\nport=6379\nip_head=$1:$port\nexport RAY_ADDRESS=ip_head\nexport ip_head\necho \"IP Head: $1\"\necho \"STARTING HEAD\"\nray start --temp-dir=/mnt/lustre/huangxinchi/ray_logs --include-dashboard=false --head --node-ip-address=\"$1\" --port=$port --num-cpus \"10\" --num-gpus \"2\" --redis-password=$2\nsleep infinity```\nstart_ray_worker.sh:\n```worker_node=SH-IDC1-10-142-4-80\npartition=toolchain\nredis_password=$(uuidgen)\nip_head='10.142.4.26:6379'\nsrun --nodes=1 --ntasks=1 -p $partition -w $worker_node --gpus-per-task=2 --cpus-per-task=10 bash start_worker.sh $ip_head $redis_password```\nstart_worker.sh:\n```echo \"STARTING WORKER 1\"\nexport PATH=/mnt/lustre/huangxinchi/.local/bin:$PATH\necho $1\nray start --temp-dir=/mnt/lustre/huangxinchi/ray_logs --address $1 # --redis-password=$2\nsleep infinity```"}
{"question": "Hey everyone,\nI load a previously trained trainer from a checkpoint (lastest nightly ray-3.0.0.dev0):\n```agent = ApexDQN(config=config)\nagent.restore(checkpoint_path)```\nI only want to use the policy for inference:\n```agent.compute_single_action(observation=observation, explore=False)```\nIs there an option to *not spawn Environments* (since its costly on memory) when loading from a checkpoint? I think the environment is generated by check_env.\nDirectly saving/loading the model with a .pt file seems not possible with the hierarchical model (fcnet + dueling). Undesirable workaround would be to use a dummy environment.\n\nPS: config.disable_env_checking seems to get rid of the environment reset but still spawns an environment."}
{"question": "Hi there, is it possible to change `max_workers` on a running cluster, while it\u2019s running?"}
{"question": "How can I get the worker that is currently running a specific task?"}
{"question": "has anyone deployed ray on Azure cluster? is it possible to pass in specific vnet info under `provider` section?"}
{"question": "how do i list then running actors? is there an api?"}
{"question": "Hi guys can someone teach me how to get core dump from Ray? All the unlimits on all the workers are set correctly yet the coredump is nowhere to be found :-("}
{"question": "Anyone runs into this <https://github.com/ray-project/ray/issues/28838|issue>? dataclass value become empty for some reason."}
{"question": "Does anyone know how to increase the parallelism for `dataset.count()`?"}
{"question": "Hi folks! I'm working on a compilation of Ray learning resources for data scientists at my company. The level would span from introductory to a deeper dive on relevant concepts. Clearly I'll be including links to Ray documentation. Are there any external resources - e.g. YouTube videos, blogs, etc. - about Ray you have found helpful in your learning and would recommend to others?"}
{"question": "Has anyone succeeded in using ThreadPoolExecutor inside of a Ray actor? Seems like Ray is throttling number of threads or something"}
{"question": "Hey everyone\nI am working with ApexDQN, where the model is split between an fcnet (or visual net) and the dueling layers from DQNTorchModel. Unfortunately it is 4 individual models (model._hidden_layers, model._value_branch, model.advantage_module, model.value_module) which is cumbersome for visualisation. I am wondering why the dueling structure of dqn_torch_policy.compute_q_values() is not moved inside the model? Is there any future plans in this direction? Current workaround would be to use a custom policy with modified compute_q_values()."}
{"question": "hi everyone, i\u2019m trying to understand the execution model of `ray.data.read_parquet` , so i\u2019m running a massive scoring map with GPU, and i\u2019m planning to use the ray dataset to prefetch the data so my GPU actor pool can be busy all the time. And i\u2019m wondering is there anything special i need to do to separate the data fetching and GPU mini batch inferencing?"}
{"question": "Is there a way to search the dashboard by nodeID?"}
{"question": "Hello Team\nI am testing out different deep learning models in local machine using Docker. Currently all the models are stored on S3. And I have written a custom download util script which downloads the models before triggering the serve deployment.\nThe two questions which I have are:\n\n\u2022 Does ray provide some utility out of the box which can used to download model files from S3 for serving purposes\n\u2022 Will I be able to hot reload the serve deployment in case new models are present on s3 and needed to be downloaded?"}
{"question": "Hello, I was using FastAPI, but want to better use available CPU resources so tried out RayServe.\nBut when I tried the example <https://github.com/anyscale/academy/tree/main/ray-serve/e2e/deploy-cloud-run|https://github.com/anyscale/academy/tree/main/ray-serve/e2e/deploy-cloud-run> on my laptop with 8 CPU cores - I even got a considerable slow down compared to the same code without Ray (only FastAPI + Docker). Can you please advice how to make use of all CPU cores available and speed up the huggingface models inference time?"}
{"question": "can i  do this in ray ?\n```@ray.remote\ndef load_model(model_path):\n    model = torch.load(download(model_path))\n    return ray.put(model.state_dict)\n\nmodel_ref_of_refs = [load_model.remote(model_path) for model_paths in a_lot_of_model_paths]```"}
{"question": "```@ray.remote\ndef load_model(model_path):\n    model = torch.load(download(model_path))\n    return model.state_dict\n\nmodel_refs = [load_model.remote(model_path) for model_paths in a_lot_of_model_paths]```\n or simply avoid ray put at the end of each task and rely on the implicit ray put ?"}
{"question": "is there any recommended way to run a native python iterator version of `group by key` to generate ray dataset batch for `map_batches` operation ?"}
{"question": "Does anybody know what Ray is doing after pushing the compressed package? It takes sometime to actually train the model."}
{"question": "In rllib, how to access the `info` variable in the step function (`return obs, reward, done, info`), and use it to measure the progress of the training? Does rllib support it?"}
{"question": "actually the zips are kind of annoying, since it looks like they only support remote URIs (and not local ones?)"}
{"question": "Hi guys,\nit seems that ray has error shutting down on errors like KeyboardInterrupt\nif you run the following\n```def ray_shutdown_on_error():\n    import ray\n    ray.init()\n    try:\n        while True:\n            pass\n    finally:\n        ray.shutdown()\n        print('yes')```\n... and KeyboardInterrupt it, you don't see the 'yes'\n\ndo you guys run into the same? or there are better ways to shutdown ray gracefully on errors etc. thanks!!"}
{"question": "Hi guys - I'm trying to launch a cluster with a command like this: `ray up example-full.yaml` . My question is what is the best / cleanest way to parametrize these .yaml files with environment variables? (preferably without saving plaintext to disk).\n\nFor example, I would like to interpolate the value of `project_id` in the .yaml to receive the value from a system environment variable. Hoping for something like `project_id: ${PROJECT_ID}`.\n\nAppreciate any help or suggestions!"}
{"question": "What is the recommended way to do spot instance training with Ray? Is it to use Ray Train (seems to support builtin spot instance training) or is it to use something like elastic horovod"}
{"question": "Hi all, Is anyone training deep learning models and in need of some extra GPU power?\nWe have some spare GPU power here for the rest of the month, and we are willing to donate it to a good cause (interesting project, research, etc.)\nOur system is still BETA, and the GPUs will be provided NO STRINGS ATTACHED. You will be able to run almost any Pytorch script on them in one click (the system is already fully configured)\nFeel free to approach me with any further questions."}
{"question": "Hi, Ray on Windows does not start with resources specified. Does anyone know why? It looks like it might be some problem with parsing because when I remove spaces it prints different errors. Windows 10, Python 3.9.5, Ray 2.0.0."}
{"question": "```ray.exceptions.RayTaskError(ArrowInvalid): [36mray::_split_block()[39m (pid=94, ip=10.134.1.8)\n  File \"/opt/miniconda/envs/deep_tagging_inference/lib/python3.9/site-packages/ray/data/dataset.py\", line 2982, in _split_block\n    b0 = block.slice(0, count, copy=True)\n  File \"/opt/miniconda/envs/deep_tagging_inference/lib/python3.9/site-packages/ray/data/impl/arrow_block.py\", line 85, in slice\n    view = _copy_table(view)\n  File \"/opt/miniconda/envs/deep_tagging_inference/lib/python3.9/site-packages/ray/data/impl/arrow_block.py\", line 399, in _copy_table\n    arr = col.combine_chunks()\n  File \"pyarrow/table.pxi\", line 354, in pyarrow.lib.ChunkedArray.combine_chunks\n  File \"pyarrow/array.pxi\", line 2526, in pyarrow.lib.concat_arrays\n  File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays```\n:pensive: can anyone help me out with this error please? it\u2019s merely doing `ray.data.read_parquet(list_of_names).repartition(200)`"}
{"question": "Has support for multiple clusters been removed?\n<https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html#connect-to-multiple-ray-clusters-experimental>\n```import ray\nray.init(address='auto', allow_multiple=True)\nTraceback (most recent call last):\n  File \"/home/david/Projects/theTrading/venvi/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-3-f062877bdccd&gt;\", line 1, in &lt;module&gt;\n    ray.init(address='auto', allow_multiple=True)\n  File \"/home/david/Projects/theTrading/venvi/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/david/Projects/theTrading/venvi/lib/python3.9/site-packages/ray/_private/worker.py\", line 1262, in init\n    raise RuntimeError(f\"Unknown keyword argument(s): {unknown}\")\nRuntimeError: Unknown keyword argument(s): allow_multiple```"}
{"question": "Aside from io being take care of what\u2019s the high level difference between  `ray.data.read_parquet(list_of_files).map_batches` and `ray.actor_pool(FileProcessor).map_unordered(lambda actor, file: actor.process.remote(file), list_of_files)` ? from my experience, the `actor_pool` somewhat performs better than `ray.dataset` (on the aspect of better distributing the workloads )which is disappointing :disappointed:"}
{"question": ":wave: Hello, team! I am facing issue where ray is `trying to recover 4 lost objects by resubmitting their tasks`. Should I object reconstruction, set `@ray.remote(max_retries=0).` or what is the recommendation?"}
{"question": "Hi, using Ray Core remote tasks I found a high latency of 5+ seconds when starting new tasks for the first time on a worker. Specifically this happens when using Numba to compile part of the remote task. With ray timeline I found `wait_for_function` taking up to 6 seconds on a single machine cluster (windows, AMD 3700x 8/16 core/threads). Without numba it still takes 1+ seconds for `wait_for_function`  which I still find very high, when handling the arguments takes much less time even though they can be pretty big. Any clue on what I could be doing wrong, or ways to mitigate this?"}
{"question": "Hi Everyone! I am facing this `autoscaler issue` randomly in between running *my production jobs*. Did anyone face this issue before?\nI am using Ray 2.0.0 with the legacy ray operator.\n```Traceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/monitor.py\", line 484, in run\n    self._run()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/monitor.py\", line 359, in _run\n    autoscaler_summary = self.autoscaler.summary()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/autoscaler.py\", line 1379, in summary\n    ip = self.provider.internal_ip(node_id)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/_kubernetes/node_provider.py\", line 89, in internal_ip\n    pod = core_api().read_namespaced_pod(node_id, self.namespace)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/kubernetes/client/api/core_v1_api.py\", line 23483, in read_namespaced_pod\n    return self.read_namespaced_pod_with_http_info(name, namespace, **kwargs)  # noqa: E501\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/kubernetes/client/api/core_v1_api.py\", line 23570, in read_namespaced_pod_with_http_info\n    return self.api_client.call_api(\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/kubernetes/client/api_client.py\", line 348, in call_api\n    return self.__call_api(resource_path, method,\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/kubernetes/client/api_client.py\", line 180, in __call_api\n    response_data = self.request(\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/kubernetes/client/api_client.py\", line 373, in request\n    return self.rest_client.GET(url,\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/kubernetes/client/rest.py\", line 240, in GET\n    return self.request(\"GET\", url,\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/kubernetes/client/rest.py\", line 234, in request\n    raise ApiException(http_resp=r)\nkubernetes.client.exceptions.ApiException: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Audit-Id': '47b41847-5a99-4e7e-9a89-35a225eb26a1', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'X-Kubernetes-Pf-Flowschema-Uid': '59510a32-58f5-4db9-b740-ef7af562f9e3', 'X-Kubernetes-Pf-Prioritylevel-Uid': 'ff29176f-9ee6-42b7-8cae-fd13df9d54cd', 'Date': 'Wed, 19 Oct 2022 12:42:09 GMT', 'Content-Length': '272'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"pods \\\"id-3mb59bw22uc5awvky5dmekeku9-ray-worker-type1-bm4l9\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"id-3mb59bw22uc5awvky5dmekeku9-ray-worker-type1-bm4l9\",\"kind\":\"pods\"},\"code\":404}```"}
{"question": "Is there a quick and easy way to install a python package across your entire Ray cluster after you have started your cluster?"}
{"question": "also how do i get an ObjectRef from its string?"}
{"question": "Hi guys, I am going to try pytorch-lightning 1.7 with Ray Tune. I saw some issue filed in the github: <https://github.com/ray-project/ray_lightning/issues/194>  and also a <https://github.com/ray-project/ray_lightning/pull/196|PR>. Do you think it is feasible to work around it and get the flow work easily? Thanks."}
{"question": "how do you tell ray cluster to use specific conda virtual env when running jobs?\nI tried `RuntimeEnv(conda=\"my_venv_name\",...)` but it doesn't work"}
{"question": "I would be deploying this custom ray image to the Kubernetes cluster @ EKS. Is there a better / alternative way to deploy my fixes ?"}
{"question": "Hey team, which  version of Ray is supporting `Redirecting Ray logs to stderr` ?"}
{"question": "I have a pretty odd use case. I have manually impmeneted a shared lock with Ray among actors as a separate actor. And actors can take out the lock by calling remote methods on the lock actor. This seems to be the recommended design pattern. However sometimes one of those actors can die (the lock actor can never die). When the actor dies it will never call the method to unlock the lock! I want the actor to unlock when it dies. Is there ab officially endorsed design pattern to solve this use case?"}
{"question": "Hey everyone, I am interested in specifying different containers for different ray job environment dependencies as an alternative to specifying the pip file\ni.e. the `container` key mentioned here. <https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#api-reference>\nAccording to the discussion here, it's actually wasn't supported in the `ray job submit` then? <https://discuss.ray.io/t/how-to-use-container-in-runtime-environments/6175/23>\n\nI'm curious to know if things have moved ahead at this front and if so, where I can find documentation on using it. I think the help by Guyang in that forum question was for the entire cluster? But I'm interested in a container per job(or actor). Thanks"}
{"question": "Hi, is there a way to ask nodes is resources are available before scheduling a Task to prevent object spillage?"}
{"question": "HI all , I am new to Ray and was trying to do a POC with ray tasks. When I run modin\u2019s tasks on Ray , there are many Ray::IDlE still exists after the application run out and some tasks\u2019s name in worke-id**.err\u3002Can someone please help? In worker\u2019s log file :  there is the same information of \u201cgcs_rpc_client.h:514: GCS channel status: 2\u201d continuous logging. In Raylet log file: the same information of \u201cThe worker pool has 10 registered workers which exceeds the soft limit of 40, and worker 74a8c8500\ncf479ef72484de33a65cd52e29b5d3bac40dfb7e3a1a8c4 with pid 1608604 has been idle for a a while. Kill it\u201d  continuous logging, how can I address the problem\u3002"}
{"question": "Hi All - I'm looking into setting up ray on GCP. Does anyone know anything about the `ray://` protocol that seems to be used by the ray client? I am trying to figure out the proper way to secure the connection between ray client and my GCP cluster."}
{"question": "Hi guys, have you ever run into this error on ray cluster:\n```The node with node id: de6a2b47af1b48f04b0af014a6d616efea7ff7ecd6572d95fe6ae9b7 and address: 10.24.8.5 and node name: 10.24.8.5 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a \t(1) raylet crashes unexpectedly (OOM, preempted node, etc.) \n\t(2) raylet has lagging heartbeats due to slow network or busy workload.```\nIs there a way to set the heartbeat timeout to larger value, if that's the cause?"}
{"question": "<@U043CQB8456> You can use Ray AIR for this (Dataset + Train/Predictor), <https://sourcegraph.com/github.com/ray-project/ray/-/blob/release/air_tests/air_benchmarks/workloads/gpu_batch_prediction.py?L7|example prediction benchmark> and &lt;repo:^github\\.com/ray-project/ray$%20file:^release/air_tests/air_benchmarks/workloads/pytorch_training_e2e\\.py|example training benchmark&gt; that covers your asks above\n\n&gt;   i have large size of image data\n`dataset = ray.data.read_images` helps to read from remote source (s3 / local disk, etc.) into ray cluster's object store at scale\n\n&gt;   i want load 1/3 images to each model/1gpu\n`dataset` handles sharding of input data to GPU works for you as long as it's specified in `ScalingConfig` . Using TorchTrainer as example:\n\n```trainer = TorchTrainer(\n        train_loop_per_worker=train_loop_per_worker,\n        train_loop_config={\"batch_size\": 64, \"num_epochs\": num_epochs},\n        datasets={\"train\": dataset},\n        preprocessor=preprocessor,\n        scaling_config=ScalingConfig(num_workers=3, use_gpu=True),\n    )```\nshards your full `dataset` to `num_workers=3`  where each worker takes 1 GPU."}
{"question": "Hi all! I have set up a cluster consisting of head on one machine and worker nodes on other machines, but I am having an issue:\n\u2022 nodes are visible ray.nodes()\n\u2022 But I'm having an issue, that head takes all the tasks and workers are idle, like here: <https://github.com/ray-project/ray/issues/23010>\nI run head with\n`ray start --head --dashboard-port=${DASHBOARDPORT} --port=${REDISPORT} --dashboard-host=0.0.0.0 --redis-password=${REDISPASSWORD} --num-cpus=0 --block\"`\n\nI run worker with\n`ray start --address=${HEAD_IP}:${REDISPORT} --redis-password=${REDISPASSWORD} --num-cpus=${NUM_CPU_WORKER} --block`\n\nDo you maybe have idea what is going wrong?"}
{"question": "In RLlib, is *parameter noise* class compatible with PPO? I got an error ValueError: step must be greater than zero. There are no error with other exploration classes, like Curiosity."}
{"question": "Question regarding \"examples\". I'm using the \"O'Reilly Learning Ray Book\" and some of the examples don't work. Other examples from the documentation on the website don't work as well, with gym no longer supporting ROMs etc... Any advice on what to stick with?"}
{"question": "Hi guys - for ray.tune Trainable (func or cls), if within a step, I need to write out lots of data, what will be the best practice? Would it be ok if I write directly out to `&lt;experiment_dir&gt;/&lt;trial_dir&gt;` ?"}
{"question": "Some months back, I have seen a basic coding ray with detailed example and comparison with minimal code between normal python function and ray remote function and also when should we use get()  when we shouldn't. but suddenly I could not find anywhere in the ray docs. is this page removed ?"}
{"question": "Hi, I\u2019m trying to use ray state api, e.g., ray list nodes, on aws ec2, I think I need to pass an address to `ray list` , the help doc says it should be `The address of Ray API server` , what would that be? and do I need a port number?"}
{"question": "Hello, Ray community!\nPlease, help to me find the reason for my issue, I tried a lot, but no luck.\nAfter running 10-20 tasks head become unavailable to run more, because of running out of memory (3.60GB/4.00GB). The only thing helps me is manually restarting head, but it is not reliable for production usage.\nWhat did I miss? Why head isn\u2019t freeing up memory after job completed? What can be tuned more to fix it?\nThanks in advance.\n\nPS: I use Ray version 2.0, helm deployment in in k8s cluster, but I reproduced the same behavior just by starting Ray locally from command line (`ray start --head ...` )"}
{"question": "Hi, ray team.\n\nI'm using ray cluster at GCP, However, I'm facing some issue when worker sets preemptible option 'true'. All new worker nodes recovering dead nodes just staying status as a 'launching' not healty.\nis it impossible to restart Worker node and add actor class atomatically using ray cluster yaml?"}
{"question": "Hi ray team - would it be possible to post the ray 3.0.0dev code on `ray.air._internal.session` , as it's missing from this page: <https://docs.ray.io/en/master/_modules/>?\n\nFor now, `get_trial_dir()` is unavaiable for public and I'm curious how the new `_get_session()` is implemented so that I can impl. the `get_trial_dir()` for now before the release.\n\nThanks"}
{"question": "Have you guys run into this issue where when you do\n```Tuner.restore(path, resume_errored=True)```\n... `Trial` will create a new trial dir instead of using the failed one?\n```Creating a new dirname XXXXXX because trial dirname 'XXXXX' already exists. ```"}
{"question": "I'm trying to run example-full.yaml for a local set of machines, and get this error when I do a ray up on the file. \tdebug_error_string = \"{\"created\":\"@1667406431.623772913\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3134,\"referenced_errors\":[{\"created\":\"@1667406431.623772542\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/lib/transport/error_utils.cc\",\"file_line\":163,\"grpc_status\":14}]}\"\n&gt; Any ideas on what I am doing wrong?"}
{"question": "Is it mandatory to use conda for ray? I am getting this error  (any suggestions on how to fix this without using conda?):\n```RuntimeError: Version mismatch: The cluster was started with:\n    Ray: 2.0.0\n    Python: 3.7.7\nThis process on node 192.168.5.251 was started with:\n    Ray: 2.0.1\n    Python: 3.8.10```\n"}
{"question": "Hi everyone,\n\nI have been using ray for a while now but mainly on google cloud. However, at the moment, I am interested in assembling a cluster with my own machines.\n\nYesterday I managed to set up everything manually. I started ray in the head node with `ray start --head --port=6379`  and then started ray in the worker nodes with ray `start --address='&lt;machine ip&gt;:6379'`. I then executed my script and everything worked.\n\nHowever, now I am interested in doing this automatically. I know that I can do this using <https://github.com/ray-project/ray/blob/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/local/example-full.yaml|this .yaml> file where I just specify the addresses of the head and worker nodes. But I have some high level questions:\n\n1. What is usually your setup. Do you have a small machine for the head node and large machines for the worker nodes?\n2. The machines I want to use have several users in them. Do you create users for the ray workers?\n3. Do yo create your own docker images?\nAlso if anyone could point me to sources, (e.g blog posts, youtube videos, GitHub accounts) that have done this it would be great.\n\nThanks in advance."}
{"question": "Hi, I have a question about Tune what is the difference between using\n`tune.run` and `Tuner` class?"}
{"question": "what if i give 2 cpus to an actor, are the threads distributed across two processes?"}
{"question": "does the call to `repartition` tries to read the entire dataset in memory ?"}
{"question": "would it make sense to augment parallelism to a value above the total number of cpu in the cluster ?"}
{"question": "In RLlib, changing\n```config.resources(num_gpus=0, num_gpus_per_worker=0)```\n To\n```config.resources(num_gpus=1, num_gpus_per_worker=0.1)```\ncauses this error. I have no clue, why?\n\nFile \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\u201d, line 1071, in _worker\n    assert len(loss_out) == len(self._optimizers)\nAssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \u201c./main.py\u201d, line 47, in &lt;module&gt;\n    results = algo.train()\n  File \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\u201d, line 347, in train\n    result = self.step()\n  File \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\u201d, line 661, in step\n    results, train_iter_ctx = self._run_one_training_iteration()\n  File \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\u201d, line 2378, in _run_one_training_iteration\n    num_recreated += self.try_recover_from_step_attempt(\n  File \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\u201d, line 2190, in try_recover_from_step_attempt\n    raise error\n  File \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\u201d, line 2373, in _run_one_training_iteration\n    results = self.training_step()\n  File \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/rllib/algorithms/alpha_zero/alpha_zero.py\u201d, line 361, in training_step\n    train_results = multi_gpu_train_one_step(self, train_batch)\n  File \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py\u201d, line 176, in multi_gpu_train_one_step\n    results = policy.learn_on_loaded_batch(\n  File \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\u201d, line 591, in learn_on_loaded_batch\n    tower_outputs = self._multi_gpu_parallel_grad_calc(device_batches)\n  File \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\u201d, line 1144, in _multi_gpu_parallel_grad_calc\n    _worker(shard_idx, model, sample_batch, device)\n  File \u201c/data/home/acw554/venv/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\u201d, line 1129, in _worker\n    e.args[0]\nIndexError: tuple index out of range"}
{"question": "When I submit a job in Ray, it is complaining that I have version 1.13.0, even though I have 2.0.1 installed. What is the best way to work out what is causing this?"}
{"question": "Is it possible to setup a ray cluster in a way that workers communicate to head while head is not communicating with workers (the communication is not bidirectional)?"}
{"question": "Which version of Ray do I need to install if I want to pull the earliest dev version with this commit?\n<https://github.com/awslabs/autogluon/commit/ab1f13a74dfd45c05e8b5851e184d46deb639b6c>"}
{"question": "It looks that the ES algorithm in RLlib only shows `episode_reward_mean` while training. How to get the `episode_reward_max` like in other algorithms?"}
{"question": "Are there recommended tools or methods for profiling Ray applications built on Ray Core? The current docs link for profiling seems to be broken:\n<https://docs.ray.io/en/latest/ray-core/troubleshooting.html>"}
{"question": "hi team, if i pass a custom object to actor init function, when creating actors remotely, would ray implicitly put the object and get the object for me?"}
{"question": "Which of these would be faster or have the least blocking calls?\n\n1. Using `asyncio.gather`\n```@serve.deployment\nclass Model:\n    ...\n    def __call__(self, texts: list[str]):\n        tasks = [self.predictor_handle.remote(self.tokeniser_handle.remote(t)) for t in texts]\n        refs = await asyncio.gather(*tasks)\n        results = await asyncio.gather(*refs)\n        return results```\n2. Awaiting individual `asyncio.Tasks`/`ray.ObjectRefs`\n```@serve.deployment\nclass Model:\n    ...\n    def __call__(self, texts: list[str]):    \n        tasks = [self.predictor_handle.remote(self.tokeniser_handle.remote(t)) for t in texts]\n        refs = [await t for t in tasks]\n        results = [await r for r in refs]\n        return results```"}
{"question": "Hi team, I am newbie so please bear with my doubt. We a ray cluster with a head node and worker nodes. When we spawn a Ray Job and initialize few actors inside that Job which do map-reduce kind of work. So, we need to share the data emitted from map() with reduce(), would all that data need to be fetched on head node via ray.get() to pass it to reduce-&gt; which would be happening on different worker nodes. Doesn't this bottleneck us on Head node's memory?"}
{"question": "Hello guys! I have a question about running Ray client and Ray remote functions inside celery worker tasks.\nBasically we have several problems with such a setup on a Staging / Production environments, but not on for the local development, locally it works fine.\n\nWe've tried Ray versions 1.13.0 and 1.11.1. We have two ray clusters, but celery workers connect only to one of them. We have not used `allow_multiple=True`  but  enabled it after receiving couple of errors mentioned this flag.\n\nCode we're using to connect to Ray cluster:\n```        ray.shutdown()\n        ray.init(address=cluster_address, **kwargs)```\nThese are couple of errors we receive on our Staging deployed using Kubernetes on AWS.\n```Request can't be sent because the Ray client has already been disconnected due to an error. Last exception: &lt;_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"Attempted to reconnect a session that has already been cleaned up\"\n\tdebug_error_string = \"{\"created\":\"@1668008495.925569579\",\"description\":\"Error received from peer ipv4:172.20.35.98:10001\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1074,\"grpc_message\":\"Attempted to reconnect a session that has already been cleaned up\",\"grpc_status\":5}\"\n&gt;```\nand\n```The client has already connected to the cluster with allow_multiple=True. Please set allow_multiple=True to proceed```\nThis error we get if we're using `allow_multiple=True`\n```Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.```\nI have currently ran out of ideas why ray client behaves differently on a deployed environment so decided to ask here.\n\n`ignore_reinit_error` does not work for us to resolve `Maybe you called ray.init twice by accident? ` error, ray.shutdown() before init  returns  `Request can't be sent because the Ray client has already been disconnected due to an error. `, checking `if not ray.is_initialized():`  provides the same result as `ray.shutdown()`.\n\nProbably Celery threading has some conflicts with gRPC / Ray?"}
{"question": "Hi guys - have you run into this error after the upgrade to 2.1?\n```ValueError: Resource quantities &gt;1 must be whole numbers.```"}
{"question": "Hi, I am trying to understand a few things in a sample code and have few questions. If anyone can help me understand those.\n```import ray\n\n@ray.remote(num_cpus=1)\nclass Poc:\n\n  def __init__(self):\n\tself.final_dict = {}  \n\n  def get_dict(self):\n    return self.final_dict\n\n  def process_file(self,fs,file_name):\n    # process the file \n\tself.final_dict[file_name] = processed_dict\n\n  def reduce(self, final_dict):\n\t# iterate over the dictionary and reduce it\n\treturn uber_dict\n\nray.init()\npoc = Poc.remote()\nfor file in file_paths:\n\tpoc.process_file.remote(file)\n\nfinal_dict = ray.get(poc.get_dict.remote())\nuber_dict = ray.get(poc.reduce.remote(final_dict))```\n\u2022 The poc.process_file.remote() -&gt; Does this run a parallel task for each file in parallel?\n\u2022 Would there be benefit of having an actor pool above rather than just one actor poc and running the process_file on that actor pool.\n\u2022 Now we are fetching the final_dict above when all the file processings have been done and we are passing this dictionary to reduce function. If final_dict is too big we would be bottlenecked on head node. What are the other options here? Passing `poc.get_dict.remote()`  and doing `ray.get()` on this object directly in reduce() function? \n\u2022 If we don't pass num_cpus=1, all this code would run only on head_node?"}
{"question": "Hey! We have a failing nested job with 1000s of tasks. In the logs we find this line frequently. Could this be the cause? All jobs seem to restart on that node right after that. Other errors say that worker died unexpectedly. There are also quite a few Keyboard interrupts. We think those are send to tasks  which ray decided to cancel. Any suggestions to further dig into this? Could this a ulimit issue?\n``` 5522 <http://ev_epollex_linux.cc:515]|ev_epollex_linux.cc:515]>    Error shutting down fd 10. errno: 9```"}
{"question": "Hi, I have a question. I want to do some complicated training using RLlib and I\u2019m not sure how.\n\nI have an environment for two agents, and I want to train the first agent while I\u2019m forcing the policy of the second agent to be a hard-coded policy that I write. I want to run that training for 10 steps. Then I want to continue training both agents normally for 10 more steps. That means that in the second training, the first agent is starting out with the policy that I trained in the first training phase, while the second agent is starting with a blank policy.\n\nIs that possible with RLlib? How?"}
{"question": "Hi Im a newbee but struggling to understand why my test run on a trained model does not provide me the same episode mean reward - or atleast close to. Im having a custom env that on each reset initializes the env to one of 328 samples incrementing it one by one until it repeats itself again. Each episode is around 100-120 timesteps and will only return done on the last timestep. My training is setup like so\n```SELECT_ENV = \"my_env\"\nregister_env(env_name, env_creator)\n\nexperiment = tune.run(\n    \"PPO\",\n    config={\n        \"env\": SELECT_ENV,\n            #\"framework\": \"tf2\",\n            #\"eager_tracing\":True,\n            #\"lambda\": 0.95,\n            #\"kl_coeff\": 0.5,\n            #\"clip_rewards\": True,\n            #\"clip_param\": 0.3,\n            #\"vf_clip_param\": 10.0,\n            #\"vf_share_layers\": True,\n            #\"vf_loss_coeff\": 1e-2,\n            #\"entropy_coeff\": 0.01,\n            #\"train_batch_size\": 10000,\n            #\"sample_batch_size\": 130,\n            #\"sgd_minibatch_size\": 130,\n            #\"num_sgd_iter\": 10,\n            \"num_workers\": 6,\n            #\"num_envs_per_worker\": 16,\n            #\"lr\": 0.0001,\n            \"gamma\": 1.0,\n            \"batch_mode\": \"complete_episodes\",\n            \"metrics_smoothing_episodes\": 328,\n            #\"num_cpus\": 4\n            #\"model\": {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [512, 512], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}\n    },\n    metric=\"episode_reward_mean\",\n    mode=\"max\",\n    stop={\"training_iteration\": 250},\n    checkpoint_at_end=True,\n)```\nand the testcode running on the SAME 328 sample dataset like so\n```register_env(env_name, env_creator)\n\nconfig = ppo.PPOConfig()\nconfig.explore=False\nagent = config.build(env=env_name)\nagent.restore(checkpoint_path)\n\nenv = env_creator(config)\nstate = env.reset()\n\nsum_reward = 0\n\nepisodes = 1\nwhile True:\n    #action = agent.compute_single_action(state)\n    action = agent.compute_action(state)\n    state, reward, done, info = env.step(action)\n\n    #if(reward != 0):\n    #    print(reward)\n    sum_reward += reward\n    \n    if done:\n        if (episodes == 328):\n            break\n        else:\n            state = env.reset()\n            #print(env.current_state)\n            episodes += 1;\n\nprint(sum_reward)\nprint(episodes)\nprint(sum_reward / episodes)\n\n=&gt; 12736.807102917062\n=&gt; 328\n=&gt; 38.83172897230812```\nthe mean reaward on test yields something like 38 while on tensorboard it seems like a much better mean reward - 123....\n```check_point = experiment.get_trial_checkpoints_paths(trial=experiment.get_best_trial('episode_reward_mean'),\n                                                       metric='episode_reward_mean')```\nreturns PPO_my_env_4cfa5_00000_0_2022-11-14_14-36-10\\\\checkpoint_000250', *123.2423709106124*\n\nIs am doing somthing wrong here? Thanks for any help"}
{"question": "I'm seeing this error when submitting a job:\n\n`2022-11-14 12:45:47,361 WARNING utils.py:1333 -- Unable to connect to GCS at 192.168.xx.xx:8265. Check that (1) Ray GCS with matching version started successfully at the specified address, and (2) there is no firewall setting preventing access.`\n\nI'm using Ray 2.0.1, and the image `rayproject/ray:2.0.1-py39-cpu`.\n\nI'm running a local cluster.\n\nI was previously running ray 1.12, so I don't think the problem is due to firewall settings.\n\nMy head machine has Ray 2.0.1 on it and runs Linux. I have deleted everything in my `/tmp` folder.\n\nWhat could I try to fix this?"}
{"question": "hi folks ! I'm curious what's the development environment that people normally use to work with Ray Python and C++? - I mean the Ray codebase itself.\nI normally use Pycharm, but idk if it has good C++ support (I think no?)"}
{"question": "hi team, is there any way to increase the plasma size ? we are seeing some issues with single object exceeding the plasma size limit"}
{"question": "Have you guys run into similar warnings when launching cluster on Azure?\n\n```  [7/7] Starting the Ray runtime\n    Running `export RAY_USAGE_STATS_ENABLED=1;ray stop`\n      Full command is `ssh -tt -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ControlMaster=auto -o ControlPath=/tmp/ray_ssh_ccc98ce839/9fe1c64cbc/%C -o ControlPersist=10s -o ConnectTimeout=120s &lt;USERNAME&gt;@&lt;IP_ADDRESS&gt; bash --login -c -i 'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (export RAY_USAGE_STATS_ENABLED=1;ray stop)'`\nPackage bash-completion was not found in the pkg-config search path.\nPerhaps you should add the directory containing `bash-completion.pc'\nto the PKG_CONFIG_PATH environment variable\nNo package 'bash-completion' found\nbash: /yum: No such file or directory```\n"}
{"question": "Hey guys, I have some issue with ray crashing out on me. I am currently using ray 2.1.0, with 16GB Ram.\nAs inputs to my custom environment I have a Box of shape (7,1) floats and another Box of shape(14000,6), connected together with a Dict.\nAfter around 28K timesteps the following errors crops up.\nWhat actually causes the problem? Is there a possible solution to it?\n`(pid=) [2022-11-15 23:42:49,107 C 17284 3760] (raylet.exe) <http://dlmalloc.cc:129|dlmalloc.cc:129>: Check failed: *handle != nullptr CreateFileMapping() failed. GetLastError() = 1450 (pid=) *** StackTrace Information *** (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) unknown (pid=) configthreadlocale (pid=) BaseThreadInitThunk (pid=) RtlUserThreadStart`\n\nThank you for any help :slightly_smiling_face:"}
{"question": "Is there a way for me to reduce the amount of memory allocated to the ray object store in my cluster's yaml file?"}
{"question": "How to figure out the number of nodes for xgboost distributed training? With 3 nodes in the cluster I was getting OOM error, increasing it to 5 nodes fixed the issue. I had about 11GB of data and node type was 8vCPU and 16GiB RAM.\n\nmy source code looks like below,\n```from xgboost_ray import RayDMatrix, RayParams, train\nimport time\nstart = time.time()\nimport ray\nray.init(\"auto\", ignore_reinit_error=True)\n\nnum_actors = 5\nnum_cpus_per_actor = 7\n\nray_params = RayParams(\n    num_actors=num_actors, cpus_per_actor=num_cpus_per_actor)\n\nfrom xgboost_ray import RayDMatrix, RayFileType\ntrain_set = RayDMatrix([\n        \"s3://############################################################6_000000.parquet\",\n        \"s3://############################################################6_000001.parquet\",\n        \"s3://############################################################6_000002.parquet\",\n        \"s3://############################################################6_000003.parquet\",\n        \"s3://############################################################6_000004.parquet\",\n        \"s3://############################################################6_000005.parquet\",\n        \"s3://############################################################6_000006.parquet\",\n        \"s3://############################################################6_000007.parquet\",\n        \"s3://############################################################6_000008.parquet\",\n        \"s3://############################################################6_000009.parquet\",\n        \"s3://############################################################6_000010.parquet\",\n        \"s3://############################################################6_000011.parquet\",\n        \"s3://############################################################6_000012.parquet\",\n        \"s3://############################################################6_000013.parquet\",\n        \"s3://############################################################6_000014.parquet\",\n        \"s3://############################################################6_000015.parquet\",\n        \"s3://############################################################6_000016.parquet\",\n        \"s3://############################################################6_000017.parquet\",\n        \"s3://############################################################6_000018.parquet\",\n        \"s3://############################################################6_000019.parquet\",\n        \"s3://############################################################6_000020.parquet\",\n        \"s3://############################################################6_000021.parquet\",\n        \"s3://############################################################6_000022.parquet\",\n        \"s3://############################################################6_000023.parquet\",\n        \"s3://############################################################6_000024.parquet\",\n        \"s3://############################################################6_000025.parquet\",\n        \"s3://############################################################6_000026.parquet\",\n        \"s3://############################################################6_000027.parquet\",\n        \"s3://############################################################6_000028.parquet\",\n        \"s3://############################################################6_000029.parquet\",\n        \"s3://############################################################6_000030.parquet\",\n        \"s3://############################################################6_000031.parquet\",\n        \"s3://############################################################6_000032.parquet\",\n        \"s3://############################################################6_000033.parquet\",\n        \"s3://############################################################6_000034.parquet\",\n        \"s3://############################################################6_000035.parquet\",\n    ], \n    label=\"funded\", \n    ignore=[\"visited\"],\n    distributed=True,\n    num_actors=num_actors,\n    filetype=RayFileType.PARQUET,\n)\n\nimport gc\ndef train_model(config):\n    evals_result = {}\n    bst = train(\n        params=config,\n        dtrain=train_set,\n        evals_result=evals_result,\n        evals=[(train_set, \"train\")],\n        verbose_eval=False,\n        ray_params=ray_params)\n    gc.collect()\n\nfrom ray import tune\n\nconfig = {\n    \"tree_method\": \"approx\",\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": [\"logloss\", \"error\"],\n    \"eta\": tune.loguniform(1e-4, 1e-1),\n    \"subsample\": tune.uniform(0.5, 1.0),\n    \"max_depth\": tune.randint(1, 9),\n    \"tree_method\": \"hist\",\n        \"gamma\": tune.loguniform(1e-8, 1.0),\n        \"grow_policy\": \"lossguide\",\n        \"eval_metric\": [\"logloss\", \"error\", \"mae\", \"auc\"]\n}\n\nanalysis = tune.run(\n    train_model,\n    config=config,\n    metric=\"train-error\",\n    mode=\"min\",\n    num_samples=3,\n    resources_per_trial=ray_params.get_tune_resources(),\n    max_failures=3,\n    max_concurrent_trials=1\n)\n\nend = time.time()\n\nprint(\"Best hyperparameters\", analysis.best_config)```"}
{"question": "Hi guys! I'm setting up a local Ray environment using Docker, but I'm facing an issue with *seeing the logs on the docker standard output*. My setup is a driver with Ray Serve executing a Ray Workflow\n\nI've tried to follow <https://docs.ray.io/en/latest/ray-observability/ray-logging.html#how-to-set-up-loggers> with something like\n\n```@ray.remote\nclass MyLogger:\n  def __init__(self):\n    logging.basicConfig(level=<http://logging.INFO|logging.INFO>)\n    self.logger = logging.getLogger('MyLogger')```\nAnd the serve code is like this:\n\n```@serve.deployment(name=\"main\", route_prefix=\"/\")\n@serve.ingress(app)\nclass MainController:\n  def __init__(self):\n    self.logger = MyLogger.remote()\n\n  @app.get('/test')\n  def test(self, request: Request):\n    logging.basicConfig(level=<http://logging.INFO|logging.INFO>)\n    self.logger.info.remote('TESTING LOG AT /test')\n    \n    import testLogWorkflow\n    testDag = testLogWorkflow(self.logger)\n\n    results = workflow.run(testDag)\n    return results```\nMy dag is like this:\n\n```def testLogWorkflow(logger):\n  logResults = testLogRemote.bind(logger)\n  return logResults\n\n@ray.remote\ndef testLogRemote(logger):\n  logging.basicConfig(level=<http://logging.INFO|logging.INFO>)\n\n  <http://logging.info|logging.info>(\"THIS IS A <http://logging.info|logging.info> MESSAGE\")\n  logger.info.remote(\"THIS IS A logger.info.remote MESSAGE\")\n\n  return \"testLog result\"```\n\nWith this setup, I can see the logs in `MyLogger` actor `.err` logs on Ray Dashboard:\n\n```1 :actor_name:MyLogger\n2 INFO:MyLogger:TESTING LOG AT /test\n3 INFO:MyLogger:THIS IS A logger.info.remote MESSAGE```\nBut none of these logs shows up on docker outputs or `docker logs`. Does anyone has a clue to how to get the remote logs to show up in docker logs?"}
{"question": "hi team, is there anyway to include some custom exception into the ray actor retry candidates? i understand the ray actor retry is for unexpected machine errors. but what if i want to make sure some of the s3 glitches are also beneficial from retrials of my actors ?"}
{"question": "Hi! I have a very basic question: I have two machines that has all ports public, but I can\u2019t get two machines to be part of the same cluster. I think what\u2019s happening is that the dashboard is listening to 127.0.0.1, instead of 0.0.0.0. But I think there\u2019s a security issue if I were to change this? What\u2019s the canonical way to get a cluster of machines set up securely? I\u2019m using the cluster launch yaml thing."}
{"question": "Hi guys, I want to ask something. In a ray cluster, can I share an actor created in head pod to worker pods? Is it accessible using `ray.get_actor()`? Thank you!"}
{"question": "Hi, I've got a question on assignment of GPUs to custom resources when using a local cluster. We can use `CUDA_VISIBLE_DEVICES=... ray start --head` to expose only a subset of devices to the Ray cluster, but how can we associate specific custom resources (e.g., `num_predict_gpus` and `num_train_gpus`) to specific devices? In AWS, these custom resources would correspond to different available node types (EC2 instance types) in the cluster configuration. But with a local cluster, we would not only want to specify how many, but which devices specifically to associate with these custom resources used for placing tasks onto the respective devices. Is this possible? Do maybe the `available_node_types` entries in a local cluster configuration support setting environment variables such as `CUDA_VISIBLE_DEVICES`? I couldn't find such option documented. One possibility I would see is to use different Docker containers for each node type and specify the subset of GPUs as docker run options?"}
{"question": "Hi, how do I limit the concurrency of Ray Tasks?\nI\u2019m trying to use an asyncio.Semaphore as described <https://stackoverflow.com/questions/48483348/how-to-limit-concurrency-with-python-asyncio|here>, but can\u2019t use await to gather the results. In my code I have a Task that wants to call many other tasks to insert batches of records in a database, but I want to limit the concurrency of the insert requests. I\u2019d like to use tasks for the inserts for the automatic retry logic.\n\nI\u2019ve also looked at using Actors, but don\u2019t see anything equivalent to `retry_exceptions` (unless I\u2019m misunderstanding how `max_task_retries` works?)."}
{"question": "I have a question on auto-scaling:\nfor now, it seems like `upscaling_speed` is the only entry point from config. other than this, are there other params to customize it?\na broader question is, if we wanna implement some more complex logics on autoscaling, is there a way to do it? (e.g. subclassing etc.)"}
{"question": "I\u2019m getting a lot of `RuntimeError: Can't create new thread` showing up on my Serve deployment. I assume it\u2019s because the deployment replicas are creating one thread for every request coming in. Is there a way I can limit the number of concurrent requests to a deployment so I can avoid this problem?\n\nFWIW: `max_concurrent_queries` doesn\u2019t seem to be stopping this `Can't create new thread` error."}
{"question": "Hi - I\u2019ve got a Ray 2.0 cluster on VMs and have been experimenting with TorchTrainer.  It looks like I have a few tasks that are \u2018hanging\u2019 or were interrupted and not cleaned up. Is there a way to force stop/kill these from the head node?"}
{"question": "Hello. Everyone, Do you think there is a need to a tool to increase Python code  execution speed? Please provide your input."}
{"question": "how to turn off this warning?\n\n``` (raylet) <http://file_system_monitor.cc:105|file_system_monitor.cc:105>: /tmp/ray/session_2022-11-19_18-20-28_407575_7261 is over 95% full, available space: 261256265728; capacity: 7584494911488. Object creation will fail if spilling is required```\nI tried setting `RAY_DISABLE_MEMORY_MONITOR=1` doesnt seem to work"}
{"question": "When I restart my head node, my workers don\u2019t auto-connect. Is that expected? Similarly, I tried looking for some sort of systemd service that would start ray when one of my system boots, but I couldn\u2019t find anything. Is this a common tactic? Or is there some more magical way that Ray keeps a system going for long periods of time, across crashes, etc?"}
{"question": "Hi Team,\n\nNew to RAY and trying to set it up in AWS EKS. It is done and able to provision the ray cluster.\nWhen I am testing an example script. It throws below error. I read a blog where it says external Redis is optional. So, I haven't created any.\n\nHow can I overcome this issue?\n\n```2022-11-21 04:22:21,063 INFO worker.py:853 -- Connecting to existing Ray cluster at address: 10.xx.xx.117:10001\n2022-11-21 04:22:41,116 ERROR node.py:1296 -- ERROR as\n2022-11-21 04:23:03,172 ERROR node.py:1296 -- ERROR as\n2022-11-21 04:23:25,227 ERROR node.py:1296 -- ERROR as\nTraceback (most recent call last):\n  File \"ray_cluster.py\", line 7, in &lt;module&gt;\n    ray.init(\"10.xx.xx.117:10001\")\n  File \"/home/kkanagar/.local/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/kkanagar/.local/lib/python3.7/site-packages/ray/worker.py\", line 959, in init\n    connect_only=True)\n  File \"/home/kkanagar/.local/lib/python3.7/site-packages/ray/node.py\", line 148, in __init__\n    \"session_name\", ray_constants.KV_NAMESPACE_SESSION)\n  File \"/home/kkanagar/.local/lib/python3.7/site-packages/ray/node.py\", line 1305, in _internal_kv_get_with_retry\n    raise RuntimeError(f\"Could not read '{key}' from GCS (redis). \"\nRuntimeError: Could not read 'b'session_name'' from GCS (redis). Has redis started correctly on the head node?```"}
{"question": "Hi, I\u2019m trying to use the Sentence-Transformers library in Ray (<http://sbert.net|sbert.net>). What is the best way to distribute the PyTorch model in the cluster?\n\nI\u2019m currently using an actor pool and calling the SentenceTransformer constructor with the model name in the actor constructor, but that means that each actor instance downloads the full model from Huggingface. SentenceTransformer also accepts a local file path or the torch.nn modules. What\u2019s the most efficient way to initialize a pool of actors?"}
{"question": "Hi everyone, is there a way to avoid scheduling actor onto head node ?"}
{"question": "Hi guys, I have an issue using ray lightning:\n\u2022 using RayStrategy(num_workers=4) on dataset subset : working\n\u2022 using RayStrategy(num_workers=4) on full dataset : not working\nGot *Socket Timeout* error in init_process_group. It fails before it should display 'All distributed processes registered'... Any idea ? Thanks a lot"}
{"question": "Hi everyone, is possible to use AMD Gpus with Ray?"}
{"question": "Hello everyone, I'm trying to get some code to work in Ray and getting pickling errors.  The code relies heavily on Cppyy, which is a dynamic C/C++ wrapper from CERN for Python.  I'm not fully understanding what gets serialized and doesn't, but it seems to fail on serializing the function reporting that it can't pickle (`TypeError: cannot pickle 'cppyy.CPPOverload' object`).  There are no Cppyy objects/wrapped objects in the arguments to the class or function, and the return type is not a cppyy wrapped object.  Everything going into or out of the function/class are simple Python primitives.  I tried moving all of the Cppyy object instantiations inside a class, with the hope that it will all run on the remote worker.  I've tried as Task and Actor.  Not sure why it needs to pickle the internal state of the object who's lifetime only needs to exist on a single worker.  Perhaps it's so that refs can be passed around, but in my case, I don't ever need to do that.  Is there a way to force it not to do this?"}
{"question": "We are running ray worker nodes in AWS with only Ray specific ports open between them. When I try to run TorchTrainer, jobs always fail with this connection timeout. It looks like PyTorch also needs some ports open between worker nodes.  Is there a way to pass parameters to pytorch directly, like port numbers?  (Ray 2.0..error message in thread)"}
{"question": "hello. i am looking over the workflow basics page <https://docs.ray.io/en/latest/workflows/basics.html>. the second code block has this line, which seems to refer to a variable `dag` that is not defined. i must be missing something obvious?\n```output_ref = workflow.run_async(dag)```"}
{"question": "Hello, newbie question. Does `TorchTrainer` class leverage `DDP` in the backend? Thank you!"}
{"question": "Is it possible to use kuberay with pytest? Similar to kubetest plugin for pytest"}
{"question": "Q to the community. There may be a PR to change the file name of gcs_server.out/err and raylet.out/err  <https://github.com/ray-project/ray/pull/26121>. Is there anyone who will have trouble in their log scraping by this change?"}
{"question": "Hi all! Does anybody know if there is a way to add a reward normalization wrapper (gym NormalizeReward) AFTER ray has converted my environments to a vectorized environment? Also, let me know if this is not the place to ask this kind of questions :slightly_smiling_face:"}
{"question": "This may be a newbie question, but is there a way to increase the number of layers or parameters in my PPO algorithm? Basically any way to tell it to make a bigger, more powerful neural net?"}
{"question": "Hi ! I'm trying to use Ray Lightning. The documentations says that \"when using with Ray Client, you must disable checkpointing and logging for your Trainer by setting checkpoint_callback and logger to False. \" (see here : <https://github.com/ray-project/ray_lightning>)\nSo how can we use tensorboard custom callbacks and checkpoint bests models when using Ray Lightning on a Ray Client ? Thanks"}
{"question": "Ray Actors are more like \"Threads\" and not like millions of \"Green processes\" right?"}
{"question": "I can't get a job ID at runtime, am I doing this wrong? (ray 2.0.0)\n```$ cat script.py\nimport ray; print(ray.get_runtime_context().job_id)\n$ ray job submit --working-dir . python script.py\nJob submission server address: <http://127.0.0.1:8265>\n2022-11-29 10:37:20,315 INFO dashboard_sdk.py:307 -- Uploading package <gcs://_ray_pkg_a9ded6b9c018920c.zip>.\n2022-11-29 10:37:20,316 INFO packaging.py:499 -- Creating a file package for local directory '.'.\n\n-------------------------------------------------------\nJob 'raysubmit_7VLKtzTp2ZUVsmAc' submitted successfully\n-------------------------------------------------------\n\nNext steps\n  Query the logs of the job:\n    ray job logs raysubmit_7VLKtzTp2ZUVsmAc\n  Query the status of the job:\n    ray job status raysubmit_7VLKtzTp2ZUVsmAc\n  Request the job to be stopped:\n    ray job stop raysubmit_7VLKtzTp2ZUVsmAc\n\nTailing logs until the job exits (disable with --no-wait):\nTraceback (most recent call last):\n  File \"/tmp/ray/session_2022-11-25_22-21-29_843396_1/runtime_resources/working_dir_files/_ray_pkg_a9ded6b9c018920c/script.py\", line 1, in &lt;module&gt;\n    import ray; print(ray.get_runtime_context().job_id)\n  File \"/usr/local/lib/python3.9/site-packages/ray/runtime_context.py\", line 51, in job_id\n    assert not job_id.is_nil()\nAssertionError\n\n---------------------------------------\nJob 'raysubmit_7VLKtzTp2ZUVsmAc' failed\n---------------------------------------\n\nStatus message: Job failed due to an application error, last available logs (truncated to 20,000 chars):\nTraceback (most recent call last):\n  File \"/tmp/ray/session_2022-11-25_22-21-29_843396_1/runtime_resources/working_dir_files/_ray_pkg_a9ded6b9c018920c/script.py\", line 1, in &lt;module&gt;\n    import ray; print(ray.get_runtime_context().job_id)\n  File \"/usr/local/lib/python3.9/site-packages/ray/runtime_context.py\", line 51, in job_id\n    assert not job_id.is_nil()\nAssertionError```"}
{"question": "Hi,\n\nI have some issues.\nI don't know this is a bug or not. Please notify me about this issue.\nI am setting up cluster. Firstly, I set Centos machine as head node, worker node1 Ubuntu, worker node2 also ubuntu. But when deployed code which simple consume resource, Centos machine not consume of resource. Also when I checked as ray status, Centos Cpu resource not adding to Head Node.\n\nThis scenario like also same between centos machines. (But Centos CPU resource not adding to cluster)\n\nI could just connected CPU resources between ubuntu 18.04 Linux distribution.\n\nWhy this happening?\n\nPlease let me know.\n\nMachines:\n\n1.Cluster structure  ( This worked)\nLinux Machines:\nx.x.x.x =&gt; Head Node       =&gt; Ubuntu 18.04.4 LTS\nx.x.x.x  =&gt; Worker Node     =&gt; Ubuntu 18.04.4 LTS\nx.x.x.x  =&gt; Worker Node     =&gt; Ubuntu 18.04.4 LTS\n\n2.Cluster structure(On this scenario, Ubuntu  CPU resource adding to cluster. But Centos CPU resource not adding to cluster)\nLinux Machines:\nx.x.x.x  =&gt; Head Node     =&gt; Centos\nx.x.x.x   =&gt; Worker Node    =&gt; Ubuntu\nx.x.x.x    =&gt; Worker Node    =&gt; Ubuntu\n\n\n3.Cluster structure  (On this scenario, only the head node is added to the cluster. CPU of other machines are not adding  to the cluster )\nLinux Machines:\nx.x.x.x  =&gt; Head Node       =&gt; CentOS-7\nx.x.x.x  =&gt; Worker Node     =&gt; CentOS-7\nx.x.x.x  =&gt; Worker Node     =&gt; CentOS-7"}
{"question": "Hi ! Just a question about ray lightning : does anybody knows if distributing trainings with ray lightning gives the same performances than when distributing with ray tune ? I am not speaking about additionnal features of ray tune such as hyperparameters tuning, but only about training distribution.\nThanks !"}
{"question": "Hi, Does passing `\"_system_config\": {\"automatic_object_spilling_enabled\": False}`  to ray completely prevent object spillage? What happens to the scheduled calls?"}
{"question": "Hi, Is there a way to programmatically fetch the logs of a specific workflow with it\u2019s `workflow_id` without having to hunt for them on the dashboard?"}
{"question": "I have successfully implemented key phrase extraction code using two approaches 1) Reading data in python data structures and processing it using Ray Actor Pool. 2) Reading data with Ray Dataset and processing it. How to decide between processing data with python lists (decorated with ray actors) vs Ray Dataset considering performance and other aspects? I am working on keyphrase extraction which uses spacy model.\nMy data lies in S3 and I have the option to keep my data in s3 either in multiple json files or parquet. In this experiment,  for ray actor implementation, i read json file and for Ray dataset implementation, i read data from parquet files. Dataset provides api to read files directly from s3 which makes it easier. If I go ahead with Ray Actor Pool I would have to write another helper function for reading files from S3. I was hoping to learn about general guidelines while developing ray applications (in this case using ray dataset vs python data structures(decorated with ray actors) for accomplishing my task.\n\nSharing snippets of code below.\n\nUsing Ray Actor Pool:\n\n```@ray.remote\nclass Textprocessor:\n    def __init__(self):\n\n       #setup instructions for spacy model\n        import pytextrank\n        self.nlp = spacy.load(\"en_core_web_lg\")\n        self.nlp.max_length = 1080000\n        self.nlp.add_pipe(\"textrank\")\n\n  def process(self, args):\n         ## some processing ###\n        return\n  def process_func2(self, args):\n         ## some processing ###\n        return\n  def mainprocess(self, doc):\n         ## some processing ###\n        return keyphrases\n\ndocuments = read_json(path) #read json is a utility function and documents is list of dict.\nactors = []\nfor actor in range(int(ray.cluster_resources()['CPU'])):\n    actors.append(Textprocessor.remote())\npool = ActorPool(actors)\nfor output in pool.map_unordered(lambda a, v: a.mainprocess.remote(v), documents):\n## processing```\nUsing Ray Datasets:\n\n```ds = ray.data.read_parquet(\"S3//PATH\")\nclass TransformBatch:\n    def __init__(self):\n        import pytextrank\n\n        self.nlp = spacy.load(\"en_core_web_lg\")\n        self.nlp.max_length = 1080000  \n        self.nlp.add_pipe(\"textrank\")  \n\n  def process(self, args):\n         ## some processing ###\n        return\n  def process_func2(self, args):\n         ## some processing ###\n        return\n\n    def __call__(self, batch):\n        import copy\n        batch = copy.deepcopy(batch)\n        batch['lower_text'] = batch['text'].map(str.lower)\n        batch['spacy_docs'] = batch['lower_text'].map(self.nlp)\n        batch['doc_phrases'] = batch['spacy_docs'].map(self.process)\n        batch['key_phrases'] = batch['doc_phrases'].map(self.process_func2)\n        return pd.DataFrame(batch['key_phrases'])  #only need key phrases\n\nds = ds_docs.map_batches(TransformBatch, compute=ActorPoolStrategy(6,8), batch_format='pandas')```"}
{"question": "How does one get the embedded metrics from Grafana to show up in the dashboard?  It's not connecting because the url it's trying to connect to is \"localhost\", and the head is remote, i.e.: `<http://localhost:3000/d-solo/rayDefaultDashboard/default-dashboard?orgId=1&amp;theme=light&amp;panelId=26&amp;refresh&amp;from=now-1h&amp;to=now>`.  Where can I specify in the dashboard config to use the host address instead of localhost for these links or make them relative to the dashboard (since in this case Grafana, dashboard, and Prometheus are all on the head node."}
{"question": "I wonder how ray tune syncer deal w/ file syncing? I saw it was done via pyarrow's fsspec `copy_files`, which doesn't seem to ensure file safety (like what if the syncing process crashed and file was corrupted) etc?"}
{"question": "Hey everybody, so I'm using the `rayproject/ray-ml:b42155-py310-cu116` image and I'm trying to run the IMPALA agent as described in the documentation <https://docs.ray.io/en/master/rllib/rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala|RLlib IMPALA documentation>\n```from ray.rllib.algorithms.impala import ImpalaConfig\nconfig = ImpalaConfig()\nconfig = config.training(lr=0.0003, train_batch_size=512)  # doctest: +SKIP\nconfig = config.resources(num_gpus=4)  # doctest: +SKIP\nconfig = config.rollouts(num_rollout_workers=64)  # doctest: +SKIP\nprint(config.to_dict())  # doctest: +SKIP\n# Build a Algorithm object from the config and run 1 training iteration.\nalgo = config.build(env=\"CartPole-v1\")  # doctest: +SKIP\nalgo.train()  # doctest: +SKIP```\nWhen I run this with 1 gpu (changing num_gpus=1) It crashes\n```INVALID_ARGUMENT: Input to reshape is a tensor with 512 values, but the requested shae has 500```\nDoes anyone have any idea why is this not working out of the box ?"}
{"question": "Hi guys, I am using RayLightning to train my model, but I am still using an InMemoryDataset (from torch_geometric.data) and a classic DataLoader (from torch_geometric.loader) to feed my pl.LightningModule (using a RayStrategy on my pytorch_lightning Trainer of course).\nMy question is quite simple : *is there any best practice to take benefit from Ray Clusters when creating Dataset or DataLoader ?* Should I use other classes ? plt.LightningDataModule seems interesting but I am wondering about its compatibility with Ray..."}
{"question": "In other words, how to prepare DataLoader for better usage of RayStrategy ? :slightly_smiling_face: Thank you"}
{"question": "Question: I'm defining a callback class that overrides the `on_train_result` so I could add some measures to my PPO train results. However, some of my measures require looking at the trajectories that the algorithm sampled while training. I assume these aren't saved by default. How do I get access to these trajectories? Is there another callback method I should be overriding?\nI noticed there's a method `on_sample_end` that's called after sampling, but I'm not sure whether this will also be called for the sampling done by training, or whether it'll require a separate sampling operation, which I prefer to avoid."}
{"question": "Hi - is there some way to specify the SSH port number when using `local` <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/local/example-full.yaml|provider> in the ray autoscaler? My VMs expose SSH on port 10000 instead of 22 and as a I result I can't use the ray autoscaler to launch a ray cluster.\n\nThis is also tracked in <https://github.com/ray-project/ray/issues/30452|#30452>, looking for a workaround if anyone is aware of one."}
{"question": "Hi Team,\nI am trying to setup GCS Fault Tolerance (FT) with an external Redis instance (which is using rediss, but that actually doesn't matter, as it doesn't even try to connect to the remote instance).\nI am using Ray 2.2 and as soon as I set the `RAY_REDIS_ADDRESS` env variable on the head node, the head node errors out with connection refused - still trying to connect to the localhost on port 6379 (!!!!).\nSo instead of trying to connect to the provided remote Redis server, it tries to connect to the head node for some reason (observe in the below logs that it tries to connect to the IP of the local node instead of the provided Redis instance).\n\nAs soon as the `RAY_REDIS_ADDRESS` env variable is removed, it start working (but obv. not connecting to the remote Redis).\n\nBelow is the error from the head node's log:\n```2022-12-01 14:28:35,349\tINFO usage_lib.py:487 -- Usage stats collection is disabled.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in &lt;module&gt;\n2022-12-01 14:28:35,349\tINFO scripts.py:702 -- Local node IP: 10.47.117.199\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/scripts/scripts.py\", line 2386, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1055, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/cli_logger.py\", line 852, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/scripts/scripts.py\", line 729, in start\n    node = ray._private.node.Node(\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/node.py\", line 269, in __init__\n    self.start_head_processes()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/node.py\", line 1059, in start_head_processes\n    self._write_cluster_info_to_kv()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/node.py\", line 1043, in _write_cluster_info_to_kv\n    ray_usage_lib.put_cluster_metadata(self.get_gcs_client())\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/usage/usage_lib.py\", line 573, in put_cluster_metadata\n    gcs_client.internal_kv_put(\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/gcs_utils.py\", line 179, in wrapper\n    return f(self, *args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/gcs_utils.py\", line 298, in internal_kv_put\n    reply = self._kv_stub.InternalKVPut(req, timeout=timeout)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/grpc/_channel.py\", line 946, in __call__\n    return _end_unary_response_blocking(state, call, False, None)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/grpc/_channel.py\", line 849, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)\ngrpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:10.47.117.199:6379: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:10.47.117.199:6379: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2022-12-01T14:28:40.369039071-08:00\"}\"```\n\nAnd below is the head node setup:\n```  headGroupSpec:\n    serviceType: ClusterIP\n    rayStartParams:\n      # port: '6379'\n      dashboard-host: '0.0.0.0'\n      node-ip-address: \"$MY_POD_IP\" # auto-completed as the head pod IP\n      block: 'true'\n      num-cpus: '1' # can be auto-completed from the limits\n      metrics-export-port: '8080'\n      disable-usage-stats: 'true'\n      redis-password: \"secretpassword\"\n    template:\n      spec:\n        containers:\n        - name: ray-head\n          image: rayproject/ray:2.2.0-py39\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: CPU_REQUEST\n            valueFrom:\n              resourceFieldRef:\n                containerName: ray-head\n                resource: requests.cpu\n          - name: CPU_LIMITS\n            valueFrom:\n              resourceFieldRef:\n                containerName: ray-head\n                resource: limits.cpu\n          - name: MEMORY_LIMITS\n            valueFrom:\n              resourceFieldRef:\n                containerName: ray-head\n                resource: limits.memory\n          - name: MEMORY_REQUESTS\n            valueFrom:\n              resourceFieldRef:\n                containerName: ray-head\n                resource: requests.memory\n          - name: MY_POD_IP\n            valueFrom:\n              fieldRef:\n                fieldPath: status.podIP\n           # RAY_REDIS_ADDRESS can force ray to use external redis\n           # see more at <https://github.com/ray-project/kuberay/blob/master/docs/guidance/gcs-ft.md#use-external-redis-cluster>\n          - name: RAY_REDIS_ADDRESS\n            # the error is the same even without the \"redis://\" part\n            value: <rediss://master.whatever.use1.cache.amazonaws.com:6379>\n          ports:\n          - containerPort: 8265\n            name: dashboard\n          - containerPort: 10001\n            name: client\n          - containerPort: 8080\n            name: metrics\n          - containerPort: 44217\n            # name can't be longer than 15 chars\n            name: autoscalermetri\n          lifecycle:\n            preStop:\n              exec:\n                command: [\"/bin/sh\",\"-c\",\"ray stop\"]\n          resources:\n            limits:\n              cpu: \"1\"\n              memory: \"4G\"\n            requests:\n              cpu: \"500m\"\n              memory: \"2G\"```\nAny ideas?"}
{"question": "Question...\n\nWe would like to keep the head node on AWS on-demand and the workers on spot.  We can do this, but might prefer using a different instance type for the head rather than the workers and not actually have the head do any work since it may be more economical to configure that node to be optimized for being a head node, rather than doing work.  Is there a way to configure the Ray cluster where the head node does not participate as a worker?"}
{"question": "Hi team,\nI have upgraded to Ray v2.2 so I can take advantage of the `rediss://` option to use an external Redis instance via TLS.\nThe external GCS with Fault Tolerance seems to work, but now I am seeing the autoscaler pod crashing with the following error:\n```2022-12-01 17:09:22,635\tINFO monitor.py:196 -- Starting autoscaler metrics server on port 44217\n2022-12-01 17:09:22,636\tINFO monitor.py:216 -- Monitor: Started\n2022-12-01 17:09:22,672\tINFO node_provider.py:211 -- Creating KuberayNodeProvider.\n2022-12-01 17:09:22,673\tINFO autoscaler.py:269 -- disable_node_updaters:True\n2022-12-01 17:09:22,673\tINFO autoscaler.py:277 -- disable_launch_config_check:True\n2022-12-01 17:09:22,673\tINFO autoscaler.py:289 -- foreground_node_launch:True\n2022-12-01 17:09:22,673\tINFO autoscaler.py:299 -- worker_liveness_check:False\n2022-12-01 17:09:22,673\tINFO autoscaler.py:307 -- worker_rpc_drain:True\n2022-12-01 17:09:22,673\tINFO autoscaler.py:355 -- StandardAutoscaler: {'provider': {'type': 'kuberay', 'namespace': 'ray-cluster', 'disable_node_updaters': True, 'disable_launch_config_check': True, 'foreground_node_launch': True, 'worker_liveness_check': False, 'worker_rpc_drain': True}, 'cluster_name': 'raycluster-autoscaler', 'head_node_type': 'head-group', 'available_node_types': {'head-group': {'min_workers': 0, 'max_workers': 0, 'node_config': {}, 'resources': {'CPU': 1, 'memory': 4000000000}}, 'small-group': {'min_workers': 1, 'max_workers': 300, 'node_config': {}, 'resources': {'CPU': 4, 'memory': 4294967296}}, 'gpu-group': {'min_workers': 1, 'max_workers': 5, 'node_config': {}, 'resources': {'CPU': 4, 'GPU': 1, 'memory': 16148070400}}}, 'max_workers': 305, 'idle_timeout_minutes': 1.0, 'upscaling_speed': 1000, 'file_mounts': {}, 'cluster_synced_files': [], 'file_mounts_sync_continuously': False, 'initialization_commands': [], 'setup_commands': [], 'head_setup_commands': [], 'worker_setup_commands': [], 'head_start_ray_commands': [], 'worker_start_ray_commands': [], 'auth': {}}\n2022-12-01 17:09:22,725\tERROR autoscaler.py:363 -- StandardAutoscaler: Error during autoscaling.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/autoscaler.py\", line 360, in update\n    self._update()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/autoscaler.py\", line 383, in _update\n    self.non_terminated_nodes = NonTerminatedNodes(self.provider)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/autoscaler.py\", line 118, in __init__\n    self.all_node_ids = provider.non_terminated_nodes({})\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/batching_node_provider.py\", line 155, in non_terminated_nodes\n    self.node_data_dict = self.get_node_data()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 237, in get_node_data\n    resource_version = self._get_pods_resource_version()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 358, in _get_pods_resource_version\n    pod_resp = self._patch(resource_path, payload)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 442, in _patch\n    result.raise_for_status()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: <https://kubernetes.default:443/api/v1/namespaces/ray-cluster/pods/raycluster-autoscaler-head-wwns5>\n2022-12-01 17:09:22,726\tINFO monitor.py:382 -- :event_summary:Resized to 9 CPUs, 1 GPUs.\n2022-12-01 17:09:27,776\tERROR autoscaler.py:363 -- StandardAutoscaler: Error during autoscaling.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/autoscaler.py\", line 360, in update\n    self._update()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/autoscaler.py\", line 383, in _update\n    self.non_terminated_nodes = NonTerminatedNodes(self.provider)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/autoscaler.py\", line 118, in __init__\n    self.all_node_ids = provider.non_terminated_nodes({})\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/batching_node_provider.py\", line 155, in non_terminated_nodes\n    self.node_data_dict = self.get_node_data()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 237, in get_node_data\n    resource_version = self._get_pods_resource_version()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 358, in _get_pods_resource_version\n    pod_resp = self._patch(resource_path, payload)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 442, in _patch\n    result.raise_for_status()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: <https://kubernetes.default:443/api/v1/namespaces/ray-cluster/pods/raycluster-autoscaler-head-wwns5>\n2022-12-01 17:09:32,828\tERROR autoscaler.py:363 -- StandardAutoscaler: Error during autoscaling.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/autoscaler.py\", line 360, in update\n    self._update()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/autoscaler.py\", line 383, in _update\n    self.non_terminated_nodes = NonTerminatedNodes(self.provider)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/autoscaler.py\", line 118, in __init__\n    self.all_node_ids = provider.non_terminated_nodes({})\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/batching_node_provider.py\", line 155, in non_terminated_nodes\n    self.node_data_dict = self.get_node_data()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 237, in get_node_data\n    resource_version = self._get_pods_resource_version()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 358, in _get_pods_resource_version\n    pod_resp = self._patch(resource_path, payload)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/kuberay/node_provider.py\", line 442, in _patch\n    result.raise_for_status()\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: <https://kubernetes.default:443/api/v1/namespaces/ray-cluster/pods/raycluster-autoscaler-head-wwns5>```\nThis doesn't seem to be related to the GCS FT. Maybe this is something triggered by Ray v2.2?"}
{"question": "Hiya I\u2019m trying to run <https://docs.ray.io/en/latest/train/examples/tensorflow_mnist_example.html|tensorflow_mnist_example> locally on my machine but it gets stuck in pending:\n\n```== Status ==\nCurrent time: 2022-12-02 02:12:37 (running for 00:01:05.14)\nMemory usage on this node: 1.1/7.5 GiB\nUsing FIFO scheduling algorithm.\nResources requested: 0/2 CPUs, 0/0 GPUs, 0.0/4.14 GiB heap, 0.0/2.07 GiB objects\nResult logdir: /home/compute/ray_results/TensorflowTrainer_2022-12-02_02-11-32\nNumber of trials: 1/1 (1 PENDING)\n+-------------------------------+----------+-------+\n| Trial name                    | status   | loc   |\n|-------------------------------+----------+-------|\n| TensorflowTrainer_a3dff_00000 | PENDING  |       |\n+-------------------------------+----------+-------+\n\n\n2022-12-02 02:12:42,555\tWARNING insufficient_resources_manager.py:128 -- Ignore this message if the cluster is autoscaling. You asked for 3.0 cpu and 0 gpu per trial, but the cluster only has 2.0 cpu and 0 gpu. Stop the tuning job and adjust the resources requested per trial (possibly via `resources_per_trial` or via `num_workers` for rllib) and/or add more resources to your Ray runtime.```\nWhy does it say it\u2019s asking for 3.0 cpu when I only asked for 2.0?"}
{"question": "hello. is there any way to specify an `endpoint_url` for the `storage` option of `ray.init`? i can\u2019t find anything in the docs, other than a mention that any boto3 env vars should work. but boto3 itself does not support endpoint_url via env var. it does support specifying the endpoint url via constructor args\u2026\n\n\u2026 hence my question here: does `ray.init` offer a way to pass through an `endpoint_url` to the boto3 client constructor? looking at the source, i think the answer is no. looking at the <https://github.com/boto/boto3/pull/2746|boto3 PR>, that has been unmerged for almost 2 years, i can only assume that the implicit answer from AWS is also \u2026 no.\n\nthis seems to render storage relatively useless, unless you are willing and able to use default aws endpoints for your storage. unless i am missing something? thanks!"}
{"question": "I'm probably missing something simple, but we are using the `ray up` with autoscaler yaml approach to launching a cluster.  The desire is to build into the yaml everything we need to run Grafana and Prometheus.  We tried adding the commands to nohup background Grafana and Prometheus.  This works well when run manually from inside the container.  It also works taking the exact same commands output from the `ray up -v` command:\n\n``` Running `docker exec -it  ray_container /bin/bash -c 'bash --login -c -i '\"'\"'true &amp;&amp; source ~/.bashrc &amp;&amp; export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore &amp;&amp; (cd /root/prometheus-2.40.4.linux-amd64/ &amp;&amp; nohup /root/prometheus-2.40.4.linux-amd64/prometheus --config.file=/tmp/ray/session_latest/metrics/prometheus/prometheus.yml &amp;)'\"'\"'' ````\nHowever, for some reason, when launched in the ray up process, both Prometheus and Grafana processes are not running.  Is the container stopped and started between stages?  We tried putting the commands in the head setup section and in the ray start section of the yaml for the head node."}
{"question": "Hey guys, \nI have developed a regression model and looking forward to optimize the hidden layer size but it seems that Bayesian optimization and PBT are not suitable for this, is it correct?\nThe search space is very big so this can not be achieved by using grid search or random search. Is there any optimization algorithm suitable for this?\n\nI tried ray PBT but it seems that the returned model is not even close to the optimal model \nI would greatly appreciated if anyone could help me in this regard.\nThanks"}
{"question": "The ref count here means the borrower\u2019s local ref count( local Python count + submitted task count), right?"}
{"question": "Hi folks. Successfully set up a Ray cluster with `ray up cluster.yaml`  and then submitted a job with `ray job submit --address http://&lt;head IP&gt;:8265 --working-dir &lt;my workdir&gt; -- python myapp.py`  . Problem is that it also showed me that *port 8265 is not secure*. Is there way [easy if possible] to secure a Ray cluster so that  only permitted users can remotely submit work to it?\nUPDATE: I tried tunneling port 8265. Now `ray job submit` to localhost:8265 does not work, yelling about GCS connection. So I tried tunneling port 6379 as well. Still no go."}
{"question": "Hello,\nWhen running `ray down` followed by a `ray up` (with some time in between), using the same config file, I'm noticing that some files still remain.\nHow can it be? Is my cloud provider responsible for that behavior?\nThanks."}
{"question": "how do I force scaling down a remote cluster?\nsometimes I found that all my jobs are completed (success), but the workers are still there.\nmy min workers are set to 0, and the upscaling speed is set to 1."}
{"question": "Hi, Does anyone have had the \"ClientObjectRef {rid} is not found for client\" problem (<https://github.com/ray-project/ray/blob/master/python/ray/util/client/server/server.py#L478|https://github.com/ray-project/ray/blob/master/python/ray/util/client/server/server.py#L478>)? I ran into this occasionally. How can I debug this? Thanks in advance. :pray:"}
{"question": "Hi team,\nI am trying out the Ray 2.2 docker images with Python3.10 and I have some Python libraries that don't have Python3.10 wheels on Pypi, so those are trying to build from source, but there is no `gcc` in the Ray docker images so they fail with `error: command 'gcc' failed: No such file or directory`\nI am trying to avoid building my own Docker images with gcc, so I just wanted to quickly check, that there is no way around it, like using the `RuntimeEnv` to get `gcc` or something?"}
{"question": "Hi I am using XGBoost API like this. In this case, self.dmat_train is from xgboost_ray but bst is still from xgboost library which is not compatible. Does anyone know how to implement this in ray manner?\n\n```eval_results = self.bst.eval_set(\n            evals=[(self.dmat_train, \"train\"), (self.dmat_valid, \"valid\")], iteration=self.bst.num_boosted_rounds() - 1\n        )```\n"}
{"question": "Every time a *Ray client* connects to my Ray cluster via `ray head` using `ray.init()`, an IDLE worker is spun up in `ray head` before I\u2019ve even submitted any Tasks.\n\nIt\u2019s almost as if I\u2019m required to have an IDLE worker in `ray head` for each client connection to the cluster.\n\nAnyone know if this is expected or if there\u2019s a setting to prevent this? Also, we\u2019ve set `num_cpus=0` on `ray head`."}
{"question": "Hi, it looks like I can access all reported metrics after training as part of the returned results object. However,  how I can access any metrics provided to `session.report` during training? Thank you in advance"}
{"question": "I see ray's source code and find a core worker generate a taskID randomly, but this can't enssure the generated TaskID is unique, so if this will may lead to wrong behavior? `"}
{"question": "Hey guys! As far as I understand from docs and GitHub issues discussion <https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html|Ray Client>  is not the best way to implement programmatic task management from lets say API to cluster as client disconnect will break current workload, also we need to connect to multiple clusters simultaneously, so issues with `allow_multiple=True` preventing us to resolve that scenario. So the question is - is JobSubmissionClient (<https://docs.ray.io/en/latest/cluster/running-applications/job-submission/sdk.html|Python SDK>) right way to go?"}
{"question": "Quick question: should I be using `RAY_memory_monitor_interval_ms` or `RAY_memory_monitor_*refresh*_ms`? The documentation is different in different places:\n\u2022 <https://github.com/ray-project/ray/blob/master/doc/source/ray-core/scheduling/ray-oom-prevention.rst>\n\u2022 <https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html>"}
{"question": "What would happen on ray if we have a ~job~ function on one node running child ~jobs~ functions on other nodes and the parent node dies. Do the child ~jobs~ functions get canceled, and if so, which part of ray would be responsible for doing so?"}
{"question": "Guys, one question about *workflows* and *actors*.\n\nI have one workflow that:\n1. Gets a Dataset from CSV file\n2. Creates an Actor to process the data\n3. Writes the results to a parquet file\nThe actor is being created using `map_batches`, like this:\n\n```data.map_batches(RowCounter, compute=ray.data.ActorPoolStrategy(1, 1), batch_format=\"pyarrow\")```\nI've noticed that the Actor creation time is taking somewhat near 8 seconds, while the processing of `__call__` is really fast (below 50ms). How can we speed up the initialization of Actors?"}
{"question": "Hi,\nI am using ray tune PBT to find the best values for hidden layers.\nI trained a model based on PBT results, but the val_loss for PBT is 0.007, and the trained model val_loss is 0.03. This difference is so huge in this context. Is there a reason for the difference?"}
{"question": "I got pickling error when trying to pickle RLock which dill supports.\nI submitted a ticket to github here: <https://github.com/ray-project/ray/issues/30972>\nAny chance `ray.cloudpickle` can support it?"}
{"question": "How is it possible that ray over schedules tasks on a single node? This is the raylet.out. It says `CPU: -160000/160000`\n\n```2034[state-dump] ========== Node: 401e567fbdb3af1c40e53f1c74e87f3c99ea7744bcf7f588f088f191 =================2035[state-dump] Infeasible queue length: 02036[state-dump] Schedule queue length: 02037[state-dump] Dispatch queue length: 02038[state-dump] num_waiting_for_resource: 02039[state-dump] num_waiting_for_plasma_memory: 02040[state-dump] num_waiting_for_remote_node_resources: 02041[state-dump] num_worker_not_started_by_job_config_not_exist: 02042[state-dump] num_worker_not_started_by_registration_timeout: 02043[state-dump] num_tasks_waiting_for_workers: 02044[state-dump] num_cancelled_tasks: 02045[state-dump] cluster_resource_scheduler state: 2046[state-dump] Local id: 9012481561590922657 Local resources: {CPU: [-160000]/[160000], node:10.164.0.61: [10000]/[10000], object_store_memory: [375068147710000]/[375068147710000], memory: [875159011330000]/[875159011330000]}node id: 7071842016779710930{memory: 874830831620000/874830831620000, node:10.164.0.34: 10000/10000, CPU: 0/160000, object_store_memory: 374927499260000/374927499260000}node id: 5352799242744933693{CPU: 0/160000, object_store_memory: 374851805180000/374851805180000, memory: 874654212100000/874654212100000, node:10.164.0.7: 10000/10000}node id: 7766600522285203646{memory: 83284156430000/83284156430000, object_store_memory: 41642078200000/41642078200000, node:10.164.0.6: 10000/10000}node id: 9012481561590922657{memory: 875159011330000/875159011330000, CPU: -160000/160000, object_store_memory: 375068147710000/375068147710000, node:10.164.0.61: 10000/10000}node id: -3675461679350364834{node:10.164.0.47: 10000/10000, object_store_memory: 374937169920000/374937169920000, memory: 874853396480000/874853396480000, CPU: 0/160000}node id: -7898719215062364763{CPU: 0/160000, node:10.164.0.51: 10000/10000, memory: 875165405190000/875165405190000, object_store_memory: 375070887930000/375070887930000}node id: 1263756040641322346{node:10.164.0.22: 10000/10000, CPU: 0/160000, object_store_memory: 374050492410000/374050492410000, memory: 872784482310000/872784482310000}node id: -7334366962099012703{node:10.164.0.49: 10000/10000, CPU: 0/160000, memory: 875150381060000/875150381060000, object_store_memory: 375064449020000/375064449020000}node id: -118809529601516935{memory: 875148316680000/875148316680000, node:10.164.0.33: 10000/10000, CPU: 0/160000, object_store_memory: 375063564280000/375063564280000}{ \"placment group locations\": [], \"node to bundles\": []}2047[state-dump] Waiting tasks size: 02048[state-dump] Number of executing tasks: 82049[state-dump] Number of pinned task arguments: 02050[state-dump] Number of total spilled tasks: 02051[state-dump] Number of spilled waiting tasks: 02052[state-dump] Number of spilled unschedulable tasks: 02053[state-dump] Resource usage {2054[state-dump]     - (language=PYTHON actor_or_task=run pid=7853): {CPU: 4.000000}2055[state-dump]     - (language=PYTHON actor_or_task=run pid=7852): {CPU: 4.000000}2056[state-dump]     - (language=PYTHON actor_or_task=run pid=7855): {CPU: 4.000000}2057[state-dump]     - (language=PYTHON actor_or_task=run pid=6805): {CPU: 4.000000}2058[state-dump]     - (language=PYTHON actor_or_task=run pid=6806): {CPU: 4.000000}2059[state-dump]     - (language=PYTHON actor_or_task=run pid=7854): {CPU: 4.000000}2060[state-dump]     - (language=PYTHON actor_or_task=run pid=6804): {CPU: 4.000000}2061[state-dump]     - (language=PYTHON actor_or_task=run pid=6803): {CPU: 4.000000}2062[state-dump] }```"}
{"question": "Hi guys, any idea of how to solve this issue when running my pytorch lighning model with ray strategy (from ray lightning) on a ray client on docker ?\n`ConnectionError: Failed during this or a previous request. Exception that broke the connection: &lt;_MultiThreadedRendezvous of RPC that terminated with:`\n        `status = StatusCode.FAILED_PRECONDITION`\n        `details = \"cannot instantiate 'WindowsPath' on your system\"`\n        `debug_error_string = \"{\"created\":\"@1670582535.159000000\",\"description\":\"Error received from peer ipv6:[::1]:10001\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1075,\"gr`\n`pc_message\":\"cannot instantiate 'WindowsPath' on your system\",\"grpc_status\":9}\"`"}
{"question": "hi team, is it possible to put tensorflow saved_model in the plasma directly?"}
{"question": "hiya, what's the significance of `arm` in `azure_arm_parameters` for cluster config? it seems to just be vm_config as far as I can tell"}
{"question": "Hi All,\n\nI'm a happy user of Ray Core for several weeks now.\nMy setup:\n\u2022 I want to tune multiple pytorch models. \n\u2022 I'm not a user of Ray Tune, I'm just generating a list of sets of hyperparams, and then each set is executed by a different ray actor on some node \n\u2022 I have a few separate Ray nodes, each having several GPUs.\n\u2022 Usually I allocate 1 GPU and 8-10 CPU per actor\nMy challenge: *I want to use many processes within one actor, but all processes should be scheduled only on the same machine that given actor is executed*\n(The main motivation is that I want to read from actor's local disk, load a lot of data to memory and do preprocessing (in parallel) )\n\nNow, I have a huge problem with using multiprocessing inside actors:\n\u2022 if I use python or torch multiprocessing.Pool,  Ray training just hangs \n\u2022 if I use Ray multiprocessing, it's not working as I would like it to. For example,  task1 on machine A would call a function to load data (from the local disk of machine A),  but this function could be scheduled to execute on machine B (since it's connected to Ray cluster that spans several nodes),  and machine (node) B could not have the data\n\u2022 creating RayPool with ray_address=None  will just connect me to existing ray_cluster (for actors) which would end up with behavior described above\n\u2022 creating RayPool with ray_address=\"local\" will create a second ray instance on a given node, and will hang my job\nAny suggestions for how to workaround this?"}
{"question": "Hi all,\n\nqq about Ray Tune. It seems on my machine the standard setup fails. Minimal code repro:\n```from ray import tune\nimport ray\nimport os\n\nNUM_MODELS = 100\n\ndef train_model(config):\n    score = config[\"model_id\"]\n\n    return {\"score\": 1.0}\n\ntrial_space = {\n    \"model_id\": tune.grid_search([\n        \"model_{}\".format(i)\n        for i in range(NUM_MODELS)\n    ])\n}\n\ntrain_model = tune.with_resources(train_model, {\"cpu\": 1})\ntuner = tune.Tuner(train_model, param_space=trial_space)\nresults = tuner.fit()```\nBefore running this code I created a new virtualenv and installed just ray[tune]  and protobuf\n```pip install ray[tune]==2.1.0\npip install protobuf==3.19.4```\nPython version: 3.8.10\n\nMy result:\nImportError: ray.tune in ray &gt; 0.7.5 requires 'tabulate'. Please re-run 'pip install ray[tune]' or 'pip install ray[rllib\n\nThe tabulate is installed however:\n```pip list | grep tabulate\ntabulate             0.9.0    ```\nAny ideas?"}
{"question": "Hi all:\nI'm using RLlib. I'm trying to train an agent only on minibatches from single episode, which mean every minibatch only contain transitions from one episode. However, the mini-batch sampling in PPO often sample minibatches from multiple episodes, which makes the advantage calculation in PPO inaccurate. I think there's two way to do that: 1. padding and clipping all episode to the same length 2. the rollout worker only sample one episode at one time, and disable \"minibatch_size\"\nHowever, I don't find any way to do that. Even when I use LSTM wrapper, the sampler considers the end of an episode, but it doesn't choose right offset and minibatch_size but return a 0 size minibatch.\nAnyone have idea how to do this?"}
{"question": "Is there a way to pip install a specific version of ray (e.g a git commit?)"}
{"question": "Can someone from the Ray team take a look at this issue: <https://github.com/pytorch/data/issues/868>? I think there's some sort of bug with a specific torchdata feature + Ray, &amp; it's preventing me from doing multi-epoch training.\n\nI've been pinging the Pytorch people, but I think it might be hard for them to reproduce since they're not using Ray?\n\n(Corresponding Ray forum URL: <https://discuss.ray.io/t/get-distributed-process-group-timeout-when-using-torch-trainer-fullsynciterdatapipe/8075>)"}
{"question": "If I have a team of 10+ people, is the recommended way of deploying ray on cloud one smaller cluster per user or is it one large cluster for the whole team?"}
{"question": "Hi all, any idea why i get the issue when using map_batches since the upload 2.2.0 ?\n`Map_Batches:   0%|                                                                                                                                           | 0/20 [00:00&lt;?, ?it/s]T`\n`raceback (most recent call last):`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\train_model_topology.py\", line 84, in &lt;module&gt;`\n    `train_model(config)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\train_model_topology.py\", line 33, in train_model`\n    `ds_train, ds_valid, fields_infos = ds_builder.get_dataset()`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\dataset_builder.py\", line 46, in get_dataset`\n    `dataset, fields_infos = self.create_ds()`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\dataset_builder.py\", line 108, in create_ds`\n    `dataloader = DatasetLoader(self.config, self.project, self.columns)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\src\\dataset_loader.py\", line 34, in __init__`\n    `self.get_all_graph_instances_with_ray()`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\src\\dataset_loader.py\", line 76, in get_all_graph_instances_with_ray`\n    `all_graph_instances_ds = self.process_keys_ray.map_batches(`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\data\\dataset.py\", line 617, in map_batches`\n    `return Dataset(plan, self._epoch, self._lazy)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\data\\dataset.py\", line 223, in __init__`\n    `self._plan.execute(allow_clear_input_blocks=False)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\data\\_internal\\plan.py\", line 314, in execute`\n    `blocks, stage_info = stage(`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\data\\_internal\\plan.py\", line 678, in __call__`\n    `blocks = compute._apply(`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\data\\_internal\\compute.py\", line 102, in _apply`\n    `refs = [`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\data\\_internal\\compute.py\", line 103, in &lt;listcomp&gt;`\n    `map_block.remote(`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\remote_function.py\", line 226, in remote`\n    `return func_cls._remote(args=args, kwargs=kwargs, **updated_options)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 307, in _invocation_remote_span`\n    `return method(self, args, kwargs, *_args, **_kwargs)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\remote_function.py\", line 247, in _remote`\n    `return client_mode_convert_function(self, args, kwargs, **task_options)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 178, in client_mode_convert_function`\n    `return client_func._remote(in_args, in_kwargs, **kwargs)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\util\\client\\common.py\", line 298, in _remote`\n    `return self.options(**option_args).remote(*args, **kwargs)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\util\\client\\common.py\", line 572, in remote`\n    `return return_refs(ray.call_remote(self, *args, **kwargs))`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\util\\client\\api.py\", line 100, in call_remote`\n    `return self.worker.call_remote(instance, *args, **kwargs)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\util\\client\\worker.py\", line 559, in call_remote`\n    `return self._call_schedule_for_task(task, instance._num_returns())`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\util\\client\\worker.py\", line 569, in _call_schedule_for_task`\n    `id_futures = [Future() for _ in range(num_returns)]`\n`TypeError: 'str' object cannot be interpreted as an integer`"}
{"question": "Hello,\nQuestion about Ray client usage with a remote Ray cluster. We are developing a API service to launch different trains.\nSo there is a possibility that we need to launch multiple trains.\nMy question is, do you recommend in this case to have different ray client (ray init with different namespace) or only one ray client ?\nHow to manage queues for the different trains with ray?\n\nThanks"}
{"question": "Does anyone use DVC and / or dagshub with Ray? We\u2019re having a bunch of issues with wandb for our amount of data files"}
{"question": "Has anyone tried WandB with Ray? How it goes?\nAny suggestions on best practices for reproducing old ML experiments with Ray as training platform on top of PyTorch?"}
{"question": "If I want to resume an experiment, is there any way I can change the configuration?"}
{"question": "Does Ray 2.2.0 still have record_env feature? I set it to be true, but no video is saved?"}
{"question": "Hi! Does anyone know how to have an object\u2019s state updated or returned from a `.map_batches` operation? `.map_batches` doesn\u2019t update the state of `transformer` in the below example, which is blocking the common pattern of fitting a transformer before using it:\n```import ray\n\n\nray.init(local_mode=True)\n\nclass Transformer:\n    def fit(self, batch):\n        self.fit = True\n        return batch\n\n    def transform(self, batch):\n        assert self.fit == True\n        return batch\n\n\nds = ray.data.range(10)\ntransformer = Transformer()\n\nds.map_batches(transformer.fit)\nds.map_batches(transformer.transform)  # results in an AssertionError because assert self.fit != True\n\n# for comparison, this does not produce an AssertionError\ndef f(ds, callable):\n    callable(ds)\n    pass\nf(ds, transformer.fit)\nf(ds, transformer.transform)```\nThank you!"}
{"question": "Hi, everyone. I'm wondering does the LSTM wrapper can accept a batch containing more than one episode, and divide them into episodes to propagate?"}
{"question": "Hey,  I'm just wondering if ray works will to parallelize a recursive function in python?  Or is it best at accelerating non-recursive functions?  Probably depends on the compute time in a single recursive stack and the size of the recursion stack I imagine.  I'm thinking ray will not work with recursive function that well in general."}
{"question": "Can I please get a sanity check, that I am installing Ray correctly? I was on v1 for a long time and I decieded to update, but no matter what I do I just can't get the PPO fine-tuned examples to run.\n\n1. Brand new conda env `conda create -n ray5 python=3.10`\n2. Just to be sure, `pip install ray[all]`\n3. Install DL reqs `pip install -r ray/python/requirements/ml/requirements_dl.txt` \n4. Install RLLib reqs `pip install -r ray/python/requirements/ml/requirements_rllib.txt`\n5. Try the typical Cartpole PPO training `rllib train --algo PPO --env CartPole-v1`\n    a. This failed with `ray.exceptions.RaySystemError: System error: RandomNumberGenerator._generator_ctor() takes from 0 to 1 positional arguments but 2 were given`\n6. Ok, no problem. Let's try specific fined tuned file `rllib train file ray/rllib/tuned_examples/ppo/cartpole-ppo.yaml`\n    a. This crashes on `ValueError: stop criteria only supported for python files.`  because the `stop` input is `{}` and it is epecting `None`  <https://github.com/ray-project/ray/blob/ed3f3c08225c7049a96a290b586c67c37e2d0bc0/rllib/train.py#L59|here>. \n    b. We can fix that with a hacky solution that changes `{}` to `None` . Yuck\n    c. And we are back at the generatior error.\nIt feels like 2.2.0 contains a ton of bugs, but noone else is having any issues, so I assume it's me. Does `rllib train file ray/rllib/tuned_examples/ppo/cartpole-ppo.yaml`  for you guys on 2.2.0?"}
{"question": "However, the offical release version is bigger than mine.For example, python3.8 + linux + ray2.0.0 is `Downloading ray-2.0.0-cp38-cp38-manylinux2014_x86_64.whl (59.2 MB)`\nIs this normal ?\n1. when i install the ray wheel which i built by the command `ray start --head --dashboard-host 0.0.0.0 --dashboard-port 8265 --block`,  Strange things happened. After about 3 minutes, the process exited. Here is the stdout:\n```...\nSome Ray subprocesses exited unexpectedly:\n dashboard [exit code=255]\n Remaining processes will be killed.```\n1. I found these log in `session_latest/logs/dashboard.log`\n```...\n2022-12-20 21:59:21,240 INFO http_server_head.py:142 -- Registered 51 routes.\n2022-12-20 21:59:21,242 INFO datacenter.py:7  -- Purge data.\n2022-12-20 21:59:21,242 INFO event_utils.py:123 -- Monitor events logs modified after 1671542961.0622056 on /tmp/ray/session_2022-12-20_21- 9-19_363980_201878/logs/events, the source types are ['GCS'].\n2022-12-20 21:59:21,244 INFO usage_stats_head.py:102 -- Usage reporting is enabled.\n2022-12-20 21:59:21,244 INFO actor_head.py:111 -- Getting all actor info from GCS.\n2022-12-20 21:59:21,246 INFO actor_head.py:137 -- Received 0 actor info from GCS.\n2022-12-20 21:59:32,244 ERROR head.py:127 -- Failed to check gcs health, client timed out.\n2022-12-20 21:59:48,245 ERROR head.py:127 -- Failed to check gcs health, client timed out.\n2022-12-20 22:00:04,248 ERROR head.py:127 -- Failed to check gcs health, client timed out.\n2022-12-20 22:00:20,252 ERROR head.py:127 -- Failed to check gcs health, client timed out.\n2022-12-20 22:00:36,255 ERROR head.py:127 -- Failed to check gcs health, client timed out.\n2022-12-20 22:00:52,257 ERROR head.py:127 -- Failed to check gcs health, client timed out.\n2022-12-20 22:01:08,260 ERROR head.py:127 -- Failed to check gcs health, client timed out.\n2022-12-2  22:01:24,263 ERROR head.py:127 -- Failed to check gcs health, client timed out.\n2022-12-20 22:01:40,267 ERROR head.py:127 -- Failed to ch ck gcs health, client timed out.\n2022-12-20 22:01:56,270 ERROR head.py:127 -- Failed to check gcs health, client timed out.\n2022-12-20 2 :02:12,273 ERROR head.py:127 -- Failed to check gcs health, client timed out.\n2022-12-20 22:02:12,273 ERROR head.py:138 -- Dashboard exiting because it received too many GCS RPC errors count: 11, threshold is 10.```\n"}
{"question": "`session_latest/logs/dashboard_agent.log`\n```...\n2022-1 -20 21:59:23,084 INFO event_agent.py:46 -- Report events to 10.9.2.41:34684\n2022-12-20 21:59:23,084 INFO event_utils.py:123 -- Mon tor events logs modified after 1671542961.9415762 on /tmp/ray/session_2022-12-20_21-59-19_363980_201878/logs/events, the source t pes are ['COMMON', 'CORE_WORKER', 'RAYLET'].\n2022-12-20 22:02:13,087 ERROR reporter_agent.py:809 -- Error publishing node phy ical stats.\nTraceback (most recent call last):\n File \"/home2/hanwen.qiu/miniconda3/envs/ray_build/lib/python3.8/site-packages/r y/dashboard/modules/reporter/reporter_agent.py\", line 806, in _perform_iteration\n await publisher.publish_resource_us ge(self._key, jsonify_asdict(stats))\n File \"/home2/hanwen.qiu/miniconda3/envs/ray_build/lib/python3.8/site-packages/ray/_private/gcs_pubsub.py\", l ne 452, in publish_resource_usage\n await self._stub.GcsPublish(req)\n File \"/home2/hanwen.qiu/miniconda3/envs/ray_build/lib/python3.8/site-package /grpc/aio/_call.py\", line 290, in __await__\n raise _create_rpc_error(self._cython_call._initial_metadata,\ngrpc.aio._call.AioRpcErro : &lt;AioRpcError of RPC that terminated with:\n status = StatusCode.UNAVAILABLE\n details = \"failed to connect to all addresses\"\n debu _error_string = \"{\"created\":\"@1671544933.087241918\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/clien _channel/client_channel.cc\",\"file_line\":3134,\"referenced_errors\":[{\"created\":\"@1671544933.087241207\",\"description\":\"failed t  connect to all addresses\",\"file\":\"src/core/lib/transport/error_utils.cc\",\"file_line\":163,\"grpc_status\":14}]}\"\n&gt;\n2022-12-20 22:0 :13,602 ERROR agent.py:217 -- Raylet is terminated: ip=10.9.2.41, id=5fa7195f6fdcb2f6f9f378604ecc5253871ddde0dffc54186ee82d09. Termin tion is unexpected. Possible reasons i clude: (1) SIG ILL by the user or system OOM killer, (2) Invalid memory access from Raylet causing SIGSEGV or SIGBUS, (3) Other termination signals. Last 20 lines of the Raylet logs:\n [state-dump]  NodeManagerService.grpc_server.RequestResourceReport - 1187 total (0 active), CPU time: mean = 52.014 us, total = 61.740 ms\n [state-dump]  NodeManagerService.grpc_server.UpdateResourceUsage - 1186 total (0 active), CPU time: mean = 43.173 us, total = 51.204 ms\n [state-dump]  RayletWorkerPool deadline_timer.kill_idle_workers - 600 total (1 active), CPU time: mean = 4.524 us, total = 2.715 ms\n [state-dump]  NodeManager.deadline_tim r.flush_free_objects - 120 total (1 active), CPU time: mean = 3.878 us, total = 465.349 us\n [state-dump]  NodeManagerService.grpc_server.G tResourceLoad - 120 total (0 active), CPU time: mean = 38.647 us, total = 4.638 ms\n [state-dump]  NodeManagerService.grpc_server.GetNo eStats - 119 total (0 active), CPU time: mean = 475.065 us, total = 56.533 ms\n [state-dump]  NodeManager.deadline_timer.record_metri s - 24 total (1 active), CPU time: mean = 155.176 us, total = 3.724 ms\n [state-dump]  NodeManager.deadline_timer.debug_state_dump -  2 total (1 active), CPU time: mean = 319.161 us, total = 3.830 ms\n [state-dump]  PeriodicalRunner.RunFnPeriodically - 7 total (0 ac ive), CPU time: mean = 271.586 us, total = 1.901 ms\n [state-dump]  InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBat h - 2 total (0 active), CPU time: mean = 75.287 us, total = 150.575 us\n [state-dump]  NodeManager.deadline_timer.print_event_loop stats - 2 total (1 active, 1 running), CPU time: mean = 265.413 us, total = 530.827 us\n [state-dump]  NodeInfoGcsServic .grpc_client.GetInternalConfig - 1 total (0 active), CPU time: mean = 50.795 ms, total = 50.795 ms\n [state-dump]  <http://AgentManagerService.grpc_server.Re|AgentManagerService.grpc_server.Re> isterAgent - 1 total (0 active), CPU time: mean = 225.880 us, total = 225.880 us\n [state-dump]  NodeInfoGcsService.grpc_client.RegisterNode - 1 tot l (0 active), CPU time: mean = 381.775 us, total = 381.775 us\n [state-dump]  JobInfoGcsService.grpc_client.GetAllJobInfo - 1 total (0 active), CPU time: mean = 4.573 us, total = 4.573 us\n [state-dump]  NodeInfoGcsService.grpc_client.GetAllNodeInfo - 1 total (0 activ ), CPU time: mean = 39.528 us, total = 39.528 us\n [state-dump]  InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (  active), CPU time: mean = 0.000 s, total = 0.000 s\n [state-dump] DebugString() time ms: 0\n [state-dump] \n [state-dump] \n 2022-12-20 22:03:13,680 ERROR utils.py:224 -- Failed to publish error job_id: \"\\377\\377\\377\\377\"\ntype: \"raylet_died\"\nerror_message  \"Raylet is terminated: ip=10.9.2.41, id=5fa7195f6fdcb2f6f9f378604ecc5253871ddde0dffc54186ee82d09. Termination is unexpected. Possible  easons include: (1) SIGKILL by the user or system OOM k ller, (2) Invalid memory access from Raylet causing SIGSEGV or SIGBUS, (3) Other termination signals. Last 20 lines of the Raylet logs:\\n [state-dump] \\tNodeManagerService.grpc_server.RequestResourceReport - 1187 total (0  ctive), CPU time: mean = 52.014 us, total = 61.740 ms\\n [state-dump] \\tNodeManagerService.grpc_server.UpdateResourceUsage - 1186 total (0 active), CPU time: mean = 43.173 us, total = 51.204 ms\\n [st te-dump] \\tRayletWorkerPool.deadline_timer.kill_idle_workers - 600 total (1 active), CPU time: mean = 4.524 us, total = 2.715 ms\\n [state-dump] \\tNodeManager deadline_timer.flush_free_objects - 120 total (1 active), CPU time: mean = 3.878 us, total = 465.349 us\\n [state-dump] \\tNodeManagerService.grpc_server.GetResourceLoad - 120 total (0 active), CPU time: me n = 38.647 us, total = 4.63  ms\\n [state-dump] \\ NodeManagerService.grpc_server.GetNodeStats - 119 total (0 active), CPU time: mean = 475.065 us, total = 56.533 ms\\n [state-dump] \\tNodeManager.deadline_timer.record_metrics - 24 total (1 active), CPU time: mean = 155.176 us, total = 3.724 ms\\n [state-dump] \\tNodeManager.deadline_timer.debug_state_dump - 12 total (1 active), CPU time: mean = 319.161 us, total = 3.830 ms\\n [state-dump] \\tPeriodicalRunner.RunFnPeriodically - 7 total (0 active), CPU time: mean = 271.586 us, total =  .901 ms\\n [state-dump] \\tInternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 2 total (0 active), CPU time: mean = 75.287 us,  otal = 150.575 us\\n [state-dump] \\tNodeManager.deadline_timer.print_event_loop_stats - 2 total (1 active, 1 running), CPU time: mean = 265.413 us, total = 530.827 us\\n [state-dump] \\tNodeInfoGcsService.grpc_client.GetInternalConfig - 1 total (0 active), CPU time: mea  = 50.795 ms, total = 50.795 ms\\n [state-dump] \\tAgentManagerService.grpc_server.RegisterAgent - 1 total (0 active), CPU time: mean   225.880 us, total = 225.880 us\\n [state-dump] \\tNodeInfoGcsService.grpc_client.RegisterNode - 1 total (0 active), CPU time: mean = 381.775 us, total = 381.775 us\\n [state-dump] \\tJobInfoGcsService.grpc_client.GetAllJobInfo - 1 total (0 active), CPU time: mea  = 4.573 us, total = 4.573 us\\n [state-dump] \\tNodeInfoGcsService.grpc_client.GetAllNodeInfo - 1 total (0 active), CPU time: mean = 39.528 us, total = 39.528 us\\n [state-dump] \\tInternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 act ve), CPU time: mean = 0.000 s, total = 0.000 s\\n [state-dump] DebugString() time ms: 0\\n [state-dump] \\n [state-dump] \\n\"\ntimestamp: 1671544933.6033 57\nTraceback (most recent call last):\n File \"/home2/hanwen.qiu/miniconda3/envs/ray_build/lib/python3.8/site-packages/ray/_private/utils.py\", line 2 2, in publish_error_to_driver\n gcs_publisher.publish_error(job_id.hex().encode(), error_data)\n File \"/home2/hanwen.qiu/miniconda3/env /ray_build/lib/python3.8/site-packages/ray/_private/gcs_pubsub.py\", line 169, in publish_error\n self._gcs_publish(req)\n File \"/home2 hanwen.qiu/miniconda3/envs/ray_build/lib/python3.8/site-packages/ray/_private/gcs_pubsub.py\", line 191, in _gcs_publish\n raise Tim outError(f\"Failed to publish after retries: {req}\")\nTimeoutError: Failed to publish after retries: pub_messages {\n channel_typ : RAY_ERROR_INFO_CHANNEL\n key_id: \"ffffffff\"\n error_info_message {\n job_id: \"\\377\\377\\377\\377\"\n type: \"raylet_died\"\n error_message  \"Raylet is terminated: ip=10.9.2.41, id=5fa7195f6fdcb2f6f9f378604ecc5253871ddde0dffc54186ee82d09. Termination is unexpected. Possible  easons include: (1) SIGKILL by the user or system OOM k ller, (2) Invalid memory access from Raylet causing SIGSEGV or SIGBUS, (3) Other termination signals. Last 20 lines of the Raylet logs:\\n [state-dump] \\tNodeManagerService.grpc_server.RequestResourceReport - 1187 total (0 active), CPU time: mean = 52.014 us, total = 61.740 ms\\n [state-dump] \\tNodeManagerService.grpc_server.UpdateResourceUsage - 1186 total (0 active), CPU time: mean = 43.173 us, total = 51.204 ms\\n [state-dump] \\tRayletWorkerPool.deadline_timer.kill_idle_workers - 600 total (1 active), CPU time: mean = 4.524 us, total = 2.715 ms\\n [state-dump] \\tNodeManager.deadline_timer.flush_free_objects - 120 total (1 active), CPU time: mean = 3.878 us, total = 465.349 us\\n [state-dump] \\tNodeManagerService.grpc_server.GetResourceLoad - 120 total (0 active), CPU time: mean = 38.647 us, total = 4.638 ms\\n [state-dump] \\tNodeManagerService.grpc_server.GetNodeStats - 119 total (0 active), CPU time: mean = 475.065 us, total = 56.533 ms\\n [state-dump] \\tNodeManager.deadline_timer.record_metrics - 24 total (1 active), CPU time: mean = 155.176 us, total = 3.724 ms\\n [state-dump] \\tNodeManager.deadline_timer.debug_state_dump - 12 total (1 active), CPU time: mean = 319.161 us, total = 3.830 ms\\n [state-dump] \\tPeriodicalRunner.RunFnPeriodically - 7 total (0 active), CPU time: mean = 271.586 us, total = 1.901 ms\\n [state-dump] \\tInternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 2 total (0 active), CPU time: mean = 75.287 us, total = 150.575 us\\n [state-dump] \\tNodeManager.deadline_timer.print_event_loop_stats - 2 total (1 active, 1 running), CPU time: mean = 265.413 us, total = 530.827 us\\n [state-dump] \\tNodeInfoGcsService.grpc_client.GetInternalConfig - 1 total (0 active), CPU time: mean = 50.795 ms, total = 50.795 ms\\n [state-dump] \\tAgentManagerService.grpc_server.RegisterAgent - 1 total (0 active), CPU time: mean = 225.880 us, total = 225.880 us\\n [state-dump] \\tNodeInfoGcsService.grpc_client.RegisterNode - 1 total (0 active), CPU time: mean = 381.775 us, total = 381.775 us\\n [state-dump] \\tJobInfoGcsService.grpc_client.GetAllJobInfo - 1 total (0 active), CPU time: mean = 4.573 us, total = 4.573 us\\n [state-dump] \\tNodeInfoGcsService.grpc_client.GetAllNodeInfo - 1 total (0 active), CPU time: mean = 39.528 us, total = 39.528 us\\n [state-dump] \\tInternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s\\n [state-dump] DebugString() time ms: 0\\n [state-dump] \\n [state-dump] \\n\"\n timestamp: 1671544933.6033757\n }\n}```\nIt looks like the wheel i built caused something wrong within GCS ? To make sure these are not related to the source code which i had changed, I follow the build wheel workflow to rebuild one directly use the original branch `releases/2.0.0`. Same error occured as mentioned above.\nIs there something wrong with me? I can only find how to install the build process after modifying the source code in the official document, but I can't find the build and release processes. Maybe something wrong in my build wheel workflow. If you have any idea, please let me know! Thank you!\n Have a nice day!"}
{"question": "Hi all, I get the following error when running ray inside a docker container:\n```<http://core_worker.cc:179|core_worker.cc:179>: Failed to register worker 01000000ffffffffffffffffffffffffffffffffffffffffffffffff to Raylet. IOError: [RayletClient] Unable to register worker with raylet. No such file or directory```\nIt works fine when running locally. Any ideas what\u2019s the issue?"}
{"question": "Hi, I just started playing with RLlib and I wanted to test offline DQN training on a CartPole. Thus, I generated the data as in the tutorial:\n```rllib train --run=PG --env=CartPole-v1 --config='{\"output\": \"/tmp/cartpole-out\", \"output_max_file_size\": 5000000}' --stop='{\"timesteps_total\": 100000}'```\nAnd then run offline the training via\n```rllib train --run=DQN --env=CartPole-v1 --config='{\"input\": \"/tmp/cartpole-out\",\"explore\": false}' --stop='{\"timesteps_total\": 1000000}'```\nHowever, now I want to reproduce offline training using Python API and I get a little confused about `timesteps_total` . I have written the following code:\n```if __name__ == '__main__':\n    config = (\n        DQNConfig()\n        .environment(env=\"CartPole-v1\")\n        .framework(\"torch\")\n        .offline_data(input_=\"/tmp/cartpole-out\")\n        .exploration(explore=False)\n    )\n\n    algo = config.build()\n    for _ in tqdm(range(100)):\n        algo.train()```\nbut I am not sure how `timesteps_total`  is related to the training loop with the above snippet. I looked inside `AlgorithmConfig` class, but I found that `self.timesteps_per_iteration = DEPRECATED_VALUE` . Thus, the question:\n*How to set up `timesteps_total` in the config in Python API?*\nDisclaimer: I asked ChatGPT about that, but he confidently gave me the wrong answer with a detailed explanation :P"}
{"question": "hiya, is there a good hack for reaching the head node for ray client with an ssh tunnel? I've gotten that ip mismatch error"}
{"question": "Hello team.\nWhat would be process of having a health-check point along with a Dag driver in ray-serve?\nBasically I am using dag driver in the following way\n\n```with InputNode() as image_url:\n    input_file = read_file.bind(image_url)\n    ocr_response = azure_deployment.forward.bind(input_file)\n    detector_response = detectron_deployment.forward.bind(input_file)\n    dag = aggregate.bind(ocr_response,detector_response)```\n```serve_dag = DAGDriver.options(num_replicas=1, route_prefix=\"/process\")  \\\n            .bind(dag)```\nI finally run serve using the following command\n```serve run main:serve_dag --host=0.0.0.0 --port=8000```\nBut along with the serving dag I also need a `/health` endpoint which accepts a get call and returns a `\"server is healthy message\"`\n\nWhat would be the best way to achieve this?\nDo I have to create a Fast API endpoint or can I use serve deployments directly?"}
{"question": "Hi all,  any ideas on how to build Ray wheel? I can't find relevant content in the document~:grinning: The wheel, i built by  following the markdown in repo `python/README-building-wheels.md` , reported errors when running."}
{"question": "hi, i've been trying to use kuberay just because we use kubernetees at my company, but i've used the vanilla ray clusters setup previously and i really liked it. i'm just wondering -- what is the actual advantage of using kuberay? i know you get the benefits of kubeternetees... but is there really a tangible difference? it seems like there's a lot more complexity to using kuberay. should I stick with normal ray clusters?"}
{"question": "Hi All, is there an equivalent of the multiprocessing <https://docs.python.org/3/library/multiprocessing.shared_memory.html#multiprocessing.managers.SharedMemoryManager|SharedMemoryManager> in Ray?"}
{"question": "I'm facing a weird problem. I'm running training on a multi-agent reinforcement learning experiment. The training works, but as I run more and more training iteration, they gradually become slower. The first iterations take around 15 seconds each. After 50 iterations, each iteration takes a full minute. This experiment I'm running now just completed its 64th iteration, which took around 6 minutes.\nAny idea what is causing this? What tools does RLlib give me to troubleshoot such problems?"}
{"question": "Hello everyone. I have been using Ray mostly for parallelizing linear algebra code on small clusters (a handful of VMs).\nI am now exploring how to make ray available to the rest of my research team, and am wondering how other folks have approached this.\n\nCurrently we manage several other \u2018Job\u2019 based cluster compute services like AWS Batch, Slurm, K8s allowing researchers to schedule work, with various amounts of support for Fair scheduling, priority, preemption, etc..\n\nI am wondering what patterns do people use to make Ray available to a team of users (multi-tenant) with considerations for limiting the resources each user has, scheduling with priority (super long training runs use resources when they are available, but short term jobs can take resources), one persons job doesn\u2019t balloon and halt the cluster, etc..?\n\nSeems like there are quite a few layers at which one could try to implement these things like:\n\u2022 A single ray cluster where users interact via client, using mechanisms like placement groups / resources\n\u2022 A single ray cluster where users interact via Jobs API\n\u2022 A ray cluster where users interact with a 3rd party scheduler that then submits their work to the ray cluster\n\u2022 KubeRay Jobs, where resource gating happens at the K8s level and each user has their own cluster spun up\nWhich patterns are people generally using for managing shared access to Ray resources for a team?"}
{"question": "Hello every one,\nYou guys have any idea why this happen when I use tuner with rllib?\n```Memory usage on this node: 15.9/16.0 GiB : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\nPopulationBasedTraining: 0 checkpoints, 0 perturbs\nResources requested: 12.0/16 CPUs, 0/0 GPUs, 0.0/5.01 GiB heap, 0.0/2.0 GiB objects```\nI would really appreciate any comment about. This is my script:\n\n```def tune(self):\n        \"\"\" tune the algorithms parameters\"\"\"\n        date_time = time.strftime(\"%Y%m%d_%H%M%S\")\n        start_experiment_time = datetime.datetime.now()\n        ray.init(object_store_memory=80000000)\n\n        self.analysis = ray.tune.run(\n            self.rl_algo_name,\n            name=\"{}_{}_version_{}_{}\".format(self.environment_name, self.rl_algo_name, str(self.version), date_time),\n            scheduler=self.tuning_algo_name,\n            verbose=self.verbose,\n            num_samples=self.tuning_num_samples,\n            stop=self.stop_criteria,\n            config=self.env_configuration,\n            local_dir=self.tuner_local_results_dir,\n            fail_fast=True\n        )```\nIm using ray 2.2.0"}
{"question": "What is the best way to apply f(key, group) on all groups of a groupby dataset (where key is the groupby key for each group).\nI looked at aggregate and map_group methods of ray GroupedDataset but I am not sure it fits my needs...\nI also try to do it using map batches on a classic Ray Dataset created from a groupby panda dataframe. Would it be more appropriate ?\nThank you !"}
{"question": "Happy holidays guys!\nI have a question on ray cluster launcher - if I create cluster w/ docker, but I wanna pull docker image from my custom private docker hub repo, what's the best practice to do that?\nThe problem I'm faced w/ now is - I need to do `docker login` to pull the image, and to do it w/o prompt, I'm doing `cat my_password.txt | docker login --username dzcap --password-stdin` , but this `my_password.txt` must be synced to the head node first.\nif I do everything in docker, I think all the file sync callbacks in `example-full.yaml` will sync to *inside* docker container, *not outside*.\nHow can I sync to the head node *outside* the container?? Can I do a rsync in `initialization_commands`? If so, how?\nThanks!!"}
{"question": "Have you guys run into this error while launching ray cluster using custom docker image?\n```ubuntu@ray-head-RAY10:~$ rsync -e 'docker exec -i' -avz /tmp/ray_tmp_mount/~/ray_bootstrap_config.yaml ray_container:/root/ray_bootstrap_config.yaml\nprotocol version mismatch -- is your shell clean?\n(see the rsync man page for an explanation)\nrsync error: protocol incompatibility (code 2) at compat.c(178) [sender=3.1.2]```\n"}
{"question": "Happy holidays\uff01\nI want to resume part of an experiment in RLlib. For example, I use `grid_search` in the experiment to try different hyper-parameters. At the end, I find hyper-parameter *1* and *3* has the best performance, so I want to keep training it with new by setting new `num_iters` . I only see using `tune.run` to restore a single checkpoint with single set of hyper-parameters. Is there any way I can use  `Tuner().fit()` to restore more than one run and change the config like `num_iters` ?"}
{"question": "Hello, Our ray head consumes a lot of memory. It increases day by day.\nI found that  a lot of tasks/actors finished but  kept the memory in the cluster\nHow can I clean  them?"}
{"question": "How to run ray tune from ray remote?\nI am trying to train multiple in parallel using ray.tune.\nPlease let me know if there is any sample code?"}
{"question": "hi - is it possible to run `initialization_commands` only on worker nodes.\nI'm running everything in docker. although there's a `worker_setup_commands` option, it will be running inside docker I believe.\nMy situation is, I'm running jobs on Azure cloud, and there are certain instance volume (e.g. nvme) that I need to mount on the host first and then mount (-v) to docker container.\nFor now, I can only see `initialization_commands`  being the only entrypoint, but there's no distinction b/w head and worker.\nIs there any way I could achieve the above?"}
{"question": "Hi, how can I get head node id in a remote task?"}
{"question": "Hey, any progress on arm64 wheels for Linux? AWS have very appealing high memory arm64 machines."}
{"question": "Any tips for troubleshooting a Dataset.map_batches that seems to kill my head node and numerous worker nodes? It's a big dataset though, 200 million records and about 150 columns"}
{"question": "I am trying to use Ray cluster to spin up an AWS cluster. I see this error:\n```grpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:172.31.17.87:6379: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:172.31.17.87:6379: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2022-12-30T01:37:35.269738983+00:00\"}\"\n&gt;\nShared connection to 35.88.123.228 closed.\n  New status: update-failed\n  !!!\n  SSH command failed.\n  !!!\n  \n  Failed to setup head node.```\nhave other people experienced this?"}
{"question": "Hi, how can I change the logging path? I have read the related docs but I'm still confused. Can anyone help me?:heart:  docs link  <https://docs.ray.io/en/latest/ray-observability/ray-logging.html#logging-directory-structure>"}
{"question": "Is\n``` Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\".```\nresolved at this point?"}
{"question": "I am running into a very odd Ray error which might be related to the question above. I have setup a Ray cluster on AWS (I checked and raylet is running on each machine) Now I try to make a remote actor\n```&gt;&gt;&gt; @ray.remote\n... class A:\n...     def __init__(self, a):\n...             self.a = a\n...     def ping(self):\n...             return self.a```\nand launch it on a specific worker instance:\n```&gt;&gt;&gt; a = A.options(num_cpus = 0.001, max_concurrency = 2, resources = {\"node:172.31.11.11:0.001\"}).remote(2)```\nThis line works. However the following hangs forever:\n```ray.get(a.ping.remote())```\nNote this only happens when I am trying to launch it on a worker instance. If I specify the private IP of the master instance and launch it there it works.\nHas anyone else seen this problem before?\nI confirmed this has something to do with firewall settings. What ports do I need to open for this to work? I have already opened 10001, 6379 and 6380."}
{"question": "Hello everyone,\n\nI am using RLLib for one of my projects. I've faced the same issue written in this GitHub issue (<https://github.com/ray-project/ray/issues/9071>). This issue is closed as solved but I got the same error. Could anyone help me?\n\nThank you in advance."}
{"question": "Hi all.\nI have a modin dataframe and perform a group_by() and apply() on it.\nThe apply method returns a single variable.\nIt seems that the result automaticaly fills the original modin dataframe. I would like to have a list instead, and then convert it to a ray dataset.\nAny solution that is ray compliant ?"}
{"question": "Hello everyone any ideas on to test cli application using ray. I am now using a pytest fixture to bring up a mock up cluster. However this technique create lots of problems when trying to run in parallel. Any suggestions ?"}
{"question": "Are data attributes of actors implicitly added to the object store or do they just get saved in the python process memory?"}
{"question": "Is it possible to add resources to a running node rather than having to declare them inside `ray.init()` ?"}
{"question": "Also, is there something as simple as `ray.get_head_address()` command ?"}
{"question": "Happy new year! I\u2019m giving a presentation and would like to use screenshots from Anyscale\u2019s/Ray\u2019s documentation. How should I get permission from Anyscale to do this?"}
{"question": "Is there any way to list all running actors via the Python API?"}
{"question": "Can someone un-flag my post? It\u2019s my first post on the forum and looks like a bot auto-flagged it to be reviewed: <https://discuss.ray.io/t/ray-tasks-sometimes-hang-in-pending-node-assignment/8840/1>"}
{"question": "Good morning everyone, and happy new year!\nJust a few questions regarding Ray's behaviour:\n1. For a given set of resource requirements, does Ray guarantee the order of execution?\n2. Is there some way define blocking points in Ray for it to wait until the entire task queue is empty before it proceeds? e.g. something like ``ray.wait_util_queue_is_empty()` ?\n3. Is there a way to prevent Ray from setting `CUDA_VISIBLE_DEVICES`?"}
{"question": "is it anything to worry about? is my group map result to be trusted?"}
{"question": "Hi, may I know what the ray GCS stands for?  google cloud storage?"}
{"question": "Hi. I'm wondering why my BC training shows `NaN` reward and the agent isn't learning at all, since I've set `evaluation_config[\"input\"] = \"sampler\"` .\nI use `SampleBatchBuilder()` to generate offline data for a multi-agent environment (This might be the problem, but `MultiAgentSampleBatchBuilder`  keeps showing `'list' object has no attribute 'postprocess_trajectory'` ).\nIt also warns me `SampleBatchBuilder` has been deprecated. Use `a child class of `SampleCollector`` instead.`  But I don't find any example. Can anyone give me an example showing how to do it?\nThanks a lot!"}
{"question": "Hello all :slightly_smiling_face:\nDoes someone know how to deal with Socket Timeout issue when using RayStrategy (ray lightning) and calling trainer.fit() ?:\n    `pg = ProcessGroupGloo(prefix_store, rank, world_size, timeout=timeout)`\n`RuntimeError: Socket Timeout`\nI am facing issue when using a local ray instance... No specific change, maybe just a bigger dataset than usually.\nThank you"}
{"question": "Are people interested in using Ray on top of aws lambda functions? Is there work being done here?"}
{"question": "I was using a Ray Actor to load a pickle object and got this error: `AttributeError: Can't get attribute 'Node' on &lt;module '__main__' from '/opt/anaconda3/lib/python3.8/site-packages/ray/workers/default_worker.py'&gt;` . How to solve this problem?"}
{"question": "Hi All! I was trying to use Ray\u2019s debugger (tried using Ray2.1 and 2.2) for post-mortem debugging following these <https://docs.ray.io/en/latest/ray-observability/ray-debugging.html|instructions>, but it\u2019s not performing as I would expect based on those instructions. Here\u2019s a minimum reproducible example:\n```import ray\n\n@ray.remote\ndef f(x):\n    result = x / 0\n    return result\n\nfutures = [f.remote(i) for i in range(2)]\nprint(ray.get(futures))```\nSaved that in `debugging_example.py` and from the terminal\n`RAY_PDB=1 python debugging_example.py`\nI was expecting it to keep the Ray instance running, but it exits with\n```...\n(f pid=64470) ZeroDivisionError: division by zero\n(f pid=64470) \n(f pid=64470) During handling of the above exception, another exception occurred:\n(f pid=64470) \n(f pid=64470) Traceback (most recent call last):\n(f pid=64470)   File \"python/ray/_raylet.pyx\", line 1135, in ray._raylet.task_execution_handler\n(f pid=64470)   File \"python/ray/_raylet.pyx\", line 1045, in ray._raylet.execute_task_with_cancellation_handler\n(f pid=64470)   File \"python/ray/_raylet.pyx\", line 782, in ray._raylet.execute_task\n(f pid=64470)   File \"python/ray/_raylet.pyx\", line 945, in ray._raylet.execute_task\n(f pid=64470)   File \"python/ray/_raylet.pyx\", line 568, in ray._raylet.store_task_errors\n(f pid=64470) AttributeError: module 'ray.util.rpdb' has no attribute 'post_mortem'\n(f pid=64470) \n(f pid=64470) During handling of the above exception, another exception occurred:\n(f pid=64470) \n(f pid=64470) Traceback (most recent call last):\n(f pid=64470)   File \"python/ray/_raylet.pyx\", line 1176, in ray._raylet.task_execution_handler\n(f pid=64470) SystemExit```\nShould I create an issue for this, or is this expected behavior?"}
{"question": "has anyone run into the situation where it took forever to scale up a cluster?\nI've set `upscaling_speed: 100.0` and it still takes very very long.\nSome background - I'm creating a cluster on azure, and each node has 8cpus. I wanted to run one job per node, so I set the cpu requirement per job to 8.\nEven that, it takes 20m to get to 5 nodes only, and it starts failing...\n\nAny insight?"}
{"question": "Following previous question - is there any limit on how many workers ray can launch at once?\nFrom the screenshot below, Ray seems to be only running one at a time, and for whatever reason, it only launched 3 instead of say 10.\nIs there any param that controls this behavior?"}
{"question": "[Question about configuring Ray resources]\n\nIs it possible to update Ray cluster resources after Ray start/init?\n\nContext: I am trying to run multiple independent jobs on the same Ray cluster. I would like to limit the number of cpus used by each job. One way to achieve that is for each job to define &amp; use a job specific \"resource\". But this would require me to set the cluster resources after the cluster has started.\n\n(If there are ways to achieve the same goal, I'd be happy to learn!)"}
{"question": "Hi,\nI will appreciate if someone can help me, its my first day with rl lib and it took me a while to have everything ready.. or so I thought.\nI'm using a custom environment based on gym, and I wish to use impala algorithm for rl to solve a problem.\nI have configured everything inside an EnvWrapper according to the examples and registered it with ray tune.\nSo far so good, however the mess starts when I run ImpalaConfig(), I'm getting an error that actor died because of an error raised in its creation task.\nIt detailed that a certain module was not found, its a module that I imported from a personal .py file and created an instance of it inside the gym environment.\nI couldn't find an example of such complex scenario, what do I do? is there a way to provide the instance of the missing module instead of initializing it inside the environment ?\nThanks for any help!"}
{"question": "Is there any official guidance or examples on how to use Actors in complex codebase? It\u2019s unclear to me when and where imports from other source files are supposed to be placed in order to avoid pickling overheads at execution time"}
{"question": "Hi, does Ray eventually clean/consume the objects spilled to disk? Disk seems to be getting full very quickly."}
{"question": "Is it possible that Ray 'forgets' to delete those objects from disk? Is there a way to tell ray something like `ray.check_and_clean_spillage()`"}
{"question": "Question: What's the difference between `tune.grid_search`  and `tune.choice`?"}
{"question": "Hi,\nI am wondering if we can override the `compute_actions` method of a policy (in this case `pg_torch_policy`) with a custom `compute_actions` method? I would like to add some heuristics to the action selection policy for a custom environment. Really appreciate any pointers on this. Thank you!"}
{"question": "Hi, I found a blog introducing the Ray automatic cluster setup tool, but the link to the docs was missing. Does this tool still exist?  blog link: <https://medium.com/distributed-computing-with-ray/how-to-scale-python-multiprocessing-to-a-cluster-with-one-line-of-code-d19f242f60ff>"}
{"question": "Any tips for unit tests Ray actors? Specifically, let\u2019s say an actor grabs connections to 3rd party services like Redis or Postgres. Normally, one simple adds `autouse` fixtures to the testing code that patch Redis/Postgres calls with `fakeredis` and `pytest-postgresql` . Then, any source code that tries to grab a connection gets a mocked object and this enables simple unit testing. However, the actors don\u2019t appear to take the mocks, they appear to go around them. How do I make mocks apply to actor methods?\n\nedit - can confirm that mocks on actors don\u2019t work as one would hope. Details in :thread:"}
{"question": "Would really like to be able to configure our cluster using terraform, as that is the tool of choice for infrastructure at our shop.\nI struggle to find any descriptions on how to proceed (Right now we are running ray on a single ECS task with a bunch of cores).\nIs anyone running ray on ECS at all (I have seen a story about fargate autoscaling in the backlog)?"}
{"question": "Hi, I wonder to know is possible to assign multi-nodes to one actor/task at the same time:heart:?"}
{"question": "I have an actor with a 6MB Pandas dataframe inside the actor. If a Ray client calls the actor to grab the dataframe directly, it takes 1.6 seconds to response. If the actor `pickle.dumps(df)` the dataframe and returns the string and the client `pickle.loads(df)` the dataframe, the entire sequence takes ~0.4 seconds to complete. Why is it 4x faster to pickle/unpickle the dataframe compared to allowing Ray to do serialization under the hood? Note that zero-copy reads are not possible here, since this is a client of the Actor (outside the Ray cluster) that is making the call"}
{"question": "I have some very basic Ray questions to address before diving into doing some deployments\n\u2022 I see that `ray up` can launch a cluster, but this can also be achieved by `kubectl` (ie: `kubectl apply -f <https://raw.githubusercontent.com/ray-project/ray/releases/2.0.0/doc/source/cluster/kubernetes/configs/xgboost-benchmark.yaml>` as in this example: <https://docs.ray.io/en/latest/cluster/kubernetes/examples/ml-example.html>) \n    \u25e6 When do we use kubectl and kubernetes specs yamls for deployment versus using `ray up`?\n\u2022 I\u2019ve noticed that some organizations (such as Shopify?) have gone through the trouble of creating microservices for creating and destroying clusters, but I also noticed that `ray submit` can take a cluster yaml as an argument.\n    \u25e6 Can we create ephemeral ray clusters for single workloads?\n    \u25e6 If this feature exists, was it recently added/only in dev versions?\n"}
{"question": "Hi, in the <https://docs.ray.io/en/latest/ray-core/actors/concurrency_group_api.html|docs>, it is stated that \"Warning: Concurrency groups are only supported for asyncio actors, not threaded actors.\".\nAccording to our tests, a simple actor with multiple non-async methods still applies the concurrency_groups. Is it expected?\n```import time\nimport ray\n\n@ray.remote(concurrency_groups={\"io\": 1, \"compute\": 1})\nclass WorkspaceConsumer:\n    def __init__(self):\n        pass\n\n    @ray.method(concurrency_group=\"compute\")\n    def start(self):\n        while True:\n            time.sleep(1)\n\n    @ray.method(concurrency_group=\"io\")\n    def apply_processor(self):\n        pass\n\n    @ray.method(concurrency_group=\"io\")\n    def remove_processor(self):\n        pass```"}
{"question": "Hi all,\n\nIs it possible to use poetry projects in environment specifications?  Is this something anyone does?"}
{"question": "Dumb question - does setting `num_cpus=N` on an actor actually limit the CPU availability of the actor or is it only for Ray\u2019s bookkeeping purposes?"}
{"question": "How can I construct a `ClientActorHandle` from a `ray.actor.ActorHandle`? I need to write a test that works over a `ClientActorHandle`, but we only use a local cluster while testing so `ray.get_actor(..)` returns a regular `ray.actor.ActorHandle`. I tried constructing an `ActorRef` using `actor._ray_actor_id.binary()`  and feeing that to a `ClientActorHandle` constructor but it didn\u2019t work."}
{"question": "Are you guys also finding PPO not scaling with resources? I have tried everything I could think of, but no matter the config, I am unable to utilize my CPU (16 threads) above 15% and GPU above 17%.\nIf the training was bottlenecked by the rollout workers, I would expect the CPU utilisation to be much higher. On the other hand, if it was bottlenecked by learner, I would assume the GPU utilisation would be higher.\nAny tips or ideas what am I doing wrong? Anything would be appreciated.\n```from ray.rllib.algorithms.ppo import PPOConfig\nalgo = (\n    PPOConfig()\n    .rollouts(num_rollout_workers=15, create_env_on_local_worker=True, num_envs_per_worker=20)\n    .resources(num_gpus=1, num_cpus_per_worker=1)\n    .environment(env=\"CartPole-v1\")\n    .build()\n)\n\nfor i in range(1000):\n    result = algo.train()\n    print(\"perf:{}\".format(result[\"perf\"]))```\nTypical output from a training loop:\n```perf:{'cpu_util_percent': 14.05, 'ram_util_percent': 31.4, 'gpu_util_percent0': 0.1625, 'vram_util_percent0': 0.05078125}```"}
{"question": "quick question on ray.tune:\n\nIs there a way to specify the runtime root dir of trials? I was under the impression that at runtime, ray tune might move around trials so as a result, ppl can't assume everything will be under `logdir` at runtime.\n\nreason for asking this is - in remote cluster, depending on the types of instances, you might wanna run some jobs on a specific mnt to speed things up (e.g. if your trial has a lot of IO, you might wanna run it on an nvme mount etc.)"}
{"question": "Hi, in ray 2.2.0 the *OOM monitor* is enabled by default. From the docs, it seems like this feature is a really good idea for running on the ray head node, because of, and quote:\n&gt;  OOM may also stall metrics and if this happens on the head node, it may stall the <https://docs.ray.io/en/latest/ray-core/ray-dashboard.html#ray-dashboard|dashboard> or other control processes and cause the cluster to become unusable.\nHowever, *we don't see OOM crashes that problematic on non-head workers*. Sometimes, it is preferable due to small spikes in memory to leave memory management to the OS.\nWhat I want to see is to be able to disable OOM monitor for tasks/actors running on non-head nodes. Is this something that has been discussed in the past?"}
{"question": "Hi,\nCould use some help from the experienced users on 2 things:\n1. I'm running ray on colab with gpu activated in the session, but it isnt detected. Tried ray.init(num_gpus=1) but ray.get_gpu_ids() returns empty list.\n2. I have a custom environment with large observation and action spaces, when I run any algorithm on CPU (since gpu isn't detected for me yet) with a single worker, it gets out of memory error. I suspect some setting might be allocating too much memory due to the large spaces - any idea what it might be? \nThanks!"}
{"question": "For ray.tune, is there a way for users to *force overwriting an existing trial_dir*, w/o ray tune automatically adding a random suffix to avoid duplication?"}
{"question": "A newbie question: Which documentation should I follow to setup Kuberay on a shared cluster running Istio? I see details on <https://github.com/ray-project/kuberay|kuberay> to setup the ray operator, but not super clear on what else I need to do to make this fully functional."}
{"question": "What is the best pattern for doing cleanup within tasks/actors when the client is disconnected or shuts down?  The specific problem is that our tasks/actors create a lot of temporary stuff in AWS while they are running and if possible, we would like to run a cleanup function if the client shuts down unexpectedly.  Is there a task/actor-side exception/signal that we could handle that would give anything running in ray a last chance to shut down gracefully in this scenario?"}
{"question": "I am exploring LSTM PPO and I got stuck on feeding the model initial data (n observations steps before it should start generating) . For example, I want to train the PPO to continue writing text, one letter at a time.\nBefore the model does any predictions, I would like to pass a phrase it should continue, such as \"Once apon a time\" and then in a loop predict a letter and feed the letter back in.\nTo be honest, I am quite flabbergasted about how this pre-seeding would fit into the RLlib training. Any tips or github links to something even remotely similar?"}
{"question": "Hi all! I was wondering what's the recommended way of getting ID inside Ray Tune config?  My problem:\n1. I define ray tune objects with a search space, like this:\n```    trial_space = {\n        \"learning_rate\": tune.loguniform(1e-5, 1e-2),\n         \"alpha\": tune.loguniform(1e-5, 1e-2),\n    }\n    train_model2 = tune.with_resources(\n        train_model, resources=tune.PlacementGroupFactory([{\"GPU\": 1}] + [{\"CPU\": 1}] * 8, strategy=\"STRICT_PACK\")\n    )\n\n    reporter = ray.tune.CLIReporter(max_report_frequency=300)\n    tuner = tune.Tuner(\n        train_model2,\n        tune_config=tune.TuneConfig(\n            metric=\"tune_score\",\n            mode=\"max\",\n            num_samples=100,\n        ),\n        param_space=trial_space,\n        run_config=ray.air.RunConfig(progress_reporter=reporter),\n    )\n    results = tuner.fit()```\n2. Now, the \"train_model\" gets my config with some learning_rate and alpha.  However, I would like also each call to train_model to contain some unique ID  (Ideally from the range  0....N-1,  where N is the number of all my trials)\n3. I tried using a global counter (thread safe, with lock) inside train_model and increment it, but then Ray complains it can't pickle sth\n(the counter was like this:\n```class ThreadSafeCounter():\n    def __init__(self):\n        self._counter = -1\n        self._lock = Lock()\n\n    def increment_and_get(self):\n        with self._lock:\n            self._counter += 1\n            return self._counter\n\nTASK_COUNTER = ThreadSafeCounter()\ndef train_model(config):\n    global TASK_COUNTER\n    my_run_id = TASK_COUNTER.increment_and_get()   # this doesn't work```"}
{"question": "How would you recommend to get any ID inside train_model ? :eyes:"}
{"question": "So I guess to elaborate, poetry packages are pip installable - If I have a poetry project I can just `pip install path/to/poetry/project`\n\nSince that is the case, I'm wondering if Ray would support local paths as pip dependencies?"}
{"question": "Hi all, what is the difference between `tune.run()` and `tuner.fit()`?"}
{"question": "hi, will there be `delete_job_logs` in job submission SDK in future that can delete job logs from ray cluster?\n<https://docs.ray.io/en/latest/cluster/running-applications/job-submission/jobs-package-ref.html>"}
{"question": "Hello everyone, I work at a company where we heavily use Ray for parallelizing different parts of our application. This works great for us and recently we were happy to also rewrote some of our logic into Ray Workflows.\nOne thing though I still struggle to understand so your help would be much appreciated.\n\nI have a simple setup with two ActorPools that seems to be working different than what I expected.\n\nHere is a simple example of my use-case, all unnecessary logic has been removed and blocking operation of DB read/write has been replaced by sleep.\n\n```import ray\nimport time \n\nray.shutdown()\nray.init()\n\n@ray.remote\nclass SimpleActor():\n    def __init__(self, mark):\n        self.mark = mark\n    \n    def work(self, connector_actor_pool):\n        print (\"Start work: \", self.mark)\n        connector_actor_pool.submit(lambda a, v: a.connect.remote(v), {\"mark\":self.mark})\n        connector_actor_pool.get_next()\n        print (\"Stop work: \", self.mark)\n \n@ray.remote\nclass ConnectorActor():\n    def __init__(self, con_id):\n        self.con_id = con_id\n    \n    def connect(self, mark):\n        print (f\"Connected ConnectorActor: {self.con_id} for SimpleActor {mark}\")\n        time.sleep (10)\n        print (f\"Disconnected ConnectorActor: {self.con_id} for SimpleActor {mark}\")\n        return 1\n \nconnector_actor_pool = ray.util.ActorPool(\n                [ \n                    ConnectorActor.options(num_cpus=1).remote(con_id=_)\n                    for _ in range(2)\n                ]\n            )\n\nsimple_actor_pool = ray.util.ActorPool(\n                [ \n                    SimpleActor.options(num_cpus=1).remote(mark=_)\n                    for _ in range(4)\n                ]\n            )\n\nfor i in range (4):\n    simple_actor_pool.submit(lambda a, v: a.work.remote(v), connector_actor_pool)```\nIn the code snippet I am instantiating two ActorPools where one `ActorPool` (`connector_actor_pool`, 2 Actors) is being passed as a parameter to Actors from another pool (`simple_actor_pool`, 4 actors).\n`simple_actor_pool` is behaving as expected, immediately all 4 Actors from that pool are scheduled with tasks.\nI would excpet `connector_actor_pool` to be scheduling tasks for both Actors from but it seems that only one Actor from `connector_actor_pool` is being used while other Actor from that pool is never scheduled with any task.\n\nHere are the logs from previous run:\n```(SimpleActor pid=26779) Start work:  3\n(ConnectorActor pid=26773) Connected ConnectorActor: 1 for SimpleActor {'mark': 3}\n(SimpleActor pid=26775) Start work:  1\n(SimpleActor pid=26777) Start work:  2\n(SimpleActor pid=26774) Start work:  0\n(SimpleActor pid=26779) Stop work:  3\n(ConnectorActor pid=26773) Disconnected ConnectorActor: 1 for SimpleActor {'mark': 3}\n(ConnectorActor pid=26773) Connected ConnectorActor: 1 for SimpleActor {'mark': 1}\n(ConnectorActor pid=26773) Disconnected ConnectorActor: 1 for SimpleActor {'mark': 1}\n(ConnectorActor pid=26773) Connected ConnectorActor: 1 for SimpleActor {'mark': 0}\n(SimpleActor pid=26775) Stop work:  1\n(ConnectorActor pid=26773) Disconnected ConnectorActor: 1 for SimpleActor {'mark': 0}\n(ConnectorActor pid=26773) Connected ConnectorActor: 1 for SimpleActor {'mark': 2}\n(SimpleActor pid=26774) Stop work:  0\n(ConnectorActor pid=26773) Disconnected ConnectorActor: 1 for SimpleActor {'mark': 2}\n(SimpleActor pid=26777) Stop work:  2```\nOnly ConnectorActor 1 is running while pool holds 2 ConnectorActors which I would except to be both available for use by `simple_actor_pool` Actors.\n\nFinally, is this a known limitation of ActorPools or am I misusing them? :)"}
{"question": "I have a basic question. Looking through the Serialization docs: <https://docs.ray.io/en/latest/ray-core/objects/serialization.html> it says:\n`Numpy arrays in the object store are shared between workers on the same node (zero-copy deserialization).`\n\nAre numpy arrays the _only_ array data type that have the zero-copy deserialization property, or does this apply to all arrow backed dataformats. What about pandas dataframes which are backed by numpy or the other data primitives supported by Ray DataSets?"}
{"question": "A question for the folks using Ray as a backend. What do you use as a frontend? I am looking for some UI landing page to link to notebooks, tensorboards, MLFlow (experiment tracking), pipelines, etc. Ideally looking for some OSS project that can use Ray as a backend."}
{"question": "Hello! I\u2019m trying to launch ray serve task in the kubernetes cluster.\nI packaged working dir as a zip and uploaded it to the Google Cloud Storage bucket.\nI used this as a starter for my deployment <https://github.com/ray-project/kuberay/blob/release-0.3/ray-operator/config/samples/ray_v1alpha1_rayservice.yaml#L14> and replaced `working_dir` url with the path in google bucket.\nI get the error message `You must `pip install smart_open` and `pip install google-cloud-storage` to fetch URIs in Google Cloud Storage bucket. But` I don\u2019t understand where this should be executed.\nI tried to add `pip` section into `serveConfig`, but it seems to be ignored\n\n```  serveConfig:\n    importPath: fruit.deployment_graph\n    runtimeEnv: |\n      working_dir: \"<gs://mybucket/fruit-example.zip>\"\n      pip:\n      - google-cloud-storage==1.43.0\n      - smart-open```\nCould someone please point me out what do I miss?"}
{"question": "Is there a way to make sure a cluster created with `ray.init(log_to_driver=True)` has flushed all its logs to the driver before the subprocesses are killed with a `ray.shutdown()` ? I have some code where I am needing to explicitly add a `time.sleep`  before calling `ray.shutdown`  to give the logs time to make it back to the driver"}
{"question": "for Ray on AWS, is there an easy way to use custom `ssh_user` name?\nit seems to give `Permission denied (publickey)` error. This is w/ `use_interal_ids`  set to True."}
{"question": "How bad of an idea would it be to use `MultiDiscrete([2, 2, 2, 2, 2])`  instead of  `MultiBinary(5)`  for `action_space` ? It feels ok, since PPO (or any other) alg in RLlib does not support `MultiBinary` action space, but wanted to check with the heros of this channel. Thoughts? Any drawbacks I should watch out for?"}
{"question": "Anyone here building things with stable diffusion?"}
{"question": "Is there some guide to securing connection to Ray users' laptops?"}
{"question": "what is ray used for?"}
{"question": "I use Java more than python and I was wondering how I can build a distributed system with ray for java batch workloads. Can I use it to this end along with kafka? both are reactive it seems."}
{"question": "Happy New Year guys :smile: question, been trying to figure out how to programmatically record node metrics like GPU % without using prometheus? Saw <https://docs.ray.io/en/latest/ray-observability/ray-metrics.html#ray-metrics|this documentation> but was wondering if anyone else has had any luck?"}
{"question": "We've stood up KubeRay and have the dashboard exposed via k8s ingress at a url: \"<https://path.to.ray.dashboard>\"\n\nI'm trying to use this url for the argument in `ray.init` . It complains about the protocol if I include it and the port if it is omitted.\n\n```ray.init(\"path.to.ray.dashboard:80\")```\nbut I see errors\n\n```2023-01-25 09:42:24,541\tWARNING utils.py:1414 -- Unable to connect to GCS at path.to.my.ray.cluster:80. Check that (1) Ray GCS with matching version started successfully at the specified address, and (2) there is no firewall setting preventing access.```\nAre there other ports that need to be exposed? Any help getting this working?"}
{"question": "Hello Ray team, a couple of other senior students and I are looking to do an external academic project for our distributed systems course for extra credit and we thought Ray would be a cool project to contribute to!\n\nWe were wondering if anyone has any pointers to projects/issues in Ray with an academic and distributed systems component that could be implemented in a timeframe of about 12 week or less (and we would be happy to work on it after the course is finished).\n\nWe\u2019ve checked the <https://github.com/ray-project/ray/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22|help wanted> on git but it\u2019s empty, so we thought it would be a good idea to check here if anyone has something on their minds."}
{"question": "Does Ray object store have any limits on object size or number of objects?"}
{"question": "Is it possible to change the max concurrency of an asynchronous or threaded actor?"}
{"question": "Hello Ray team, I was looking at the GRPC ingress (<https://docs.ray.io/en/latest/serve/direct-ingress.html|here>) and I understand that it is still in alpha and I was just wondering if it is possible to use that with deployment graph (<https://docs.ray.io/en/latest/serve/tutorials/deployment-graph-patterns/linear_pipeline.html|like this>)?\nIf so, I am assuming we would be able to autoscale the different models independently?\nIf not, is this something that is in the roadmap for the future?"}
{"question": "Hi everyone! I was wondering what is the proper way to retraining RL Algorithm on an additional offline dataset.\nLet's say that I have 3 offline datasets (for simplicity each one is in one `.json` file):\n`offline_ds = ['a.json', 'b.json', 'c.json']`\nThen I build and train an offline RL algorithm:\n```config = (    \n    MARWILConfig()\n    .environment(env=None, observation_space=env.observation_space, action_space=env.action_space)\n    .offline_data(input_=offline_ds)\n)\nalgo = config.build()\nfor _ in tqdm(range(20)):\n    algo.train()```\nThen I get the new dataset `d.json` which I want to add somehow to `inputs_`:\n`algo.config.update_from_dict({'input_': ['a.json', 'b.json', 'c.json', 'd.json']})`\nHowever, if I try to do so, I got an error that `Cannot set attribute (input_) of an already frozen AlgorithmConfig!`.\nThus the question: what is the proper way to \"update/rebuild\" algorithm with new config, when the algorithm was already trained (and so also build)?"}
{"question": "Has there been any discussion around securing access to a cluster?"}
{"question": "Anyone know if there\u2019s a way to prevent `ray.init` connections to a remote cluster from creating `ray_client_server_[port].out/err` files? Is there some flag we can set to suppress this?"}
{"question": "Hi team, i want to use ray like a k8s operator, each job submitted to cluster will create multi worker to complete it, is it possible for ray? thank you in advance!"}
{"question": "Is there a way to prevent creating new environment when restoring from a checkpoint?\nI am following <https://docs.ray.io/en/latest/serve/tutorials/rllib.html> and every time I call `config.build(env=\"MyEnv\")` it creates new instance of the environment.\nThe problem is, that during serving the environment not available and, as per example, we interact with the agent manually via `algorithm.compute_single_action`"}
{"question": "Hi guys, is there any documentation where I could find all possible env vars for RayCore?"}
{"question": "Good afternoon all. I came across Ray when integrating some existing tools where it was a dependency but ran into an issue when attempting to install where he dependency is listed as: ray[default]==1.4.1 although pip chokes on this reporting \u2018No matching distribution found\u2019. I should further clarify that I was put off Python 25 years back when Redhat selected it over Perl for their admin tools, so I\u2019m just now brushing off the rust. In any case, The dependency is apparently specified incorrectly. I\u2019d appreciate any guidance as to he correct dependency specification, as well as where breaking changes may exist preventing use of newer Ray releases. A quick review of release notes suggests I should be able to move to 1.13.0 without issue, but was \u2018default\u2019 ever a valid module specification within Ray? I recognize this is an extremely noob question, but any guidance would be helpful. Thanks."}
{"question": "Anyone who can tell me how to deal with the \"The learning thread died while training!\" Error?"}
{"question": "<@U04LY22AC21> afaik `ray[default]`  has been a valid and one of the common installation option of ray such as current docs: <https://docs.ray.io/en/master/ray-overview/installation.html#from-wheels>. People typically uses this option to install dashboard (which is a poor name given dashboard also acts as the \"API Server\" of Ray cluster)\n\nRay 1.4.1 is ... very old, mind give it a try to ray 2.0 and above if possible ?"}
{"question": "Hey guys! I have a new issue here. I have been scrolling through the log files &amp; open issues for hours but couldn't seem to find any solution. Has anyone seen this behaviour before? <https://github.com/ray-project/ray/issues/32047>"}
{"question": "Hi! I have a question regarding the collective communication. Currently, the collective communication supports 3 tensor types: `torch.Tensor`, `numpy.ndarray` and `cupy.ndarray` . Is it possible to support a new tensor type? (e.g. JAX's array. I know that JAX is supported in remote functions.)"}
{"question": ":wave: I'm having trouble with the fact that raydp depends on the backport of the `typing` lib. Some more details in <https://stackoverflow.com/questions/75262818/avoid-transitive-dependency-on-python-typing-backport|this StackOverflow post>. Any thoughts?"}
{"question": "for memory aware scheduling it does say that by specifying memory requirements it does not limit, but is there a way to limit? <https://docs.ray.io/en/latest/ray-core/scheduling/memory-management.html#memory-aware-scheduling>"}
{"question": "ERROR WHEN model.fcnet_hiddens tuned using tune.grid_search AND SCHEDULER \"PB2\"\nHello guys,\nThank you so much for your super amazing work constructing ray!\nOne question, you know what following error? this does not happened before 2.2.0"}
{"question": "Hi. Regarding `RAY_USE_TLS` in Ray on k8s. I\u2019ve defined env vars in the helm chart values:\n    `- name: RAY_USE_TLS`\n      `value: \"1\"`\n    `- name: RAY_TLS_SERVER_CERT`\n      `value: \"/tmp/ray/certs/server.crt\"`\n    `- name: RAY_TLS_SERVER_KEY`\n      `value: \"/tmp/ray/certs/server.key\"`\n    `- name: RAY_TLS_CA_CERT`\n      `value: \"/tmp/ray/certs/rootCA.crt\"`\n\nAll files are mounted and existing in `/tmp/ray/certs/` as expected. During the head node launch I have this error message:\n\n`details = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:10.108.163.118:6379: Peer name 10.108.163.118 is not in peer certificate\"`\n\t`debug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:10.108.163.118:6379: Peer name 10.108.163.118 is not in peer certificate {grpc_status:14, created_time:\"2023-01-31T00:35:58.84152496-08:00\"}\"`\n\nI added in SAN:\n`DNS:localhost and IP:127.0.0.1` \nBut seems like it tries to connect using pod IP. But I cannot define in SAN pod IP cause its dynamic and will be changed during the restart. Any suggestions/workarounds? Thanks."}
{"question": "Is there anyway to mention RAM requirement in RayParams?\n```RayParams(\n        num_actors=6,  # Number of remote actors\n        cpus_per_actor=1)```"}
{"question": "Hi, I have a question about conditional sampling. I need to sample pairs of params (one depend on the other), so ideally I'd have sth like this:\n```trial_space = {\n   (\"param_a\", \"param_b\"): tune.choice ( [   (1, \"xyz\"),  (2, \"foo\"), (42, \"bar\") ])\n}```\n(correct me if I'm wrong, but I think such sampling is impossible with Ray Tune ^  ?)\n\nAfter reading <https://docs.ray.io/en/latest/tune/tutorials/tune-search-spaces.html#tune-custom-search>  I understood this probably should be implemented as sth like this:\n\n```def fun(spec):\n    if spec.config.a == 1:  return \"xyz\"\n    if spec.config.a == 2: return \"foo\"\n    if spec.config.a == 42: return \"bar\"\n\ntrial_space = {\n    \"param_a\": tune.choice([1, 2, 3]),\n    \"param_b\": tune.sample_from(fun),\n}```\nBut now, I'm really wondering... how Ray Tune makes sure all \"dependencies\" used by sampling function are already computed?  What if my sampling function depends on a parameter, that's computed by other sampling function?"}
{"question": "Is there a better place to ask questions about `raydp` ?"}
{"question": "Ah this is a clearer to me. I think jax mesh transformer only use Ray core api thus it shouldn't matter much if you have Ray 2.2 or 1.4 \u2026 can you give that a try? We also had better platform support of Ray wheels comparatively"}
{"question": "does ray work with terraform?"}
{"question": "Hmm.  Maybe the prerelease version of ray that got pulled into the default cluster build?"}
{"question": "Quick question, is there a way to get the resources requested from within a task?\n\nSomething like:\n```@ray.remote(num_cpus=1)\ndef f():\n    assert ray.get_resources()[\"num_cpus\"] == 1```\nI understand that for GPUs this is possible by inspecting `CUDA_VISIBLE_DEVICES`, but was wondering if there is a more general method here for other resources so I can ensure my function is well behaved."}
{"question": "hiya - we're having issues connecting to aws ec2 head node.\nWe're using private vpc and only allowing private ipv4 only access.\nProblem  is `ray.init` cannot connect to the head node.\nAny idea why? Thanks!"}
{"question": "For ray.init() to succeed does the version of python on the client side need to exactly match the version used to start the ray node (i.e. major/minor/micro) - all three must match?"}
{"question": "Hi,\nI'm having a small problem during rl training, I have tried several algorithms but the results are the same, and I suspect something is happening with the gradient.\nI'm using a normalized continues action space, and what happens is that at some point during the training, usually after about 50k timestamps - the mean reword starts to drop and actions become skewed towards the edges of the space range, until it dies completely with 0 reward.\nIf I try lower learning rate, then it repeats but in many more timestamps.\nI tried to use tune and set different parameters, including the clip to find if any settings could provide a stable learning.. but its not happening.\nI'm also getting warnings during the tune training: WARNING:tensorboardX.x2num:NaN or Inf found in input tensor.\nIt might also suggest the something is going wrong with the gradient, any idea how to handle it? thanks!"}
{"question": "What\u2019s ray\u2019s governance model? I found a \u201cMore coming soon!\u201d message from 10 months ago on:\n<https://github.com/ray-project/community/tree/main/governance>\n\nAre there any plans to donate the project to ASF, LF, CNCF?"}
{"question": "tune.report and session.report does not work with ray.train specifically xgboost_ray? how to report custom metrics to SearchGenerator?"}
{"question": "Hi all, I have a question regarding DAG construction in Ray: is it possible to subscript/index Task outputs in a DAG during construction time?\n\nFollowing from the example in the <https://docs.ray.io/en/latest/ray-core/ray-dag.html|documentation>, I wish to do the following:\n\n```@ray.remote\ndef func(src, inc=1):\n    return src + inc\n\n@ray.remote\ndef func_returns_dict(src, inc=1):\n    return {\"increase\": src + inc,\n            \"decrease\": src - inc}\n\na_ref = func.bind(1, inc=2)\n\nassert ray.get(a_ref.execute()) == 3 # 1 + 2 = 3\n\nb_ref = func_returns_dict.bind(a_ref, inc=3)\n\nc_ref = func.bind(b_ref[\"increase\"], inc=a_ref)```\n\nbut this raises `TypeError: 'FunctionNode' object is not subscriptable`\n\nIn Dask, for example, this is allowed:\n\n```@dask.delayed\ndef func(src, inc=1):\n    return src + inc\n\n@dask.delayed\ndef func_returns_dict(src, inc=1):\n    return {\"increase\": src + inc,\n            \"decrease\": src - inc}\n\na_ref = func(1, inc=2)\n\nb_ref = func_returns_dict(a_ref, inc=3)\n\nc_ref = func(b_ref[\"increase\"], inc=a_ref)\n\nassert c_ref.compute() == 9 ```\nIs this FunctionNode object not being subscriptable done by design as a safety feature? Is there a way to work around it without creating some new `get_key`  Task? Thanks!"}
{"question": "Do Ray Async/Threaded Actors deserialise tasks in parallel if max concurrency is greater than 0?"}
{"question": "<@U04MBS39RCH> it can be supported. Currently accessing dict / attribute by key is only supported by <https://sourcegraph.com/github.com/ray-project/ray/-/blob/python/ray/dag/input_node.py?subtree=true|InputNode> to facilitate accessing partial fields of original input, which under the hood each access spawns a new `InputAttributeNode` that get replaced by real value at runtime. This access pattern is not implemented by `FunctionNode`  yet however, but can be done following same pattern."}
{"question": "Hi all, I'm adding metrics (with prometheus and grafana) to our ray cluster deployed with kuberay on our K8s cluster. While its not straightforward, we can now see the metrics in grafana.\nThe problem is when checking the metrics in the Ray dashboard. For this, we usually do a port-forward (`kubectl port-forward`). Of course it doesn't work with iframe showing content from (port-forwarded) localhost.\nHow does everyone do it? I tried to play with `RAY_GRAFANA_HOST` and `RAY_GRAFANA_IFRAME_HOST` vars without much success. We don't want to expose the dashboard and grafana publicly (without authentication)."}
{"question": "Hey guys, I have been wondering where I can adjust the network optimizer? I found lots of config here: <https://docs.ray.io/en/releases-2.1.0/rllib/rllib-models.html> but nothing alike. However in my config file, there is \"adam_epsilon\": 1e-08. I am currently experiencing a very poorly performing DQN and am trying to see why that is. I tryed tuning for theses parameters:\n\u2022 v_min\n\u2022 v_max\n\u2022 n_step\n\u2022 hiddens\n\u2022 dueling\n\u2022 noisy\n\u2022 lr\n\u2022 train_batch_size\n\u2022 gamma\n\u2022 final_epsilon\nIs there maybe any crucial parameter I might be missing? My environment is multi-agent but otherwise fully deterministic. I am using a shared policy and experience worse performance when using a distributed one."}
{"question": "Or is there a way to pass reference to ActorPool to Ray Workflow tasks so all tasks from workflow submit to the same ActorPool instead of individual ActorPool copies?"}
{"question": "I am getting\n```ValueError: Trial returned a result which did not include the specified metric(s) `['val_ks', 'ks_diff']` that `SearchGenerator` expects. Make sure your calls to `tune.report()` include the metric, or set the TUNE_DISABLE_STRICT_METRIC_CHECKING environment variable to 1.```\nhow can I report custom metrics from train_breast_cancer in <https://github.com/ray-project/xgboost_ray/blob/master/xgboost_ray/examples/simple_tune.py> ?\n\nI have tried train.report, session.report and tune.report nothing seems to work."}
{"question": "Hi everyone,\n\nWe have been using Ray Train and Ray Tune independently on each of our on-premise machines (SSH into a given machine, and start the training/hyperparameter tuning scripts manually). Now we have some spare time to re-think/re-organize our on-premise infrastructure, but I need to ask some architectural/conceptual support/recommendations from you guys, who are a black belt in Ray clusters.\n\n*Current architecture/methodology*\n&gt; We have 3 on-prem machines with different GPU setups:\n&gt; \u2022 machine-1: 12x vCPU + 128GB RAM + *3x 12GB GPU*\n&gt; \u2022 machine-2: 8x   vCPU + 128GB RAM + *4x 12GB GPU*\n&gt; \u2022 machine-3: 64x vCPU + 256GB RAM + *2x 48GB GPU*\n&gt; \n&gt; machine-1 and machine-2 are mainly used for \n&gt; \u2022 training/hyperparameter tuning smaller (e.g.: time-series data analysis) models\n&gt; \u2022 training/testing larger computer vision models with small batch sizes (typically batch-size=1)\n&gt; \n&gt; machine-3 is mainly used for\n&gt; \u2022 training/hyperparameter tuning larger computer vision models (with medium batch sizes) \n&gt; \u2022 training/hyperparameter tuning smaller (e.g.: time-series data analysis) models with large batch sizes\n&gt; \n&gt; For making calculations simple, let's assume that \n&gt; \u2022 each 12GB GPU can fit 2 time-series model trainer replicas\n&gt; \u2022 each 12GB GPU can fit 1 computer vision model testing (testing codes are not related to Ray Train or Tune)\n&gt; \u2022 each 48GB GPU can fit 10 time-series model trainer replicas\n&gt; \u2022 each 48GB GPU can fit 3 computer vision model trainer replicas \n*Desired architecture/methodology*\n&gt; Being able to do the following scenarios (with minimal manual configuration each time):\n&gt; \u2022 scenario-1: starting a *SINGLE* time-series model hyperparameter tuning on pre-defined number of (1...7) 12GB GPUs (2 trainer replicas each) and on pre-defined number of (1...2) 48GB GPU(s) (10 trainer replicas each)\n&gt; \u2022 scenario-2: \n&gt;     \u25e6 starting a *SINGLE* time-series model hyperparameter tuning on e.g.: 5x 12GB GPUs (2 trainer replicas each)\n&gt;     \u25e6 starting 2 separate computer vision model testing on the remaining 2x 12GB GPUs (1 testing process on each GPU)\n&gt;     \u25e6 starting a *SINGLE* computer vision model hyperparameter tuning on the 2x 48GB GPU(s) (3 trainer replicas each)\n&gt; \u2022 ...\nAs our GPUs have different GPU memories, setting for example _resources={\"cpu\": 8, \"gpu\": 0.33}_ for a trainable would mean different GPU memory in case of a 12GB and a 48GB GPU. Furthermore, the CPU and RAM setup of our machines vary as well.\nSo I am a little bit confused whether Ray is flexible enough to handle all the scenarios above. But if it is, then how would you set the Ray cluster(s) up and configure it according to the scenarios?\n\nPlease note that I am a newbie in Ray clusters so any detailed answer/solution would be very appriciated :slightly_smiling_face:"}
{"question": "I have some questions about best practices and limitations for unit testing Ray Actors, especially in non-toy examples.\nFor any sufficiently complex system, unit testing involves mocking large portions of that system, including environment variables, constants, network requests, and especially external dependencies like database.  However, when unit testing Ray Actors, the only way for mocks to actually apply is to do the following:\n1. Run in `local` mode when initializing a local Ray cluster\n2. Create actors in a \u201cJIT\u201d fashion by forgoing the `@ray.remote()` decorator and instead turning your classes into actors at the last minute (`ray.remote(**options)(ActorType).remote(**kwargs)` )\nSome questions about this\n\u2022 Is this correct about local mode? From what I can tell, when not running in local mode the Actors are actually run in another process after being pickled and sent via the GCS to the worker. Pickling and running remotely in this fashion avoids picking up any of the mocks/global fixtures that one generally relies on when unit testing\n\u2022 If this is the case, and given that local mode appears to be widely used for unit testing purposes (based on Ray github issues that mention it), why is local mode being deprecated? It seems like the only reasonable way to actually test a sufficiently complex system where mocks are required and _especially_ if someone is trying to integrate actors into an existing complex codebase\n\u2022 Does the Ray team (or anyone else here who has built a sufficiently complex system with actors) have any other tips and tricks for unit testing and specifically mocking subcomponents of actors? We would _love_ to run in non-local mode to better test the full system and have access to functionality like killing actors, the Ray state APIs, etc, but it seems impossible without just writing full integration tests that forgo the mocks"}
{"question": "What is the status of the ray cpp api it still being developed? Is there any important missing features ?"}
{"question": "Has anyone got below error when you were running on a sizable cluster (~200 aws ec2 instances)?\n```[specific-server] Failed to put health check on 10.28.49.38:6379                                                                                                                                                                                                                                                                               \n&lt;_InactiveRpcError of RPC that terminated with:                                                                                                                        \n        status = StatusCode.UNAVAILABLE                                                                                                                                                                                                                                                                                                        \n        details = \"failed to connect to all addresses\"\n        debug_error_string = \"{\"created\":\"@1675463687.018215781\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3217,\"referenced_errors\":[{\"created\":\"@1675463687.018214277\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/lib/transport/error_util\ns.cc\",\"file_line\":165,\"grpc_status\":14}]}\"```\nBasically the head node gave up and and I cannot reach from launcher machine, so nothing updates.\n\nDo I need a \"bigger\" head node when the cluster size gets bigger?"}
{"question": "hiya, have you guys run into this?\n\nSo as advised by <@UN5QFN0AV>, I've moved to `r5dn.4xlarge` head node w/ resources set to `{\"CPU\": 0, \"GPU\": 0, \"memory\": 0}` , so no jobs will run on it.\n\nThe cluster is on aws, w/ only 100 r5.xlarge instances. I wouldn't say it's tiny, but definitely not big.\n\nAny reason why this keeps happening? Appreciate any help and advice!\n```==&gt; /tmp/ray/session_latest/logs/monitor.out &lt;==\n2023-02-06 00:11:41,203 WARNING utils.py:1414 -- Unable to connect to GCS at 10.28.49.209:6379. Check that (1) Ray GCS with matching version started successfully at the specified address, and (2) there is no firewall setting preventing access.\n2023-02-06 00:11:44,988 WARNING utils.py:1414 -- Unable to connect to GCS at 10.28.49.209:6379. Check that (1) Ray GCS with matching version started successfully at the specified address, and (2) there is no firewall setting preventing access.```\nTo add, the exact error behavior is that:\n1. I can access head initially \n2. after some time, I start receiving the above GCS error, and I cannot ping head node ip any more\nI've checked and there was no firewall restriction on in/out bound IPs etc."}
{"question": "I have an actor which has a method A that requires X number of CPUs, e.g. X == 10. If the worker of the actor is placed onto a node with exact X number of CPUs, could the resources of this node be used for other tasks when A is not running?\n\nRelated: I'd like to specify different resource requirements for different methods of the same actor, and remember seeing it from somewhere that `ray.remote(...)` can be applied to an actor's methods. However it seems once decorated the methods could no longer be found on the instances of the actor."}
{"question": "Hi all, have a question regarding a weird error encountered when executing a DAG in Ray:\n\nI have a series of Tasks that get executed as part of a DAG.\n\nPreviously I executed them using just Dask, but have converted the code to use all Ray Tasks.\n\nThe workload executes to completion without errors when using `local_mode=True` in `ray.init()` but when I try to run the work in parallel, I get the following:\n\n```(pid=90000)  Traceback (most recent call last):\n(pid=90000)   File \"python/ray/_raylet.pyx\", line 1135, in ray._raylet.task_execution_handler\n(pid=90000)   File \"python/ray/_raylet.pyx\", line 1045, in ray._raylet.execute_task_with_cancellation_handler\n(pid=90000)   File \"python/ray/_raylet.pyx\", line 741, in ray._raylet.execute_task\n(pid=90000)   File \"/Users/lev.udaltsov/dsenv/lib/python3.8/site-packages/ray/thirdparty_files/colorama/ansitowin32.py\", line 47, in write\n(pid=90000)     self.__convertor.write(text)\n(pid=90000)   File \"/Users/lev.udaltsov/dsenv/lib/python3.8/site-packages/ray/thirdparty_files/colorama/ansitowin32.py\", line 177, in write\n(pid=90000)     self.write_and_convert(text)\n(pid=90000)   File \"/Users/lev.udaltsov/dsenv/lib/python3.8/site-packages/ray/thirdparty_files/colorama/ansitowin32.py\", line 205, in write_and_convert\n(pid=90000)     self.write_plain_text(text, cursor, len(text))\n(pid=90000)   File \"/Users/lev.udaltsov/dsenv/lib/python3.8/site-packages/ray/thirdparty_files/colorama/ansitowin32.py\", line 210, in write_plain_text\n(pid=90000)     self.wrapped.write(text[start:end])\n(pid=90000)   File \"/Users/lev.udaltsov/dsenv/lib/python3.8/site-packages/ray/_private/utils.py\", line 468, in write\n(pid=90000)     self.stream.write(data)\n(pid=90000) ValueError: I/O operation on closed file.```\nI never came across this error when I ran in parallel using Dask, where the code works across multiple processes/threads. From my understanding the issue here is from the `colorama`  module trying to log some information, is that correct?\n\nAny tips on how to resolve this would be appreciated"}
{"question": "So, I think that what I want to do is set up an autoscaling cluster and access it in remote \"interactive\" mode (even though it is going to be accessed by our python apps being orchestrated by Airflow).  We're already building docker containers with our development environment in them, which should be a great place for us to run workers in.  They're in Amazon Elastic Container Registry.  I'm just starting to see if I can get them to \"just work\" with the cluster config.\n\nAnyway: does this sound reasonable?  Also, I did start up a cluster using one of the default ray containers.  I couldn't tell if it was in docker or not -- is there a way to either connect into the host (outside of docker) or to verify that when I do a `ray attach` that docker is running and I'm connected into that?"}
{"question": "Hi again,\nAdditional question about rllib please - I'm trying to use an impala trained model for inference, for that reason I've used Policy.from_checkpoint().\nNote that I only need the policy, not the entire functionality for training and so on.\nUsing Policy.from_checkpoint() takes a while, I would say 2-3 minutes.. is that normal? is there any way to speed this up? I'm looking a speed solution for inference time.\nThanks!\n\n*Edit: I'm using colab pro"}
{"question": "Is there an easy way to get some logs out of the autoscaler? Specifically debug logs from `resource_demand_scheduler`."}
{"question": "So, we have a fairly big and kinda clumsy object that our R&amp;D folks insist on passing into a bunch of functions.  From what I can see, we're having issues serializing it and passing it over the wire and deserializing it, judging by the stack trace.  It seems to work fine if I just use python's pickle.dumps() and reload it, though.  Any tips on figuring out what's going on?  I'm going to embark on refactoring a pile of code to pass in simpler objects, but if someone has suggestions, I can try that."}
{"question": "So, for remote processing, what part of my application actually gets distributed to workers?  Is it just the stuff tagged with @remote or some such, or my entire python process, or something in between?"}
{"question": "Hi guys - I have a question on terminating ray remote worker node as soon as ray task is finished\n\nUsing the example below on Ray doc:\n```start = time.time()\nresult_ids = [do_some_work.remote(x) for x in range(4)]\nsum = 0\nwhile len(result_ids):\n    done_id, result_ids = ray.wait(result_ids)\n    sum = process_incremental(sum, ray.get(done_id[0]))\nprint(\"duration =\", time.time() - start, \"\\nresult = \", sum)```\nif `result_ids` are scattered on 4 different ec2 instances/worker nodes, how do I kill the node running `done_id` , as soon as `ray.wait` is returned?\nThe behavior I'm seeing now is that on that worker node, cpu usage is almost 0, but I could still see raylet running.\nIs there a way to programmatic tell ray to terminate that worker node?"}
{"question": "Hi,\nI have already posted a related issue here: <https://discuss.ray.io/t/memory-pressure-issue/9022/4> and no reply in 20 days.\nIt seems related to everything I'm experiencing, maybe the model created is too big?\nI tried a simplified action space and with a single worker the ram usage is still skyrocketing.\nTo recreate my issue, you can simply create random custom environment with multi discrete action space like spaces.MultiDiscrete(np.full((2500), 8)).\nI'm writing it here on slack because my time for this project is very limited and I'm still confident that I can use rllib to solve the problem\nThanks!"}
{"question": "Hi Folks!  I have a question around spinning up a cluster on AWS. Is it possible to launch nodes in a specific vpc, if there are multiple vpcs in the AWS account + region ? I would like to use all available public or private subnets in a given vpc. Intention is to be able to specify a vpc and use either all private or all public subnets.  At the moment,  I am using this kind of config to limit the nodes to a particular subnet (just to tie it to a given vpc), but it limits the ec2 inventory to only 1 availability zone which is not my intention. I want to use all private or public subnets in a given vpc:\n```    node_config: \n      InstanceType: m6i.8xlarge\n      ImageId: ami-0aa7d40eeae50c9a9\n      NetworkInterfaces:\n        - AssociatePublicIpAddress: True\n          SubnetId: subnet-004c4ce44e904a937\n          Groups: [sg-0b7b434da6b0c24c2]\n          DeviceIndex: 0```"}
{"question": "Hi all, I've been trying out RayDP for running Spark on Ray. I'm needing to leverage some Spark plugins. I've been looking through the docs and haven't found any instruction on adding Spark dependencies for RayDP. Does anyone know how to do that or have a link to docs that describe it?"}
{"question": "I have a program which runs `ProcessPoolExecutor` . When running this program with Ray, if the it is executed on the Ray head it seg faults\n```(main pid=7531) *** SIGSEGV received at time=1675987513 on cpu 10 ***\n(main pid=7531) PC: @     0x7f62db3707f2  (unknown)  (unknown)\n(main pid=7531)     @     0x7f6306ee3090  (unknown)  (unknown)\n(main pid=7531) [2023-02-10 00:05:13,315 E 7531 7609] <http://logging.cc:361|logging.cc:361>: *** SIGSEGV received at time=1675987513 on cpu 10 ***```\nif executed on a Ray worker it gets stuck. I've confirmed without `ProcessPoolExecutor` everything works fine. Has anyone seen similar issues before?"}
{"question": "I have requested `-c 32` and `1 gpu` on a slurm-based cluster. I am trying to run RayTune using `ASHA Schedular` with PyTorch Lightning. What will be the value of `cpus_per_trial` ?\n\nI assume if I have `32` cores available and for each trial I want to dedicate `4` cpu cores. Then number of sample/trial should be `8` . Can someone quickly confirm it?\n\nI am getting following errors:\n```2023-02-10 06:29:27,220 INFO worker.py:1538 -- Started a local Ray instance.                                                                                                                                                                                            \n(raylet) [2023-02-10 06:29:33,884 E 44690 44699] <http://logging.cc:97|logging.cc:97>: Unhandled exception: St12system_error. what(): Resource temporarily unavailable                                                                                                                         \n(raylet) [2023-02-10 06:29:33,924 E 44690 44699] <http://logging.cc:104|logging.cc:104>: Stack trace:                                                                                                                                                                                           \n(raylet) conda/envs/raytune/lib/python3.8/site-packages/ray/_raylet.so(+0xcf00ea) [0x1555539920ea] ray::operator&lt;&lt;()                                                                                                                      \n(raylet) conda/envs/raytune/lib/python3.8/site-packages/ray/_raylet.so(+0xcf28a8) [0x1555539948a8] ray::TerminateHandler()                                                                                                                 \n(raylet) conda/envs/raytune/bin/../lib/libstdc++.so.6(+0xb0524) [0x155552b9e524] __cxxabiv1::__terminate()                                                                                                                                    \n(raylet) conda/envs/raytune/bin/../lib/libstdc++.so.6(+0xb0576) [0x155552b9e576] __cxxabiv1::__unexpected()                                                                                                                                   \n(raylet) conda/envs/raytune/bin/../lib/libstdc++.so.6(__cxa_current_exception_type+0) [0x155552b9e7b4] __cxa_current_exception_type                                                                                                           \n(raylet) conda/envs/raytune/lib/python3.8/site-packages/ray/_raylet.so(_ZNSt6vectorISt6threadSaIS0_EE17_M_realloc_insertIJMN3ray3rpc17ClientCallManagerEFviEPS6_RiEEEvN9__gnu_cxx17__normal_iteratorIPS0_S2_EEDpOT_+0x200) [0x15555324fde0]\n std::vector&lt;&gt;::_M_realloc_insert&lt;&gt;()                                                                                                                                                                                                                                   \n(raylet) conda/envs/raytune/lib/python3.8/site-packages/ray/_raylet.so(_ZN3ray3rpc17ClientCallManagerC2ER23instrumented_io_contextil+0x222) [0x1555532576a2] ray::rpc::ClientCallManager::ClientCallManager()                              \n(raylet) conda/envs/raytune/lib/python3.8/site-packages/ray/_raylet.so(+0x6408f3) [0x1555532e28f3] ray::core::CoreWorkerProcessImpl::InitializeSystemConfig()::{lambda()#1}::operator()()                                                  \n(raylet) conda/envs/raytune/lib/python3.8/site-packages/ray/_raylet.so(+0xe35fd0) [0x155553ad7fd0] execute_native_thread_routine                                                                                                           \n(raylet) /lib64/libpthread.so.0(+0x8539) [0x155555117539] start_thread                                                                                                                                                                                                  \n(raylet) /lib64/libc.so.6(clone+0x3f) [0x155554508cff] clone                                                                                                                                                                                                            \n(raylet)                                                                                                                                                                                                                                                                \n(raylet) *** SIGABRT received at time=1676039373 on cpu 52 ***\n...```\nMy same script run perfectly on PC without a gpu."}
{"question": "does anyone have a guess as to why `ray debug \u2013address=&lt;ip:port&gt;` results in a UnicodeDecodeError on k8s cluster (<https://discuss.ray.io/t/ray-debug-address-ip-port-results-in-unicodedecodeerror-on-k8s-cluster/9208|details>)?"}
{"question": "how do I find the idle time of a node?"}
{"question": "Piggybacking on this tread because the original problem was not addressed and I am running into that.\n\n*Problem*: Allow python clients outside of k8s cluster connect to a Ray cluster running on Kuberay (not just the dashboard, but the client port `10001` using `ray://` protocol)\n\n*Current setup*: The current <https://docs.ray.io/en/releases-2.2.0/cluster/kubernetes/user-guides/config.html#servicetype-ingresses|docs> show how to connect to the ray dashboard from outside of the k8s cluster using k8s ingress. But k8s ingress only supports http/https so `ray.init(address='ray://...')` will not go through this ingress and hence cannot be done from outside of the cluster. This means I cannot connect to a ray cluster from my laptop, to run my python code. This feels like something should be easy to do, for the infinite laptop experience.\n\n<@U04JC1JTTJN>\u2019s solution was to work around this by making the header services a `LoadBalancer` service, but this will not work for me as we do not use the cloud provider's LB. Also, I need ephemeral ray clusters, expecting 10's of clusters going up and down every minute. Using `Nodeport` seems clunky as we need to either handle port conflicts at the k8s cluster level or discover randomly assigned ports at run time.\n\nHow do others do this? Do you always connect from inside the k8s cluster?"}
{"question": "Never mind, just saw this another tread in <#C02GFQ82JPM|kuberay> that says this is not supported or recommended. The ray cluster is expected to be connected from the same k8s cluster. Kind of disappointing for the design I was working on, but at least now I know.\n\n<https://ray-distributed.slack.com/archives/C02GFQ82JPM/p1669714458575799?thread_ts=1669563017.212259&amp;cid=C02GFQ82JPM>"}
{"question": "Hi all, I am trying to deploy model serving on k8s cluster using kuberay. I want to use autoscaler feature for serving which can auto scale up/down the worker pods based on inference traffic. Is this possible with kuberay?\nI am refering to <https://github.com/ray-project/kuberay/blob/v0.4.0/ray-operator/config/samples/ray_v1alpha1_rayservice.yaml> sample. But I dont know how to use autoscaler with this."}
{"question": "Hello Everyone! We are working with/on Pytorch Lightning using strategy ddp_notebook for it to be compatible with IPython. Unfortunatelly, we are finding many issues with this setup. Thus we are looking into the Lightning plug-in from Ray. But it does not seem actively maintained: the last Lightning version supported is old, there are several issues without answer, etc., so our question is whether it is actively maintained or not? Thanks"}
{"question": "is the way to reach out to them through ray github ? or there is some other channel ?"}
{"question": "Just curious is there any plan to provide some profiling APIs? Often users don't have a good sense of how much resources a job actually consumes."}
{"question": "Hi team, i am running an experiment rayjob as the <https://ray-project.github.io/kuberay/guidance/rayjob/|guide> , but the job status keep created, what should i do for it?"}
{"question": "Hey, I'm stuck on something for a paper I have to submit today :scream: Can anyone help?\nI ran an experiment with RLlib for 5,000 training iterations. I'd like to write in the paper how many episodes ran per iteration, or the total number of episodes. How do I obtain this information?"}
{"question": "Hi, is there a reason why ray installs 3.0dev version by default in the setup_command when launching up clusters? I tried to change it to install ray==2.2.0 but ended up getting ray runtime not started in the cluster and a dashboard connects to it returns nothing"}
{"question": "Is the only way to submit job to a remote cluster from local machine is port forwarding? I'm trying to automate the whole process for the end user, but the port forwarding requires a new terminal and would block it."}
{"question": "hiya - have you guys seen this warnings on grpc:\n```E0217 00:03:51.720010603   93578 <http://fork_posix.cc:76]|fork_posix.cc:76]>           Other threads are currently calling into gRPC, skipping fork() handlers```\nI'm using ray 2.2.0.\nDoes anyone know how to turn this off? and does this warning matter?\nThanks"}
{"question": "Hallo everyone, I am new here. And I am new to ray. I checked the docs and some examples about ray tune for hyperparameter optimization. And I am asking myself if there is an example of how to set up a sync with a minio S3? I have tried a few things, but I am a bit stuck. There is an issue from 2018, but I think it is a bit deprecated: <https://github.com/ray-project/ray/issues/3390>"}
{"question": "Hi Folks! Does the spilling config only need to be defined for the head node (head_start_ray_commands) or worker node as well  (worker_start_ray_commands) in cluster yaml file? Is spilling config  carried over from head node to worker nodes automatically or it has to be specified explicitly for worker nodes ?\n```--system-config='{\"max_io_workers\":24,\"object_spilling_config\":\"{\\\"type\\\":\\\"filesystem\\\",\\\"params\\\":{\\\"directory_path\\\":[\\\"/data/spill_1\\\"]}}\"}'```"}
{"question": "Hello folks, I just joined this slack channel, and I have a question or maybe you can guide me through it, what would be the best way to achieve this:\nI have a lot of parquet files on AWS S3, these are the data I am going to feed into a custom gym environment,\nI used ray.data to load these parquet files into a ray.data.Dataset, and used map_batch to do some other data cleaning work.\nI have a custom gym environment, which will consume the Dataset above to find the best actions.\n\nsince the Dataset could be huge, and I need to train the agent via multiple workers to speed up the process. What is the best or correct way to pass the Dataset to the environment here?\nthe goal is to consume each row in the Dataset in each step in the environment.\n\nany suggestion is appreciated.\nthank you"}
{"question": "Hi, we are running into repeated and mysterious instances of the `kuberay` operator crashing in our k8 cluster. I found this note on the `ray-operator` README.md:\n```While updating `replicas` and `workersToDeleteUpdate` is supported, updating other fields in RayCluster manifests is **not** supported.\nIn particular, updating Ray head pod and Ray worker pod configuration is not supported. To update pod configuration,\ndelete the RayCluster, edit its configuration and then re-create the cluster.```\nWhat happens when someone inadvertantly attempts to `kubectl apply` on a ray cluster? Does it tend to cause the operator to crash?"}
{"question": "What is the old way, but on ray==2.2.0, with tune.run(), to pass env_config so that it's not overwritten when writing a custom model? (I found out PPOConfig() for instance wants it passed in .environment(env_config=env_config), otherwise, when passed to env_creator(), it gets overwritten. What is the way to pass the env_config when using tune.run() so that the dict entries are not cleared?"}
{"question": "Hi :slightly_smiling_face:\nI\u2019m considering using ray workflow for developing a computation DAG, though I need my tasks to be stateful as each task should load a machine learning model to perform some inferences. Given that requirement, ray Actor seams the right way to implement the tasks, but when I try to bind the methods performing the inference I get this error that \u201cCurrently workflow does not support classes as DAG inputs\u201d. Is there another option for me to reach the same functionality?\nany help will be very much appreciated\nhere is a code snippet that demonstrate they way I thought would work but failed\n```with InputNode() as dag_input:\n\n    class ActA:\n\n        def __init__(self):\n             # load some model\n             pass\n        def transform(self, df):\n            df['a'] = 1\n            return df\n\n    class ActB:\n        def __init__(self):\n            # load some model\n            pass\n        def transform(self, df):\n            df['b'] = df['a'] + 2\n            return df\n\n\n    \n    f = ray.remote(ActA).bind().transform\n    g = ray.remote(ActB).bind().transform\n    wf = g.bind(f.bind(dag_input['x']))\n\n    \nworkflow.run(wf, x=pd.DataFrame({'a': [2,3]},columns=['a']))```"}
{"question": "Hi! Can ray be run in scala? Has anyone tried that?"}
{"question": "Hi folks, I am spinning up EC2 instances by running `ray up config.yaml`  from the command line and I was wondering whether there was a simple way to obtain information about the instances (i.e. public IP address, private IP address, ...) after having run that command? Basically, is there a way to get the same information about the instances as when I run `boto3.client(\"ec2\").run_instances()` ?\nThanks!"}
{"question": "I see the Ray <https://docs.ray.io/en/master/index.html|docs> now say \"Ray 3.0.0.dev0\".  Where can I read more about Ray 3.0 features and timeline?\nI am in the process of building our ML platform with Ray and if Ray 3.0 is expected soon, I would like to learn more about it."}
{"question": "I am running into an odd Ray bug where I am trying to cloudpickle loads/dumps ray object refs. The cloudpickle.dumps of my data structure containing object refs return without problems. When I send this binary string to another machine and try to cloudpickle.loads,  Iget the following result:\n```  File \"/home/ubuntu/.local/lib/python3.8/site-packages/ray/util/client/common.py\", line 95, in __init__\n    self._set_id(id)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/ray/util/client/common.py\", line 186, in _set_id\n    self._worker.call_retain(id)\nAttributeError: 'NoneType' object has no attribute 'call_retain'```\nHave people seen this before?"}
{"question": "Hi guys, I am just beginning to use ray. I tried using it to download data from a gcp bucket. Here is my code to do it. I am running into some pickling errors\n\n```import download_json_from_gcp\nfrom google.cloud import storage\n\nclient = storage.Client()\n@ray.remote\ndef download_aggregated_json(gs_url):\n    json_data = download_json_from_gcp(gs_url=gs_url, storage_client=client)\n    file_name = gs_url[5:].replace(\"/\", \"_\").replace(\".jpg\", \".json\")\n    file_path = os.path.join(SAVE_DIR, )\n\n    with open(file_path, \"w\") as f:\n        json.dump(json_data, f)\n\nurls = \"<gs://test_bucket/abc.jpg>\"\nray.get(download_aggregated_json.remote(urls))```\nHere is the error I am getting . Can anyone help me with this ?\n```---------------------------------------------------------------------------\nPicklingError                             Traceback (most recent call last)\n/home/sri_overjet_ai/python-package-overjet_coco_utils/test_bed.py in line 3\n      36 # %%\n      37 urls = jsons[0]\n----&gt; 38 ray.get(download_aggregated_json.remote(urls))\n\nFile ~/python-package-overjet_coco_utils/.direnv/python-3.9.11/lib/python3.9/site-packages/ray/remote_function.py:129, in RemoteFunction.__init__.&lt;locals&gt;._remote_proxy(*args, **kwargs)\n    127 @wraps(function)\n    128 def _remote_proxy(*args, **kwargs):\n--&gt; 129     return self._remote(args=args, kwargs=kwargs, **self._default_options)\n\nFile ~/python-package-overjet_coco_utils/.direnv/python-3.9.11/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py:307, in _tracing_task_invocation.&lt;locals&gt;._invocation_remote_span(self, args, kwargs, *_args, **_kwargs)\n    305     if kwargs is not None:\n    306         assert \"_ray_trace_ctx\" not in kwargs\n--&gt; 307     return method(self, args, kwargs, *_args, **_kwargs)\n    309 assert \"_ray_trace_ctx\" not in kwargs\n    311 tracer = _opentelemetry.trace.get_tracer(__name__)\n\nFile ~/python-package-overjet_coco_utils/.direnv/python-3.9.11/lib/python3.9/site-packages/ray/remote_function.py:271, in RemoteFunction._remote(self, args, kwargs, **task_options)\n    261 # There is an interesting question here. If the remote function is\n    262 # used by a subsequent driver (in the same script), should the\n    263 # second driver pickle the function again? If yes, then the remote\n   (...)\n    268 # first driver. This is an argument for repickling the function,\n...\n    200         )\n    201     )\n\nPicklingError: Pickling client objects is explicitly not supported.\nClients have non-trivial state that is local and unpickleable.```"}
{"question": "Hello folks, I have a question about the training result of \u201cray_results\u201d folder, I have read <https://docs.ray.io/en/latest/tune/tutorials/tune-output.html#how-to-log-to-tensorboard|this link>, and used a Tuner with local_dir=\u201cmyrayresults\u201d, after training, I do see some files showed up in it, like trainable.pkl, tuner.pkl, etc. but tensorboard --logdir=myrayresults shows nothing, and I do realize that the actual tensor events are still  written to ~/ray_results. is this expected? How do I change the dir to another path, especially on Kubernetes, I am thinking to attach a PV/PVC to the ray head pod and use another pod to run tensorboard, so that I can have tensorboard port open to public. Is this achievable?"}
{"question": "Hi <@U014VUZ4Y13>  Thanks for conducting this workshop. I just registered for virtual. Will I get recorded videos of 2 days as I can't attend due to India timings?\n\nWe have deployed Ray 2.2.0 in Kubernetes . So Could you please include examples of Ray Training in Kubernetes?"}
{"question": "Hi all, is it possible to run remote tasks inside the training function of a TorchTrainer? Here is a minimal example which does not reach the `print(\u201cend\u201d)` command. So, I wonder how the train function can obtain enough resources to call the remote task `f()`.\n\n```import ray\nfrom ray.train.torch import TorchTrainer\nfrom ray.air.config import ScalingConfig\n\n\n@ray.remote\ndef f():\n    import time\n    time.sleep(1)\n    print(\"check\")\n\n\ndef train_func():\n    print(\"start\")\n    ray.get([f.remote() for i in range(10)])\n    print(\"end\")\n\n\ndef main():\n    ray.init()\n    scaling_config = ScalingConfig(num_workers=2)\n    trainer = TorchTrainer(\n                train_loop_per_worker=train_func,\n                scaling_config=scaling_config)\n    trainer.fit()    \n\nif __name__ == \"__main__\":\n    main()```"}
{"question": "hi guys - I'm using the latest ray 2.3.0.\n\nis there any public api that I can call to know the *node_ip -&gt; jobs running on that node* mapping?\nMy goal is to know the idle nodes at anytime during the execution of a program so that I can manually kill them, as I'm running into some autoscaling issue.\n\nThanks"}
{"question": "Are there any examples for running vector search engines like milvus/qdrant on top of ray ?  Has anybody tried it ?"}
{"question": "Hi guys, have anyone tried to run multi zones/cloud provider ray cluster before ? "}
{"question": "Hi all, I'm exporting application-level metrics using the provided API. However, it adds multiple useless (for us) labels to the metrics. Example:\n`ray_app_model_calls{Component=\"core_worker\", JobId=\"01000000\", NodeAddress=\"10.12.34.567\", SessionName=\"session_2023-02-27_08-13-06_509395_8\", Version=\"2.2.0\", WorkerId=\"cdd6ab7ffd1acc6027058674cdadc718a065b78ddf0ad7c6fe60b032\", instance=\"10.12.34.567:8080\", job=\"ray\", model=\"0-model/1\"}`\nThe only label added by us is `model`.\nThis results in multiple series for the same metric because the set of label is different for each run, and makes it significantly difficult to make queries.\n\nIs there a way to remove these labels?"}
{"question": "is there a specific channel/forum for `ray workflow` questions?"}
{"question": "Does `ray debug` integrate well with vscode attach debugger etc? I can\u2019t seem to get it to work."}
{"question": "Does ray supports external redis cluster? I found that the `hiredis` ray used cannot support redis-cluster: <https://github.com/redis/hiredis/issues/591>"}
{"question": "Hi all,\nI am using ray job submission API, but I have a use case that I think is not supported yet. When launching a job, I would like to add an other submission ID as an optional parameter so that the job we want to run waits the corresponding other job to be finished.\nDoes someone know if such a feature will be available in a near future ?"}
{"question": "Hello there! I noticed that each actor in Ray seems to be taking up quite a bit of memory, even the simpler ones like the Counter from the documentation. I'm interested in running multiple actors to serve my deep learning models, but the memory usage adds up quickly, especially if I need to replicate preprocessing actors to handle tasks like image resizing and reading. Do you know of any ways to reduce the memory consumption of Ray actors?"}
{"question": "Hello all, it seems like it's impossible to view the tags for ray images on docker (<https://ray-distributed.slack.com/archives/C02GFQ82JPM/p1677608677463809>) -- but for a custom image, I want to make sure that my entrypoint is the same as the ones that ray uses. Can anyone report these commands? For clusters is there a specific command that needs to be run?"}
{"question": "Hello,\nI have created a docker image for environment setup on ray cluster. I can use the image from <http://hub.docker.com|hub.docker.com> by mentioning the image in cluster YAML. What are the possible options to use a docker image for environment setup on Ray cluster other than pushing it to <http://hub.docker.com|hub.docker.com>? Any help would be highly appreciated."}
{"question": "Hi, I'm spinning up EC2 instances in different regions (us-west-2 and eu-west-2). I then launch the ray head node on one of the EC2 instances in us-west-2 and run ray start on one of the eu-west-2 instances. To do this, I use the public ip address of the head node. The eu-west-2 instance manages to connect to the head node. However, after 30 seconds I get the following error: `The node with node id: xyz and address: xzy and node name: yzx has been marked dead because the detector has missed too many heartbeats from it.`  I suspect that this has to do with the head node trying to use the worker node's private IP (and not its public IP) to communicate with it (which does not work because the instances are in different regions and therefore different private IP spaces). Does anybody have any ideas about how to solve this issue? Any help is much appreciated!!"}
{"question": "Hi\nCreate a cluster with Kubernetes API  <https://someid.us-east-2.eks.amazonaws.com/apis/cluster.ray.io/v1/>   how to send a post request to create a cluster in my Kubernetes? It only works get the method"}
{"question": "hello everyone,\n\nI'm new to Ray. Our team is planning to explore the usage. One of our initial requirements is to read zipped(.zip) csv files from S3. I tried using\n\n```ray.data.read_csv(\"s3://&lt;path&gt;\", arrow_open_stream_args={\"compression\": \"zip\"})```\nBut ended up getting\n\n```(_execute_read_task_split pid=28622) ValueError: Invalid value for compression: 'zip'```\nCan anyone please suggest if it's possible to read zip data?"}
{"question": "I have a Ray object ref. This means it must be stored in *some* object store across my cluster. I would like to know which object store it's in, i.e. get the IP address of the node that has the object ref. Is this possible?"}
{"question": "Hi! I wanted to pick brains of more experiened users. I have coded up a quick racing game, with 84x84x3 observation space, and 2 continuous actions (gas/break and steering). I am having slight issues making PPO learn to drive the car.\nEach 5 meters of the track there is a reward gate that gives 100 reward, small punishment each tick if any wheel is off the track and small reward for current speed (higher speed, higher reward).\n\nSadly I can't get past 600 reward.\n\nHere is my current PPO config, where most notibly `train_batch_size = 15_000, sgd_minibatch_size = 512` which I am not sure are fine valuest to use. One episode is from 500-2000 steps (depending on how well the agent is doing).\n\nVideo is what the agent is doing after 8 hours of training.\n\nAny tips what I might be doing wrong?"}
{"question": "Is there a way to prefix a unique ID, e.g. job submission ID to my own logging? Some of the logging is from Python and the others are from C++ executables. I'd like to leverage GCP logging for log aggregation and searches, which requires adding a unique ID to all logs from the same job.\nAlso is there a way to only redirect my own logging to stderr of the host nodes, not including internal Ray logging? `RAY_LOG_TO_STDERR=1` redirects both."}
{"question": "How can I find the default settings used when Autogluon fits a RandomForest? From <https://auto.gluon.ai/stable/_modules/autogluon/tabular/models/rf/rf_model.html|here>, I tried\n\u2022 n_estimators = 300\n\u2022 max_leaf_nodes = 150,000\n\u2022 bootstrap=True\nBut the resulting model was a lot larger that what was produced when I used Autogluon to fit on the same data."}
{"question": "What are some common causes of `ray.exceptions.LocalRayletDiedError` ?"}
{"question": "Good evening all. After a detour 6 weeks ago, abandoning Ray for lack of reliable arm64 builds, and wasting that time trying trying to get the Nvidia Triton Inference server FasterTransformer_backend to compile for the Nvidia AGX Orin Dev Kit hardware, I\u2019ve returned to the fold, now seeing nightly builds for arm64, which brings me to my question\u2026 Why no Cuda 11.4 CI builds? <https://github.com/ray-project/ray/blob/master/ci/build/build-docker-images.py#L44> I\u2019m looking through through the main docker build system and see a lo of i links back to he CI docker build code and I can appreciate the well thought out integration evident here but i\u2019s making assembly of what would oherwis be a simple package (wheel) builder container somewhat cumbersome. This is relatively self explanatory <https://github.com/ray-project/ray/blob/master/build-docker.sh#L55> and I can use this base image <https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-ml> bu I\u2019m concerned as to  h conspicuous absence of Cuda 11.4 in any of your CI builds. Any guidance?"}
{"question": "Hi all,\nI got this stacktrace when trying to use ray workflow. Anybody knows about what is happening ? It seems like it is an issue with RAY workflow logger that I don't get when using classical ray.get(ray.remote)...\n`Exception in thread ray_print_logs:`\n`Traceback (most recent call last):`\n  `File \"C:\\Users\\Leonard.Caquot\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 980, in _bootstrap_inner`\n    `self.run()`\n  `File \"C:\\Users\\Leonard.Caquot\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\", line 917, in run`\n    `self._target(*self._args, **self._kwargs)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 799, in print_logs`\n    `global_worker_stdstream_dispatcher.emit(data)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\_private\\ray_logging.py\", line 317, in emit`\n    `handle(data)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 1663, in print_to_stdstream`\n    `print_worker_logs(data, print_file)`\n  `File \"C:\\Users\\Leonard.Caquot\\PycharmProjects\\ai-developments\\venv\\lib\\site-packages\\ray\\_private\\worker.py\", line 1769, in print_worker_logs`\n    `print(`\n  `File \"C:\\Users\\Leonard.Caquot\\AppData\\Local\\Programs\\Python\\Python39\\lib\\encodings\\cp1252.py\", line 19, in encode`\n    `return codecs.charmap_encode(input,self.errors,encoding_table)[0]`\n`UnicodeEncodeError: 'charmap' codec can't encode characters in position 160-164: character maps to &lt;undefined&gt;`\n`Getting project's metrics and dimensions`"}
{"question": "Hello, I was wondering if there  is a way to associate a Worker (process) with a  specific GPU, so any Task will use that GPU and that GPU alone?"}
{"question": "You mean pining it to a specific GPU?"}
{"question": "Good afternoon. I was finally able to compete a successful Ray build on Arm64v8 with Cuda11.4 bu I don\u2019t see anywhere documented, the correct method to assemble he built artifacts into a wheel. I assume i\u2019s in one of h ci Dockerfiles but I\u2019m just not seeing it. Can someone point me to the right place? While I\u2019d expect he release directory to contain h relevant items, i appears o contain demo code a version 0.001 upon examination of setup.py,  Please advise\u2026 EDIT: looks as though I may have found it at the ed of a command line example, normally scrolled out of visibility <https://github.com/ray-project/ray/blob/master/python/README-building-wheels.md>"}
{"question": "Hi, I'm trying to deploy a cluster with my own image. However, I keep getting error `NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.` even it's a cpu image and the instance is a cpu instance. Any idea?"}
{"question": "What is the recommended method for tracking a running job? For example, let's say I have a training job (not with Tune) and want to keep track of how many epochs have run, what is the best practice here?"}
{"question": "hi folks!\ncurious - does ray have a collection of project improvement proposals/designs? (similar to SPIP for spark and others)\ni've found a ray v2 architecture <https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview>"}
{"question": "If I have a function like this:\n```def my_funtion(txt_input):\n    result = whatever I want to do with this txt_input\n    return result```\nI want to run this function 1000 times for 1000 different inputs on a ray cluster with 1 head node and 5 worker nodes each having 16 CPUs. How can I do that?? All the examples I find in the document using `remote` are still running using `ray.init()` which is still \"local\". I want to run this on a cluster. Does any one has \"working\" example?"}
{"question": "is there a way to programmatically check if a Ray actor is referenced by something?\nfor example, if i have a detached actor using as \"global\" var, if I wanna kill it, I wanna make sure nobody is using it"}
{"question": "Hello : ) I\u2019m wondering if there is a better way to select specific columns from Ray Datasets, or is converting to Pandas the only way right now? (assuming I can\u2019t use 2.2+)"}
{"question": "OOM prevention - Can someone clarify. The following doc (<https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html?highlight=oom>) states we can use `RAY_memory_monitor_refresh_ms` but when I set it using the `runtime-env-json`, the log shows:\n```WARNING runtime_env_agent.py:298 -- runtime_env field RAY_memory_monitor_refresh_ms is not recognized by Ray and will be ignored.  In the future, unrecognized fields in the runtime_env will raise an exception.```\nUsing Ray 2.3.0"}
{"question": "I am new in Ray.  I tried the examples listed in the  ray-project  on  github.  Unfortunately, it is hard for me to complete even one experiment so far.  For the simplest  example 1  in project  <https://github.com/ray-project/ray-educational-materials/blob/main/Ray_Core/Ray_Core_2_Remote_Objects.ipynb|A Guided Tour of Ray Core: Remote Objects> ,  when  I increase the number of  tasks, say, from 100 to 300  and run it in a single head node with 128 CPUs  on a CentOS ,  the system will be halted and  there will many  tasks failed. From the log it seems the raylet died but  no way  to recover from the crash ?"}
{"question": "Anyone received notification on if their abstract got accepted for Ray Summit yet? Just trying to plan out my year and want to make sure I didn't get a spot."}
{"question": "Hey all, I am struggling with a long-running experiment. I have my own environment that produces observations, and I am training a `DQN` agent on it. My last run crashed with the following error messages. I _think_ it might be killed by the OS due to a steady increase in memory consumption (which I can't explain, there might be a memory leak somewhere). I am using Ray 2.3.0, TF 2.10.\n\nDoes anyone recognize this error message:\n\n```  File \"/Users/seba/TUHH/i3/simulator/sim_multi_channel__new.py\", line 87, in train\n    iteration_results = trainable.train()\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 368, in train\n    raise skipped from exception_cause(skipped)\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 365, in train\n    result = self.step()\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 782, in step\n    results, train_iter_ctx = self._run_one_training_iteration()\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 2713, in _run_one_training_iteration\n    results = self.training_step()\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn.py\", line 472, in training_step\n    self.workers.sync_weights(global_vars=global_vars)\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 422, in sync_weights\n    self.foreach_worker(\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 692, in foreach_worker\n    remote_results = self.__worker_manager.foreach_actor(\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 583, in foreach_actor\n    remote_calls = self.__call_actors(\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 422, in __call_actors\n    calls = [self.__actors[i].apply.remote(func) for i in remote_actor_ids]\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py\", line 422, in &lt;listcomp&gt;\n    calls = [self.__actors[i].apply.remote(func) for i in remote_actor_ids]\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/actor.py\", line 138, in remote\n    return self._remote(args, kwargs)\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py\", line 425, in _start_span\n    return method(self, args, kwargs, *_args, **_kwargs)\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/actor.py\", line 184, in _remote\n    return invocation(args, kwargs)\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/actor.py\", line 171, in invocation\n    return actor._actor_method_call(\n  File \"/Users/seba/.local/share/virtualenvs/simulator-w6EsBzyq/lib/python3.10/site-packages/ray/actor.py\", line 1169, in _actor_method_call\n    object_refs = worker.core_worker.submit_actor_task(\n  File \"python/ray/_raylet.pyx\", line 2164, in ray._raylet.CoreWorker.submit_actor_task\n  File \"python/ray/_raylet.pyx\", line 2169, in ray._raylet.CoreWorker.submit_actor_task\n  File \"python/ray/_raylet.pyx\", line 425, in ray._raylet.prepare_args_and_increment_put_refs\n  File \"python/ray/_raylet.pyx\", line 416, in ray._raylet.prepare_args_and_increment_put_refs\n  File \"python/ray/_raylet.pyx\", line 509, in ray._raylet.prepare_args_internal\n  File \"python/ray/_raylet.pyx\", line 1780, in ray._raylet.CoreWorker.put_serialized_object_and_increment_local_ref\n  File \"python/ray/_raylet.pyx\", line 1669, in ray._raylet.CoreWorker._create_put_buffer\n  File \"python/ray/_raylet.pyx\", line 209, in ray._raylet.check_status\nray.exceptions.RaySystemError: System error: No such file or directory```\nThe reason I suspect out-of-memory is because I've been struggling with this issue for a while now. I don't know why Ray would want to look for a file during training, unless the OS took away its memory allocations during `kill`?"}
{"question": "Hi I wonder to know which technique does the ray used to implement the communication between nodes? Socket or grpc or something else?"}
{"question": "Hi, is. there a way to force submit job to upload the built wheel as runtime env? I'm building my own package as a wheel and pass it as `py_modules` in `runtime_env` when submitting job. However, gcs will skip uploading my package after the first time even though I updated my package source code and rebuilt it."}
{"question": "*Node change state to DEAD but num_alive_node is 0*\n\nMy EKS cluster has the nodes up for an 1 hr but the current Ray Task log shows the above message.\n\nHow does one interpret this? Is there any documentation one can read?"}
{"question": "Hi, is there any API to sync files between ray cluster? I only found an api for ray tune. Wonder is there a general one if I'm not using ray tune"}
{"question": "Hi all, just getting started with Ray (writing a book and covering it in one of the chapters). I\u2019m getting inconsistent behaviour for the dashboard being available dependent on whether I pip install ray[default] (dashboard is created and address is given) vs pip install ray[air] or ray[tune] etc ... anyone else seen this? Same thing as in here but only solution was to install ray[default], which is not what I want? <https://github.com/ray-project/ray/issues/24051>"}
{"question": "Hi Folks! Does ray.autoscaler.sdk.request_resources support autoscaling on custom_resources ? or only gpu and cpu are supported ? <https://docs.ray.io/en/latest/cluster/running-applications/autoscaling/reference.html#ray-autoscaler-sdk-request-resources>"}
{"question": "Hi all, Looking through the <https://docs.ray.io/en/latest/ray-core/api/doc/ray.runtime_env.RuntimeEnv.html|RuntimeEnv> docs for `ray.remote(runtime_env=RuntimeEnv(...))`, the `container` kwarg seems to take registered docker images. Does this work for locally-built docker images as well? I wasn\u2019t able to get this running for locally-built images, so wondering if there\u2019s something I\u2019m missing here."}
{"question": "How to start Ray Actor with numa config? Thanks for any help"}
{"question": "Hi, I can't get py-spy to work in a kuberay cluster (Ubuntu).\nI tried to log directly inside the worker and launch the commands in the pod:\nWhat did I miss? Anyone got the same problem?"}
{"question": "does anyone know of any examples that use ray to train a yolo model?"}
{"question": "Hey Folks! Are there ways to execute some custom clean up script code when we tear down a VM based cluster through \"ray down cluster.yaml\" command ?"}
{"question": "does anyone know whether ray was also involved in training gpt4?"}
{"question": "hi guys - do you know what could trigger the error below? It feels like OOM but I looked at all the logs under /tmp/ray both at head and worker node, but I can't pinpoint the error.\nThis is running on remote aws cluster tuning params\nAny help?\n\n```Failure # 1 (occurred at 2023-03-19_20-49-19)\nTraceback (most recent call last):\n  File \"/home/ubuntu/lib/python3.8/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1276, in get_next_executor_event\n    future_result = ray.get(ready_future)\n  File \"/home/ubuntu/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ubuntu/lib/python3.8/site-packages/ray/_private/worker.py\", line 2382, in get\n    raise value\nray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n        class_name: MyTrainable\n        actor_id: 9cce77b7330c4b55720b174e03000000\n        pid: 3120\n        namespace: 981a50a9-c3ed-424e-adea-b39bc6486396\n        ip: 10.28.64.48\nThe actor is dead because its node has died. Node Id: 21803fda2d5a0410678d1d098b0a5c082f131aacafb1ea92ed2cecc4```\n"}
{"question": "Hi, Suppose I have a model which takes 3D images as input on GPU.  Technically, I can not know the the size of the 3D images ahead and there may be a big variance.   Do I need to specify a memory usage with a very big  size to deal with all the possible cases to prevent the OOM?    Say, for a  12G mem GPU , now only  6G memory left, and the incoming 3D image    will take 7G mem to work.   The ray will schedule  this inference until the 6G mem is release or  just let OOM happen?"}
{"question": "Hey all :wave: , I try to wrap my head around <https://github.com/ray-project/xgboost_ray#scikit-learn-api|xgboost_ray sklearn integration>. Will it emit multiple models under the hood as referred to in the general docs of Ray <https://docs.ray.io/en/latest/ray-overview/workloads.html#distributed-training-of-large-models> ?"}
{"question": "Hello all -- I am running into an issue where when I leave my ray cluster running the head node uses up too much local storage (I am running on kuberay). Is there any reason it is using up so much storage? The limit is 10GB, and I can add a persistent volume but then I would need to change the storage location -- how do I do this?"}
{"question": "Hi guys, I have a question about the example in this article: <https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray>, specifically this point: `Building microservices and actors that have state and can communicate`\n\nHere is the code snippet from the article, under \u201cRemote Objects as Actors\u201d section. Why can\u2019t this be achieved by using normal tasks? I don\u2019t see any \u201ccommunication\u201d of the state between the two actors, but instead, we actually get the result of the two actors separately and get the intersection in the main process (the process that initiated the actors).\n```    gsod_y1 = GSODActor.remote(year1, high_temp)\n    gsod_y2 = GSODActor.remote(year2, high_temp)\n\n    ray.get([gsod_y1.load_data.remote(), gsod_y2.load_data.remote()])\n    y1_stations, y2_stations = ray.get([gsod_y1.get_stations.remote(),\n               \t                    gsod_y2.get_stations.remote()])\n\n    intersection = set.intersection(y1_stations, y2_stations)```\n I\u2019d like to understand the actor concept better, is there any materials/exercise that I can try it out myself? Thanks a lot!"}
{"question": "Hey all - newbie Ray user trying to understand the best architecture for parallelizing my ML pipeline with Ray:\n\nI have a model already being served at a non-Ray gRPC endpoint. The model pipeline looks like this:\n```Input: PDF document in S3\nHigh-level ML process:\n1. Download PDF, convert first 4 pages to byte array\n2. Run Layout Analysis and OCR for each page (finds text within certain blocks)\n3. Tokenize text\n4. NER to extract certain fields\n5. Return JSON of extracted fields```\nThe whole process takes ~35 seconds for 1 doc. Step 2 takes the most time (about 25 seconds) so we are hoping to parallelize each page being processed (4 total).\n\nA few questions:\n1. I noticed that it now takes about 40 seconds to initialize the singleton predictor class; is that expected? \n2. Given we're only processing 4 images at a time, should I be looking to do this with Tasks or I see there's the mapping Actor inferences to the data? Not sure which methods are best practice for these cases / will most optimize our runtimes.\n"}
{"question": "my nodes are not scaling down even well after my job completes. the idle timeout is only 1 minute and they are still there an hour later --- I don't want to waste cloud $$$, any idea what could be causing this? what are the actual conditions to be tagged \"idle\" ?"}
{"question": "Hey Folks! How many nodes can a Ray cluster scale up to (using VMs) ? Has any benchmarking been done so far ?"}
{"question": "Hey when installing ray on k8s, do you guys use helm or kustomize? And any advice on which approach to take? Thanks."}
{"question": "how can I forward all environment variables to workers? particularly AWS credentials. it seems I can access my s3 buckets from the head node but not from any workers"}
{"question": "Hi, quck question. Can I specify tune hyperparameters in `.yaml` file?"}
{"question": "Hey what\u2019s the difference between the runtime_env in submit_job\n```client.submit_job(\n    # Entrypoint shell command to execute\n    entrypoint=\"python script.py\",\n    # Path to the local directory that contains the script.py file\n    runtime_env={\"working_dir\": \"./\"}\n)```\nand the one ray.init()?\nIt seems that it only installs the dependencies if I add the requirements.txt in the `submit_job` .\n\nI also see that you can specify the dependencies for each task and actors: <https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#runtime-environments>\n\nCan you help with clarifying the three different ways of specifying dependencies and who takes precedence? Thanks"}
{"question": "I\u2019m getting `Error: No available node types can fulfill resource request {'CPU': 3.0}. Add suitable node types to this cluster to resolve this issue.` Is there a way to stop this job? And also is there a way to add more resources adhoc?"}
{"question": "Hey guys, when I followed this guide: <https://docs.ray.io/en/latest/ray-more-libs/joblib.html>, I noticed that actors are created per task. I\u2019m wondering how should the proper resource request be?\nThe task is stuck with this message `Warning: The following resource request cannot be scheduled right now: {'CPU': 2.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.`\n\n```@ray.remote(num_cpus=2)\ndef train_model_for_one_customer_distributed_sklearn(customer_id, customer_df, start_date, end_date, model_version=1):\n    ... training code .....\n\n  \n    from ray.util.joblib import register_ray\n    register_ray()\n    with joblib.parallel_backend('ray', ray_remote_args=dict(num_cpus=2)):\n        model.fit(X_train.values, y_train.values)\n    dump(model, '{}/{}.joblib'.format(model_path, customer_id))\n    ```\n My questions:\n1. Can you help clarify different places where you can specify resources? How does the @ray.remote(num_cpus=2) and the ray_remote_args=dict(num_cpus=2) work together?\n2. `Consider creating fewer actors or adding more nodes to this Ray cluster` Does each actor needs its own node? Is there a guide on how should I configure ray cluster base on my need? "}
{"question": "@ray.remote(num_cpus=1)\ndef my_task():\n       pass\n\nIn above code, each task will own one logic cpu.   Suppose I have 16 CPUs ,  is there any way to only  run 4 such tasks  simultaneously even I have launched 50 such tasks?\nThanks"}
{"question": "Hello, is there any timeline for python 3.11 - arm64 whl package for apple silicon ?"}
{"question": "Hi, we train Pytorch models with the model and loader wrapped in Ray and the same hyper parameters as without Ray, but receive significantly worse performance of the models after the same amount of training iterations (with the same random seeds). Do we need to re-tune our hyper parameters for the Distributed Training rather then expecting it to be similar or should they be more similar?"}
{"question": "anyone know if there\u2019s any plans to create distributed Trainers for scikit-learn? It says <https://docs.ray.io/en/latest/ray-air/trainer.html#scikit-learn-trainer|here> that \u201cThis trainer is not distributed\u201d. Maybe this can be done by using SGD for those Estimators that support it"}
{"question": "Hey guys, I\u2019ve been exploring ray for a couple of days, it\u2019s a very extensive framework, there are quite some different ways to achieve the same thing.\nI just found out about SklearnTrainer, how does it relate to using just normal task to train a model, which one should I choose? <https://docs.ray.io/en/latest/train/api/doc/ray.train.sklearn.SklearnTrainer.html>"}
{"question": "Hi, I am using Ray train with Tensorflow. I make the training distributed on 4 workers (thus 4 processes). When one worker (PID) is killed, I notice that the other three PIDs are also killed and later 4 new PIDs are started for resuming training. What I would like to do is to make the training resumed with the existing 3 PIDs not killed, so only one PID is renewed. I\u2019m wondering if it is possible off the shelf, if not, can someone give me some suggestions regarding how I can achieve that? Thanks in advance!!!"}
{"question": ":wave: Hey all -\n\nI noticed that :ray: `ActorPool.map` seems to return results in reverse order. Is there documentation to confirm this is always the case (or how mapping result order is determined)? Here is the code I'm using\n\n```# Create async Ocr and LayoutAnalysis actors to execute OCR and layout analysis processes\nocr_actors = [Ocr.remote() for i in range(4)]\nlp_actors = [\n    LayoutAnalysis.remote(self.layout_analysis_model) for i in range(4)\n]\n\n# Gather actors into an ordered pool and distribute tasks to each\nactor_pool = ActorPool(ocr_actors + lp_actors)\nactor_pool_mapping = actor_pool.map(\n    lambda a, d: a.process.remote(d), jpgs_buffer[:4] * 2\n)\n\n# ActorPool mapping synchronously pulls result futures in reverse order\nresults = list(actor_pool_mapping)\nlp_results = results[:4] # I expect lp_results to be results[4:] given I passed that list of Actors to the pool 2nd \nocr_results = results[4:] # I expect ocr_results to be results[:4] given I passed that list of Actors to the pool 1st ```"}
{"question": "is there a workflow channel for ray.workflow specific questions ?"}
{"question": "Has anyone tried using Ray object store for Key Value Pair? I have a usecase where I am using Ray Serve to host APIs. Internally I call multiple other services and requires caching of the same data as the data is not going to change. Can I use ray object store for caching or do I need a dedicated cache like Redis? I am okay with 10ms-20ms latencies."}
{"question": "Kudos on this dashboard overview video. One of the best intros that I've seen. A++ <https://www.youtube.com/watch?v=VPksEcjACOM>"}
{"question": "Hey folks! `snakeyaml` CVE is rearing its ugly head again. This is preventing us (and probably many other folks) from deploying.\n\nRay currently depends on snakeyaml 1.33. However, NIST describes a CVE (<https://nvd.nist.gov/vuln/detail/CVE-2022-1471|CVE-2022-1471>) that <https://bitbucket.org/snakeyaml/snakeyaml/wiki/Changes|was fixed in snakeyaml 2.0>, which appears to be a potentially backwards incompatible change.\n\nWhat would it take for us to upgrade snakeyaml to 2.0 in Ray? Or, what would break if we patched it to snakeyaml 2.0 ourselves?"}
{"question": "Does anyone have a way to get GCP logs to work correctly inside Ray workers? I usually use jsonlogger for all other tools, but it looks like per-process logs have their own internal logger and I haven't found a way to override it. I want to override it because all of Rays logs have severity=ERROR"}
{"question": "Hey guys, about resource management, I read <https://docs.ray.io/en/latest/ray-core/scheduling/resources.html#resource-requirements|here>\n&gt; \u2022  Resource requirements of tasks or actors do NOT impose limits on actual physical resource usage. For example, Ray doesn\u2019t prevent a `num_cpus=1` task from launching multiple threads and using multiple physical CPUs. It\u2019s your responsibility to make sure tasks or actors use no more resources than specified via resource requirements.\n&gt; \n1. How can I do that? I\u2019m facing OOM issue with a very simple RayActor, each of them has a method to train a sklearn random forest model. Would you recommend using Modin instead of pandas? Or should I try the ray object store? \n2. Even if the physical resource of a task/actor is not limited by ray, it\u2019s still limited to the amount of resources of the ray node (k8s pod). Does it mean if some existing task and actors are using excessive amount of resource, it can prevent more tasks/actors being scheduled? Is there a way to limit the amount of physical resources? \n3. In <https://docs.ray.io/en/latest/ray-core/patterns/limit-running-tasks.html#core-patterns-limit-running-tasks|this> page, there is the option to specify memory requirement, so can I understand it as having a memory request but no memory limit if you compare it to the k8s resources?\n```result_refs.append(\n        process.options(memory=2 * 1024 * 1024 * 1024).remote(f\"{i}.csv\")\n    )```"}
{"question": "Hey guys, is there anywhere a good documentation on the Rays Datasets implementation internals. Its magic, which works fairly well, but the issue that we are getting into is resource consumption. In unfrtunate case fo de duplication, we have to have all of the data in memory, we can't apply windowing. We are using map_batches, to localize most of the processing, but have not found the way to effectively control the amount of executors. We can use actors for this (sort of), but this seems like an unnnecessary overhead. Any help here will be appreciated. Also is there a way to better control shuffle resources? Any suggestions on best ways to implement de duping of docs with Ray datasets will be appreciated"}
{"question": "Is ray.get() required in the driver process in order for the tasks/actors to execute the remote method? Since my driver doesn\u2019t really need to access the result of the tasks/actors, the result of those will be for example writing the trained model to s3 bucket. When I don\u2019t include the ray.get method, the tasks don\u2019t seem to be executed. What would you recommend in this case?"}
{"question": "We have a pipeline on Ray and use pub/sub to submit jobs to the pipeline. Is there a good way to rate limit pulling pub/sub messages based on Ray cluster resources available? We also have kubeRay auto-scaler enabled but the k8s cluster has limited amount of resources."}
{"question": "A simple survey question for dashboard users. Thanks for your attention!\n\u2022 *Has anyone used the toolbar (search, time filter, etc.) in the dashboard log UI before? If yes, why did you use them?*"}
{"question": "It seems Ray could schedule tasks onto dying nodes `due to error:Task failed due to the node dying` . With KubeRay auto-scaler disabled I no longer see this error. Is this expected?"}
{"question": "For an actor,\n@ray.remote(num_cpus=0)\nclass  MyActor:\n      ...\n\n\nI can not figure out the outcome of num_cpus=0 because I can run it as usual.  Anybody can explain some ?"}
{"question": "Also is there a way to allocate more memory to Object Store when starting Ray on VMs?"}
{"question": "Hey guys when you submit a ray job like this, with a entry point script, a driver I guess is what\u2019s called in ray, which starts the remote functions, where does ray schedule the entry point script? Is it a task itself?"}
{"question": "Hello, I\u2019m trying to rsync-up a file from my laptop to a remote cluster (Ray on EC2), but I get the following error:`Error: SSH command failed.`  I am, however, able to ray attach and ssh into the cluster. Why is SSH working with `ray attach`  but not with `ray rsync-up` ? Or is it some other issue?"}
{"question": "Hey guys, if I write a file in a ray task, where can I find that file? I tried to ssh into the ray node (k8s pods) but couldn\u2019t find it. Also what is the best practices to export artifacts to a s3 bucket for example?"}
{"question": "I've got two quick questions:\n\u2022 it looks like by default Ray's worker processes run at a lower priority.  Not a big deal but at least one person looked at htop and said \"why is everything blue?\"  Any easy way to change that?  (or should I just tell people that lower priority doesn't really mean anything when we have 192 CPUs wandering around)\n\u2022 I'm using the Ray core API and `.remote()` calls to create futures and then aggregate them and whatever.  Is there a `.map()` API somewhere similar to Python's multiprocessing that I can use?  Would save me a lot of bookkeeping."}
{"question": "why is `ray job submit` preferred over `ray submit` ? is there a way to specify a `working_dir` when using just `ray submit` ?"}
{"question": "and follow-up, if I have some code I want to import in a module `utils`, right now I did something very ugly which is like\n```import ray\nimport other, libraries, here\n\nruntime_env = {\n    \"working_dir\" : \"/path/to/utils\",\n}\nray.init(runtime_env=runtime_env)\n\n@ray.remote\ndef foo():\n    import utils # imported INSIDE THE REMOTE CALL\n\nif __name__ == '__main__':\n    foo()```\nbut I really hate the import inside `foo`, but if I put that up with the other imports I get `ModuleNotFoundError: No module named 'utils'`. how can I approach this problem?"}
{"question": "is there a way to `breakpoint()` into jobs or the like? I'm having some flaky failures accessing files in a bind-mounted `/fsx` where sometimes the jobs can find the files and sometimes they can't. `os.path.exists` is totally unreliable here, and I just want to see what's going on interactively"}
{"question": "anyone using TPUs here?"}
{"question": "if a ray task is only allocated some finite resources like `@ray.remote(num_cpus=1)` but it calls something inherently multithreaded like some BLAS call or a C++ binary via `subprocess.check_call`, what will happen? will it be restricted to just the 1 cpu? what if it's trying to \"auto-detect\" cpu count like in sklearn's `n_jobs=-1`. will it use the cpu count on the bare node or the logical cpus that Ray has allocated?\n\nalso, I know I'm asking a lot of questions here. if there's a better channel for this stuff like <#CMVUQ1NSV|random> or #help or something let me know"}
{"question": "Hi Folks! I have  a long running task across 20+ nodes. I find the disk root (ebs mounted) usage keeps creeping up across all nodes. In 4 hours it hits 60 % of 1 TB. I have the object spilling configured to a different partition (instance store) so the disk is not being used by the object store. df reports the increased disk usage but du fails to find anything significant.  I notice lot of deleted open files and this may be the reasons for the disk usage: lsof | grep -i deleted\n raylet    19250             ec2-user   11u      REG               0,21 77116735496          3 /dev/shm/plasmayLGEPa (deleted)\nraylet    19250             ec2-user  440u      REG              202,1 10182762504  939524237 /tmp/ray/plasmatpsSwl (deleted)\nraylet    19250             ec2-user  446u      REG              202,1 10182762504  939524234 /tmp/ray/plasmawwKo0i (deleted)\nraylet    19250             ec2-user  447u      REG              202,1 10182758408  939524235 /tmp/ray/plasmaPD8qWO (deleted)\nraylet    19250             ec2-user  448u      REG              202,1 10182766600  939524236 /tmp/ray/plasma5x0BZk (deleted)\nraylet    19250             ec2-user  462u      REG              202,1 10181189640  939524238 /tmp/ray/plasmaoooTq6 (deleted)\nIf I run the task for 3 more hours it will cause the machines to run out of disk space. This pretty much becomes a blocker for executing long running tasks. Is this a bug in ray ? Is there any workaround ?"}
{"question": "HI Folks! I have a long running task across multiple nodes. The job succeeds most of the times but sometimes one reduce task gets suck in waiting for scheduling : node assignment state (just 1 CPU needed), even when there is nothing else running on the cluster (and no resource demands). How do I debug this scheduling hang ? Is there any workaround to detect stuck tasks waiting on scheduling and taking some corrective action in the driver code ? Right now I am manually resubmitting the whole job after cleaning up any output, but this won't fly in a production setting."}
{"question": "Hello <@U02P5FKRT9U> do you know when this feature was introduced? I believe that in the past this environment was not setted. It appears to be controlled by this function (<https://github.com/ray-project/ray/blob/a2259b62aab30fba53a85875a8f1d7b5f74d52ed/python/ray/_private/utils.py#L329>)"}
{"question": "How different framework import work in RLib. How its implemented? \n\nI can see in RLlib GitHub repo that there is entirely different logic for both tensorflow and pytorch in their respective subfolders.\n\nI wanted to know how RLlib selects and imports the framework when we provide framework in config.\n\nI want to know this because I wanted to use IVY(<https://lets-unify.ai/|https://lets-unify.ai/>) functional API and create another subfolder for IVY so that we can use RLlib with any machine learning framework which IVY support."}
{"question": "Hi, anyone can help me understand the actions for  local scheduler and global scheduler ?    In Paper, \"Ray: A Distributed Framework for Emerging AI Applications\" , the section 4.2.2 Bottom-up distributed scheduler,  it says, \"To avoid overloading\nthe global scheduler, the tasks created at a node are submitted\nfirst to the node\u2019s local scheduler. A local scheduler\nschedules tasks locally unless the node is overloaded\n(i.e., its local task queue exceeds a predefined threshold),\nor it cannot satisfy a task\u2019s requirements (e.g., lacks a\nGPU). If a local scheduler decides not to schedule a task\nlocally, it forwards it to the global scheduler\"\n\n From this statement, it is possible the task's scheduling won't bother the global scheduler as long as it can be satisfied locally.    But in the section 4.3 Putting everything together ,   with example add(), it says: The driver submits\nadd(a, b) to the local scheduler (step 1), which forwards it to a global scheduler (step 2).\n\nIn this example, the add task will be forwarded to the global scheduler directly or  just because it can not be satisfied locally ?\n\nThanks for your time."}
{"question": "Has anybody had any luck installing specific versions of Ray using the commit SHA? I tried following <https://docs.ray.io/en/master/ray-overview/installation.html#installing-from-a-specific-commit|this section> but the example url (<https://s3-us-west-2.amazonaws.com/ray-wheels/master/ba6cebe30fab6925e5b2d9e859ad064d53015246/ray-3.0.0.dev0-cp37-cp37m-macosx_10_15_intel.whl>) doesn\u2019t download anything.\n\nThis is an example of what I was trying: <https://s3-us-west-2.amazonaws.com/ray-wheels/master/ba0309536f14248b0d97ddadc3982b53aba0cfd7/ray-2.1.0-cp38-cp38-manylinux2014_x86_64.whl>\n\nAny help would be greatly appreciated! I am trying to do a git bisect of Ray because I think there might be a performance regression between Ray 2.1.0 and 2.2.0."}
{"question": "is there a Ray container somewhere with a more recent (or granular options for) version of `GLIBC` ? I'm using the nightly py3.9 and looks like it's on\n```ldd (Ubuntu GLIBC 2.31-0ubuntu9.9) 2.31```\nbut my app requires `2.34`"}
{"question": "hey! Is it possible to partition a Dataset based on a specific column? For example, let\u2019s say I have a dataset with two columns: id, value. I want to run a task for each of the ids. Is it possible to do that? (This is similar to `repartition(n, 'id')`  in pyspark)"}
{"question": "Hi, I have a issue about the log of loading checkpoints from HDFS path.\n```def train_func():\n    file_path = \"<hdfs://10.1.0.133:9000/ray_checkpoints_error/>\"\n    try:\n        checkpoint_dict = Checkpoint.from_uri(file_path).to_dict()\n    except Exception:\n        print(\"Training from scratch.\")\n        pass\n\nruntime_env = {\n    \"env_vars\": {\n        \"JAVA_HOME\": os.getenv(\"JAVA_HOME\"),\n        \"ARROW_LIBHDFS_DIR\": os.getenv(\"ARROW_LIBHDFS_DIR\"),\n        \"CLASSPATH\": os.getenv(\"CLASSPATH\"),\n    }\n}\nray.init(address=\"auto\", runtime_env=runtime_env)\ntrainer = TorchTrainer(\n    train_func,\n    scaling_config=ScalingConfig(\n        num_workers=2,\n    ),\n)\ntrainer.fit()```\n`from_uri()` is put in `train_func()`, sometimes the log is normal:\n&gt; Training from scratch.\n&gt; hdfsOpenFile(/ray_checkpoints_error/.metadata.pkl): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:\n&gt; Training from scratch.\n&gt; hdfsOpenFile(/ray_checkpoints_error/.metadata.pkl): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:\nBut sometimes the log is not as expected. How can I prevent it from logging the error `RemoteException`?"}
{"question": "Do I need to do anything special for very long-running jobs on Ray?\nFor example: I have some code that spawns one very long running worker function for each GPU in a one-node cluster to do batch inference on data.\nHowever, whenever I do this on a 2 GPU cluster, I notice that initially, both workers are running, but after a while, one of the workers stops running for no discernible reason.\n```@ray.remote(num_gpus=1)\ndef worker(rank_idx, n_gpus, cfg):\n    import torch\n    model_path = cfg.local_checkpoint_path\n    model = # some code\n\n    np_kwargs = {}\n    # some very long running inference code\n\n    temp_file = cfg.temp_dir / f\"rank_{rank_idx}.npz\"\n    print(f'Saving: {temp_file}')\n    np.savez(temp_file, **np_kwargs)\n    return (rank_idx, temp_file)\n\ndef embed_parallel(n_gpus, cfg):\n    from distributed_ml.logging.structlog import structlog_logger\n    l = structlog_logger(log_level=logging.DEBUG).bind(tag=\"worker\")\n\n    workers = [worker.remote(rank, n_gpus, cfg) for rank in range(n_gpus)]\n    all_results = ray.get(workers)\n\n    all_results.sort(key=lambda x: x[0])\n    np_kwargs = {}\n    # code to recombine temporary numpy files\n    np.savez(cfg.local_dest_path, **np_kwargs)\n\nif __name == \"__main__\":\n     embed_parallel()```"}
{"question": "Hi, what is the recommended way to deploy Ray Serve application in CI/CD workflows?"}
{"question": "I'm trying to start ray in the background via my Dockerfile. I believe this should save time when `ray.init()` is called when invoking my script. I run\n```# Start ray head node for parallelization of tasks\nRUN ray start --head --port=6379 --dashboard-host \"0.0.0.0\"\nEXPOSE 8265```\nBut I don't see that ray is running when I create the container. I also notice that, on a long running container, the Ray dashboard gets killed. Is there a way to start ray in the Dockerfile and keep it running?"}
{"question": "Is it possible to set resources for each worker indivudually? I have environment that can run accelerated on GPU or slow on CPU and I would like to specify that 3 rollout workers should be allocated with GPU and the rest (let's say 40) should be allocated on CPU."}
{"question": "I guys,\nDoes someone know what is the best way to use a private ssh key inside a ray cluser ? Here is my problem:\n\u2022 My program uses a ssh_tunnel with a ssh_pkey located in my personnal files on my computer.\n\u2022 I can run my program using local ray instance as it also has access to that private key\n\u2022 But when I try to run my code on a remote ray cluster (using docker) it does not work because it there is no private key on that cluster\nWhat should I do ? Share my personnal pkey (maybe the same way as sharing the whole project with the cluster using working_dir = ./ in ray.init()) ? Or directly create or add a private key in ray cluster when lauching docker ?\nAny idea ? Thanks"}
{"question": "Hi guys how can I configure service account to access my google cloud storage bucket from ray worker? I\u2019m installing ray on google kubernetes with helm. \n\nI also asked here, it seems the answer in this post is outdated. <https://discuss.ray.io/t/google-cloud-storage-access-from-worker/1899/9?u=y_c|https://discuss.ray.io/t/google-cloud-storage-access-from-worker/1899/9?u=y_c>"}
{"question": "Hi everyone. I am trying to build a fault tolerant app with ray using deployments but I have some questions I cannot answer by reading docs. Why there are no max_task_retries or max_restarts options for deployments? I couldn't find anything in the docs and it seems like max_task_retries is set to 0 for deployments. Is there way to retry requests to deployments? And what are the advantages of using deployments instead of actors?"}
{"question": "Link to the same topic on discuss.ray <https://discuss.ray.io/t/xgboost-ray-object-creation-and-spilling-bottleneck/10090|XGboost-Ray Object Creation and Spilling bottleneck>\nHello Ray Community,\nWe are training XGboost ray model but facing following issues, would be great if someone can guide us. We are using ray 2.3.1 on python 3.9.16\n*Dataset*\n1. train: 35GB parquet, 250GB on memory\n2. validation: 35GB parquet, 250GB on memory\n*Context:*\n1. Machine specification: 1TB RAM, 128 CPUs, 3.78TB local NVMe (SSD PCIe Gen4).\n2. We are using Ray Datasets instead of RayDMatrix based on this Ray documention.\n3. Codeflow is as below fashion:\n```ray.init(_system_config={\n         \"max_io_workers\": 4,\n         \"min_spilling_size\": 100 * 1024 * 1024, #spill atleast 100MB \n         \"object_spilling_config\":json.dumps({\"type\":\"filesystem\",\n                                    \"params\":{\n                                        \"directory_path\": \"&lt;local_ssd_path&gt;/ray_spillage_dir\",\n                                        },\n                                    \"buffer_size\": 100 * 1024 * 1024,\n                                 })\n          },\n         _plasma_directory=\"&lt;local_ssd_path&gt;/ray_plasma_dir\",\n         object_store_memory=300000000000.0, #300GB\n         _memory=600000000000.0)  #600GB\n\ntrain_dataset = ray.data.read_parquet(train_files__, columns = features__, schema = custom_schema) # int32t datatype\nvalidation_dataset = ray.data.read_parquet(validation_files__, columns = features__, schema = custom_schema) #int32t datatype\n\ntrainer = XGBoostTrainer(\n    scaling_config=ScalingConfig(\n        # Number of workers to use for data parallelism.\n        num_workers=10,\n        resources_per_worker={\"CPU\": 12},\n        # _max_cpu_fraction_per_node=0.8,\n        use_gpu=False,\n    ),\n    label_column=\"time_curr_obs\",\n    num_boost_round=50,\n    params={\n        # XGBoost specific params\n        \"tree_method\": \"hist\",\n        \"max_depth\":\"8\",\n        \"eval_metric\": [\"logloss\", \"error\"],\n    },\n    datasets={\"train\": train_dataset, \"valid\": validation_dataset},\n)\nresult = trainer.fit()```\n*Issues being observed:*\n1.\n<https://global.discourse-cdn.com/business7/uploads/ray/original/2X/1/1a3e380c61c48266699148ea139f647508471d93.png|image748\u00d7443 48.7 KB>\n\n&gt; The CPU usage during the load data phase is very low and one RayXGBoostActor.load_data is created in the intial loading phase when data is converted to ray objects.How can we increase the #load_data actors? Even after setting the num_workers to 10 only one load_data actor is created.\n&gt; In case of RayDMatrix (instead of Ray Datasets) increasing the num_actors would lead to creation of than many load_data actors. However here it seems num_workers are used only during training,\n1. If we do not pass the plasma directory then the time taken in the ray objects creation and spillings is around 10x faster compared to its path in local SSD.\n2. We have noticed that in this case /dev/shm is used by default. But training is very slow with this as it complains that the system is low on memory.\n3. We have tried tuning the \u201cmax_io_workers\u201d: 4 parameter but this isn\u2019t increasing the runtime nor the write throughput during the ray_objects creation and spilling.\n&gt; What all parameters canbe tuned to optimize this?\n1. How should _max_cpu_fraction_per_node be used in combination with num_workers and resources_per_worker, as we tuning this parameter doesn\u2019t seem to change the runtime, but this is prompted in the warning of the run.\n2. Can we pass num_actors for data parallelism while ray object creation and spilling in dmatrix_params parameter of XGBoostTrainer ?"}
{"question": "Hi, is there an api allowing me to check if the worker nodes inside a cluster has been setup so I can submit job to it? I know you can do `ray exec config \"ray status\"`  and manually parse the output. However, this is not ideal given the output could change easily. So wonder if there's built in api avaialble"}
{"question": "Another weird issue I have is that I found the FIRST job that I submitted that uses ray core to do distributed model training with ray cluster would be extremely slow (more than 10x slower in the case of torch models)... However, the following runs would be normal on the same cluster.\n\nI've added checks and know that it's not because communication overhead, i.e. transferring arguments of the remote function. The training is slow simply because of the `fit` call of the specific model happening on a worker node is much slower.\n\nI wonder is there specific setup that's being ran during the first distributed job? Or could the machine continue to run compute intensive setup even when it's already added to the cluster and available in the dashboard?"}
{"question": "Hey Folks! Are there any plans to integrate ray with kubeflow officially ?"}
{"question": "Hey folks! I noticed that the <https://github.com/ray-project/ray/blob/master/docker/base-deps/Dockerfile|ray docker image> uses python 3.7.16. Given that the <https://endoflife.date/python|end of life for python 3.7 > is 27 June 2023 (in ~3 months), are there any plans to upgrade the image to use a more recent python version?"}
{"question": "1. Is there a way to get user (job submitter) info on the ray jobs, when submitted through ray cli or python client? \n2. If this can be accomplished by some metadata or envvar, is there a recommended way of doing this and a way of automating/enforcing?"}
{"question": "I have a long running script that keeps pulling messages from pubsub and submits a Ray job per message to the Ray cluster. I have ray autoscaling in place but some of the jobs still fail with the error\n`ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.` Is there a way to stall jobs or to check the resource usage before submitting a job?"}
{"question": "Hi there, how do I troubleshoot a build error on Mac OS:\n\n```     % pip install -e . --verbose\n     .... \n     /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; PYTHONHASHSEED=0 external/cython/cython.py --cplus python/ray/_raylet.pyx --output-file bazel-out/darwin-opt/bin/python/ray/_raylet.cpp')\n    # Configuration: 8d46c5aec9628554010c70645c965950bc02aff6bcc542797c2ac12d9a16e07e\n    # Execution platform: @local_config_platform//:host\n\n    Use --sandbox_debug to see verbose messages from the sandbox and retain the sandbox build root for debugging\n    env: python: No such file or directory\n    INFO: Elapsed time: 77.393s, Critical Path: 19.83s```\nI am interested in guidance on how to troubleshoot it, for example, where do I specify `--sandbox_debug`  as this is Bazel option?\n\nIn general, is it a good idea to develop with Ray on Mac OS?\n\nThanks"}
{"question": "Hi Ray team! Need some help. My  pytorch ray training program is crashing with segfault in dataloader workers after 2 - 3 minutes. It works when num_workers = 0. I have stripped away all code and now there is a really bare bones Torchtrainer with a dummy iterabledataset, it still crashes when num_wokers &gt; 0 in dataloader. It works otherwise for num_workers = 0 . I am using ray 2.3.1 and torch '1.13.1+cu117'. Has something changed? I have attached the code for reproducing the issue. I am using EC2 VM based setup. It still works if the dummy dataloader loop is moved out of Torchtrainer function  directly into  the main function and then it is submitted as a ray job."}
{"question": "does anyone know how to run Catboost on ray?"}
{"question": "Hello Team, I have a Linux machine with 6x AMD cards and want to use it inside ray docker, any advice on where I can find more info how to enable it ?  "}
{"question": "<@U01AK865G10> and <@U03M0L6UBKN>, we are facing the same issue with Ray Air XGBoostTrainer.\nCould you guys please help us with how this was resolved ?\n``` WARNING: ../src/learner.cc:222: No visible GPU is found, setting `gpu_id` to -1```"}
{"question": "hi guys - for object spilling to cloud from the doc here:\n```# Note that `object_spilling_config`'s value should be json format.\nray start --head --system-config='{\"object_spilling_config\":\"{\\\"type\\\":\\\"filesystem\\\",\\\"params\\\":{\\\"directory_path\\\":\\\"/tmp/spill\\\"}}\"}'```\nis it possible to specify above in the cloud launcher yaml file? the json dumps format is certainly inconsistent w/ what yaml expects, so I'm not sure how to \"convert\" it so that ray also understands.\nthanks"}
{"question": "Hi, I am looking for ways to bypass kernel stack for the ray cluster nodes communication. Is there a support for the ray cluster nodes to communicate using DPDK?"}
{"question": "Hi everyone. I am trying to use `ray.train.tensorflow.TensorflowTrainer`  for training. In 50% of my runs I see gRPC unavailable error, I was able to debug that when the underlying  `os.environ[\"TF_CONFIG\"]`  is made, we are getting workers with same host and port. In `python3.9/site-packages/ray/train/tensorflow/config.py`  I see that ray is getting address and port in parallel, because of which two workers are getting same port.\n```class _TensorflowBackend(Backend):\n    def on_start(self, worker_group: WorkerGroup, backend_config: TensorflowConfig):\n        # Compute URL for initializing distributed setup.\n        def get_url():\n            address, port = get_address_and_port()\n            return f\"{address}:{port}\"\n\n        urls = worker_group.execute(get_url)\n\n        # Get setup tasks in order to throw errors on failure.\n        setup_futures = []\n        for i in range(len(worker_group)):\n            setup_futures.append(\n                worker_group.execute_single_async(\n                    i, _setup_tensorflow_environment, worker_addresses=urls, index=i\n                )\n            )\n        ray.get(setup_futures)\n\n    @classmethod\n    def _encode_data(cls, checkpoint: Checkpoint):\n        checkpoint = super()._encode_data(checkpoint)\n        if type(checkpoint) is Checkpoint:\n            _warn_about_bad_checkpoint_type(TensorflowCheckpoint)\n            checkpoint = TensorflowCheckpoint.from_checkpoint(checkpoint)\n        return checkpoint```\nIs this a valid bug or am I missing something here and what could be the fix for this?\n<@U01AK865G10>  <@U029HP6FVHB>"}
{"question": "Hi, is there any python api allowing one to know the node id of the head node? Or an api to know if the current task is being executed on the head node or not"}
{"question": "Hi Everyone, I am facing difficulties consuming logs generated by / on Ray dashboard for example my job which runs fine most of the time encountered following error\nstacktrace :\n```*** SIGTERM received at time=1681300163 on cpu 57 ***\nPC: @     0x7fd153321b30  (unknown)  accept4\n    @     0x7fd153407420  (unknown)  (unknown)\n[2023-04-12 04:49:23,961 E 16730 15602] <http://logging.cc:361|logging.cc:361>: *** SIGTERM received at time=1681300163 on cpu 57 ***\n[2023-04-12 04:49:23,961 E 16730 15602] <http://logging.cc:361|logging.cc:361>: PC: @     0x7fd153321b30  (unknown)  accept4\n[2023-04-12 04:49:23,961 E 16730 15602] <http://logging.cc:361|logging.cc:361>:     @     0x7fd153407420  (unknown)  (unknown)\n---------------------------------------------------------------------------\nRayTaskError(AttributeError)              Traceback (most recent call last)\nFile ~/anaconda3/lib/python3.9/site-packages/xgboost_ray/main.py:1181, in _train(params, dtrain, evals, ray_params, cpus_per_actor, gpus_per_actor, _training_state, *args, **kwargs)\n   1179     ready, not_ready = ray.wait(\n   1180         not_ready, num_returns=len(not_ready), timeout=1)\n-&gt; 1181     ray.get(ready)\n   1183 # Get items from queue one last time\n\nFile ~/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105, in client_mode_hook.&lt;locals&gt;.wrapper(*args, **kwargs)\n    104         return getattr(ray, func.__name__)(*args, **kwargs)\n--&gt; 105 return func(*args, **kwargs)\n\nFile ~/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py:2309, in get(object_refs, timeout)\n   2308 if isinstance(value, RayTaskError):\n-&gt; 2309     raise value.as_instanceof_cause()\n   2310 else:\n\nRayTaskError(AttributeError): ray::_RemoteRayXGBoostActor.train() (pid=42178, ip=100.64.70.221, repr=&lt;xgboost_ray.main._RemoteRayXGBoostActor object at 0x7f832e340f40&gt;)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/xgboost_ray/main.py\", line 630, in train\n    self.load_data(dtrain)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/xgboost_ray/main.py\", line 596, in load_data\n    param = data.get_data(self.rank, self.num_actors)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/xgboost_ray/matrix.py\", line 847, in get_data\n    self.load_data(num_actors=num_actors, rank=rank)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/xgboost_ray/matrix.py\", line 833, in load_data\n    refs, self.n = self.loader.load_data(\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/xgboost_ray/matrix.py\", line 559, in load_data\n    x, y, w, fw, b, ll, lu, qid = self._split_dataframe(\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/xgboost_ray/matrix.py\", line 262, in _split_dataframe\n    _qid, local_data = ensure_sorted_by_qid(local_data, self.qid)\n  File \"/home/ray/anaconda3/lib/python3.9/site-packages/xgboost_ray/matrix.py\", line 66, in ensure_sorted_by_qid\n    if _qid.is_monotonic:\nAttributeError: 'NoneType' object has no attribute 'is_monotonic'\n\nThe above exception was the direct cause of the following exception:\n\nRayActorError                             Traceback (most recent call last)\nFile ~/anaconda3/lib/python3.9/site-packages/xgboost_ray/main.py:1494, in train(params, dtrain, num_boost_round, evals, evals_result, additional_results, ray_params, _remote, *args, **kwargs)\n   1493 try:\n-&gt; 1494     bst, train_evals_result, train_additional_results = _train(\n   1495         params,\n   1496         dtrain,\n   1497         boost_rounds_left,\n   1498         *args,\n   1499         evals=evals,\n   1500         ray_params=ray_params,\n   1501         cpus_per_actor=cpus_per_actor,\n   1502         gpus_per_actor=gpus_per_actor,\n   1503         _training_state=training_state,\n   1504         **kwargs)\n   1505     if training_state.training_started_at &gt; 0.:\n\nFile ~/anaconda3/lib/python3.9/site-packages/xgboost_ray/main.py:1206, in _train(params, dtrain, evals, ray_params, cpus_per_actor, gpus_per_actor, _training_state, *args, **kwargs)\n   1204     _stop_rabit_tracker(rabit_process)\n-&gt; 1206     raise RayActorError from exc\n   1208 # Training is now complete.\n   1209 # Stop Rabit tracking process\n\nRayActorError: The actor died unexpectedly before finishing this task.\n\nThe above exception was the direct cause of the following exception:\n\nRuntimeError                              Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 model = xgb_ray.train(\n      2     params, \n      3     dtrain=d_train,  \n      4     evals=[(d_train, 'train'), (d_val, 'eval')],\n      5     evals_result=evals_result,\n      6     ray_params = xgb_ray.RayParams(num_actors = NUM_ACTORS, cpus_per_actor = NUM_CPUS_PER_ACTOR),\n      7     early_stopping_rounds=STOPPING,\n      8     num_boost_round=MAX_EPOCHS,\n      9     verbose_eval=0\n     10 )\n\nFile ~/anaconda3/lib/python3.9/site-packages/xgboost_ray/main.py:1571, in train(params, dtrain, num_boost_round, evals, evals_result, additional_results, ray_params, _remote, *args, **kwargs)\n   1569             queue, stop_event = _create_communication_processes()\n   1570         else:\n-&gt; 1571             raise RuntimeError(\n   1572                 f\"A Ray actor died during training and the maximum number \"\n   1573                 f\"of retries ({max_actor_restarts}) is exhausted.\"\n   1574             ) from exc\n   1575         tries += 1\n   1577 total_time = time.time() - start_time\n\nRuntimeError: A Ray actor died during training and the maximum number of retries (0) is exhausted.```\n```RuntimeError: A Ray actor died during training and the maximum number of retries (0) is exhausted.```\nWhat is the best way to get the reason why my actor died ? logs generated on ray dashboard doesn't give any relevant information , please help."}
{"question": "Hello everyone, I have some question about using PopulationBasedTraining Scheduler on hyperparameter tuning.\nHow to control the size of population? By adjusting num_samples argument of tune.run()?"}
{"question": "In Ray is it possible to pass different docker arguments to the docker container depending on whether if it's the head node or worker node?"}
{"question": "Hi all, I've been reading the docs but I am not getting this concept, let me explain/ask by example. This is a duplicate of my <https://discuss.ray.io/t/when-are-named-actors-shared-and-to-whom/10217|forum post>.\n\nI am running ten repetitions of one experiment. Each one of them calls a Python script, and so each calls\n\n```git.init()\n...\nMyActor.options(name=\"my_actor\", namespace=\"my_namespace\", lifetime=\"detached).remote(...)\n...\n# then, inside code that runs on the workers\nray.get_actor(\"my_actor\", namespace=\"my_namespace\")```\n- The `init` <https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html|creates a Ray cluster or connects to one if already present>.\n- I have implemented my own actor, it is named, and detached, and remote.\n- **In the workers** I `get` the named actor.\n\nSince I execute the same code ten times for the ten repetitions, which run in parallel on a SLURM cluster, does this create ten different actors, or just one actor?\n\nMy **intended behavior** is to have the ten repetitions be completely independent of each other. I want each run to create their own actor, its workers connect to it, and do their thing. I **do not** want one actor to be created that is somehow shared between the ten repetitions.\n\nI suppose it comes down to a lack in my understanding of what a Ray cluster is, and what resources are shared how?\nI am wondering if I'm doing it wrong especially because **sometimes** my repetitions crash with the following error:\n\n```ValueError: Failed to look up actor with name 'my_actor'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.```\nand **sometimes** they don't. I see no reason why the identical code should sometimes work and sometimes fail if the runs were properly independent. I would love some clarifications on this!\n\nFurther, if I do in fact create a shared, named actor between my parallel runs, would I work around this issue by either naming each actor of each repetition differently, i.e. `my_actor_rep1`, `my_actor_rep2`, or by using different namespaces? Which method is better?\n\nThanks!"}
{"question": "Anyone knows what would be the reason why during setup I am getting `{'Agent1': array([0, 0, 0, 0, 0], dtype=int64)}` 64bit int and on the first step, (when the step is coming from model) I get 32 bit? `{'Agent1': array([0, 0, 0, 0, 0], dtype=int32)}`\n\nThe action space is `gymnasium.spaces.MultiDiscrete([2, 2, 2, 2, 2])`, which should be 64 bits ints according to the docs.\nIs there any flag or setting I can use to enforce model outputing bit format?\nI assume the actual reason is perfomance on the model side, but the inconsistency is breaking things."}
{"question": "It is currently impossible to view ray docker images on docker hub; <https://hub.docker.com/r/rayproject/ray/tags>, I'm not sure if this is because of their size or some other issue. Is there any other way to view the images?"}
{"question": "Hi <@U01AK865G10> <@U029UHYGRUL> <@U029HP6FVHB>,\nIs there a way for me to increase the timeout for health check done by gcs server in head node and increase no of times that the health check is called before marking the node dead?"}
{"question": "Hi, is there a way to let ray cluster auto-shutdown when a job is finished?"}
{"question": "Hi everyone, wondering is there a minimal version of protobuf required by ray 2.3.1?\nIt defaults installed 4.22.3 on my machine, but one of the other package I used throws error `Descriptors cannot not be created directly.`\nDowngrading to 3.19.6 works fine, but don\u2019t know what\u2019s the impact on Ray side. Thanks!"}
{"question": "Hi, anyone know how to obtain instantaneous performance data of computing tasks or worker processes, such as CPU utilization, memory usage, etc. Is there any api for this? I can see them in ray dashboard."}
{"question": "Has anyone tried setting up an nginx reverse proxy over the ray head service? I am trying to set that up and can get my requests forwarded to the ray head service (dashboard port) but I get \"You need to enable JavaScript to run this app\" in response. I double checked I have JS enabled in my browser. And I can get the dashboard just fine if I cll the service without the nginx proxy.\n\nAm I missing some cookie or some other header? Does anyone know when the server responds with this message? What is the detection mechanism?"}
{"question": "I have question - why would ray overload a node w/ more task that the resources allow ?\nAs an illustrative example:\nI declare node A to have `{'custom_resource': 1}` , and I specify in task to be:\n```@ray.remote(resources={\"custom_resource\": 1})\ndef foo():\n    pass```\nIf I launch 2 tasks, many times I would observe the one node getting overloaded 2 tasks, w/ one spare node.\n\nHow can I avoid this ?"}
{"question": "[Ray Java Question] Hi everyone, I am trying to run `Ray.init()` in a Java test class, and this Java application depends on `ray-api` and `ray-runtime` 2.0.0 But I encountered the following error:\n```java.lang.RuntimeException: Failed to initialize Ray runtime.\n        at io.ray.api.Ray.init(Ray.java:21)\n        Caused by:\n        java.lang.RuntimeException: Failed to initialize ray runtime\n            at io.ray.runtime.DefaultRayRuntimeFactory.createRayRuntime(DefaultRayRuntimeFactory.java:39)\n            at io.ray.api.Ray.init(Ray.java:32)\n            at io.ray.api.Ray.init(Ray.java:19)\n            ... 1 more\n            Caused by:\n            java.lang.RuntimeException: Failed to start Ray runtime.\n                at io.ray.runtime.runner.RunManager.startRayHead(RunManager.java:43)\n                at io.ray.runtime.RayNativeRuntime.start(RayNativeRuntime.java:72)\n                at io.ray.runtime.DefaultRayRuntimeFactory.createRayRuntime(DefaultRayRuntimeFactory.java:35)\n                ... 3 more\n                Caused by:\n                java.io.IOException: Cannot run program \"ray\": error=2, No such file or directory\n                    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n                    at io.ray.runtime.runner.RunManager.runCommand(RunManager.java:80)\n                    at io.ray.runtime.runner.RunManager.startRayHead(RunManager.java:41)\n                    ... 5 more\n                    Caused by:\n                    java.io.IOException: error=2, No such file or directory\n                        at java.lang.UNIXProcess.forkAndExec(Native Method)\n                        at java.lang.UNIXProcess.&lt;init&gt;(UNIXProcess.java:247)\n                        at java.lang.ProcessImpl.start(ProcessImpl.java:134)\n                        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n                        ... 7 more```\nWondering if I need to config anything in my application?  Any help would be appreciated. Thanks!"}
{"question": "Has anyone figured out how to host Streamlit backed by Ray ?\nI see it mentioned in Anyscale's PDF &amp; in Ray docs, but could only find Gradio examples."}
{"question": "Hi team, I am new to Ray Serve. I am testing the fault tolerance of GCS with redis for this I have set up a ray cluster running on kuberay. I am not able to find the documentation around how the head node is talking to GCS? can anyone help me with it. Thank you."}
{"question": "Hi all,\nI am trying to launch a FastAPI application in a Docker container (named ml_service). Upon startup, this application needs to connect to a ray cluster hosted by a container named ray_head. The address used in the ray.init() function at the launch of the application is, therefore, <ray://ray_head:10001>.\n\nI got the following stacktrace (looks like a ressource or permission issue...):\n```2023-04-20 17:31:12 Skipping virtualenv creation, as specified in config file.\n2023-04-20 17:31:12 INFO:     Will watch for changes in these directories: ['/']\n2023-04-20 17:31:12 INFO:     Uvicorn running on <http://127.0.0.1:8000> (Press CTRL+C to quit)\n2023-04-20 17:31:12 INFO:     Started reloader process [1] using WatchFiles\n2023-04-20 17:31:18 INFO:     Started server process [10]\n2023-04-20 17:31:18 INFO:     Waiting for application startup.\n2023-04-20 17:31:18 INFO:     Connecting to Ray cluster...\n2023-04-20 17:31:18 INFO:     Ray cluster address: <ray://ray_head:10001>\n2023-04-20 17:31:18 INFO:     Storage directory:None\n2023-04-20 17:31:35 2023-04-20 15:31:35,392     ERROR packaging.py:125 -- Issue with path: /proc/sys/abi/vsyscall32\n2023-04-20 17:31:35 ERROR:    Traceback (most recent call last):\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 645, in lifespan\n2023-04-20 17:31:35     async with self.lifespan_context(app):\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 540, in __aenter__\n2023-04-20 17:31:35     await self._router.startup()\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/starlette/routing.py\", line 624, in startup\n2023-04-20 17:31:35     handler()\n2023-04-20 17:31:35   File \"/ml_service/api.py\", line 67, in startup_event\n2023-04-20 17:31:35     ray.init(\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n2023-04-20 17:31:35     return func(*args, **kwargs)\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/worker.py\", line 1267, in init\n2023-04-20 17:31:35     ctx = builder.connect()\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/client_builder.py\", line 182, in connect\n2023-04-20 17:31:35     client_info_dict = ray.util.client_connect.connect(\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/util/client_connect.py\", line 57, in connect\n2023-04-20 17:31:35     conn = ray.connect(\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/util/client/__init__.py\", line 252, in connect\n2023-04-20 17:31:35     conn = self.get_context().connect(*args, **kw_args)\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/util/client/__init__.py\", line 102, in connect\n2023-04-20 17:31:35     self.client_worker._server_init(job_config, ray_init_kwargs)\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/util/client/worker.py\", line 838, in _server_init\n2023-04-20 17:31:35     runtime_env = upload_working_dir_if_needed(\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/runtime_env/working_dir.py\", line 64, in upload_working_dir_if_needed\n2023-04-20 17:31:35     working_dir_uri = get_uri_for_directory(working_dir, excludes=excludes)\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py\", line 457, in get_uri_for_directory\n2023-04-20 17:31:35     hash_val = _hash_directory(directory, directory, _get_excludes(directory, excludes))\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py\", line 173, in _hash_directory\n2023-04-20 17:31:35     _dir_travel(root, excludes, handler, logger=logger)\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py\", line 129, in _dir_travel\n2023-04-20 17:31:35     _dir_travel(sub_path, excludes, handler, logger=logger)\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py\", line 129, in _dir_travel\n2023-04-20 17:31:35     _dir_travel(sub_path, excludes, handler, logger=logger)\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py\", line 129, in _dir_travel\n2023-04-20 17:31:35     _dir_travel(sub_path, excludes, handler, logger=logger)\n2023-04-20 17:31:35   [Previous line repeated 1 more time]\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py\", line 126, in _dir_travel\n2023-04-20 17:31:35     raise e\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py\", line 123, in _dir_travel\n2023-04-20 17:31:35     handler(path)\n2023-04-20 17:31:35   File \"/usr/local/lib/python3.10/site-packages/ray/_private/runtime_env/packaging.py\", line 162, in handler\n2023-04-20 17:31:35     data = f.read(BUF_SIZE)\n2023-04-20 17:31:35 OSError: [Errno 12] Cannot allocate memory\n2023-04-20 17:31:35 \n2023-04-20 17:31:35 ERROR:    Application startup failed. Exiting.```\nHere is a sample of my docker_compose.yml file if needed:\n``` version: '3.9'\nservices:\n  ## RAY CLUSTER ##\n  ray_head:\n    container_name: ray_head\n    image: ${RAY_IMAGE_NAME}:${RAY_IMAGE_VERSION}\n    volumes:\n      - \"../workflow_data:/home/ray/workflow_data\"\n    command: [ \"ray\", \"start\", \"--head\", \"--block\", \"--object-manager-port=8076\", \"--port=6379\", \"--dashboard-host=0.0.0.0\", \"--dashboard-port=8265\",  \"--num-cpus=0\", \"--num-gpus=0\",  \"--disable-usage-stats\",  \"--storage=/home/ray/workflow_data\" ]\n    ports:\n      - 8265:8265\n      - 10001:10001\n      - 6378:6378\n      - 8076:8076\n  ray_worker:\n    container_name: ray_worker\n    volumes:\n      - \"../workflow_data:/home/ray/workflow_data\"\n    image: ${RAY_IMAGE_NAME}:${RAY_IMAGE_VERSION}\n    command: [ \"ray\", \"start\", \"--block\", \"--address=ray_head:6379\", \"--object-manager-port=8076\", \"--num-cpus=1\", \"--num-gpus=0\",  \"--disable-usage-stats\",  \"--storage=/home/ray/workflow_data\" ]\n    \n  ##ML_SERVICE##\n  ml_service:\n    container_name: ml_service\n    image: ${ML_SERVICE_IMAGE_NAME}:${ML_SERVICE_IMAGE_VERSION}\n    command: poetry run uvicorn ml_service.api:app --reload\n    volumes:\n      - \"../ml_core:/ml_core\"\n      - \"../ml_service:/ml_service\"\n    ports:\n      - 8000:8000```\nCan someone help me to understand what is happening ? Cause ChatGPT can not ^^ !\nThanks a lot !"}
{"question": "Hi Folks! When can we expect this commit to make it in the official release ? <https://github.com/ray-project/ray/commit/57a6d21f4f451ab34879c87968ab98520ba64c61>"}
{"question": "Hey, so I have my ray cluster running on kubernetes, but when I try to do `kubectl get pods -l rayCluster=rayservice-sample-raycluster-6f7cz,component=ray-head`\n or `kubectl get pods -l rayCluster=rayservice-sample-raycluster-6f7cz,component=ray-worker`\n It says `No resources found in default namespace`  can anyone tell what is wrong?"}
{"question": "Hi everyone,\nI am a beginner user of Ray's RLlib and I am currently struggling in understending the proper process that needs to be followed when implementing custom Models and Policies.\nWould anyone be so kind as to help me boostrapping the process? I've already read the docs but I found them to be a bit unclear."}
{"question": "One of Ray's oss collaborator, Zilliz just added Ray support to their Q&amp;A chatbot <https://osschat.io/chat?project=Ray|OSSChat> :slightly_smiling_face:"}
{"question": "Hi guys; want to take your feedback on this, I\u2019ve build a ChatGPT plug-in that deploys ray clusters in one prompt, this is a 4min demo <https://www.youtube.com/watch?v=fa-tBLlWh2Y|https://www.youtube.com/watch?v=fa-tBLlWh2Y>\nI would like to take your feedback on the project I\u2019m building a decentralized gpu  cloud platform for ray "}
{"question": "Hi All,\n\nWe are developing our own ray cluster manager. Because of our current architecture constraints we are planning on having *local head node*.\n\nWe see that this is not a common pattern out here, so my question is - can you help us understand what could go wrong in local head node setup? Or point out the aspects we should take into consideration first?"}
{"question": "Hello all! I am working with Rllib using a server with multiple CPUs. I would like to speed up the training time, but the learning performance decreases when I increase the CPUs per worker or the number of training workers. I don't know how this happens. any advice? Thanks in advance."}
{"question": "I've got a question about @ray.remote. (<https://docs.ray.io/en/latest/ray-core/api/doc/ray.remote.html> )\n\nIf I create a class with an initializer that does a ray.get() for a chunk of data (say, a 10 megabyte numpy array), and it holds it as `self.grid` or something and then I call a function on it, say, 10 million times, will there be only one instance of that remote object, or will each worker in my pool get a copy and be able to process at full rate?"}
{"question": "Hi everyone,\nI am running a ray cluster with 16 nodes( all CPU machines(128 core per machine), head nodes is has num_cpus=0). I am facing the following issues.\nI am using TensorflowTrainer for trainng, when I run with  750 workers, everything runs fine, but when I increase the workers to 1500 I get nodes randomly dying\n1. In died node, `received SIGTERM`  line presents in raylet.out file. Is there a way for me to see from where SIGTERM is sent?\n2. In gcs_server.out, I only see something as `Draining node` line when node dies, nothing more regarding this statement(Cause) above it in logs( even in debug)\nIn head node, I start ray using\n```AY_BACKEND_LOG_LEVEL=debug LD_PRELOAD=/usr/local/lib/libtcmalloc.so.4 TF_CPP_MIN_LOG_LEVEL=2 ray start --node-ip-address=192.168.165.27 --head --system-config='{\"object_spilling_config\":\"{\\\"type\\\":\\\"filesystem\\\",\\\"params\\\":{\\\"directory_path\\\":\\\"/srv/weka/cluster_testing/ray_spillage_objects_lokx\\\"}}\",\"health_check_timeout_ms\":45000,\"health_check_period_ms\":15000,\"health_check_failure_threshold\":30,\"grpc_keepalive_timeout_ms\":250000,\"grpc_client_keepalive_time_ms\":600000, \"grpc_client_keepalive_timeout_ms\":1200000}' --object-store-memory=450000000000 --disable-usage-stats --temp-dir=\"$HOME/ray_temp_dir/\" --num-cpus=0 --dashboard-host=\"0.0.0.0\"```\nIn worker nodes\n```TF_CPP_MIN_LOG_LEVEL=2 LD_PRELOAD=/usr/local/lib/libtcmalloc.so.4 ray start --object-store-memory=450000000000 --address=192.168.165.27:6379 --disable-usage-stats --node-ip-address=192.168.165.8```\nI have gone through logs too many times and runs :sweat_smile:, I found nothing relevant.\nCan anyone help me on how I can look into this?\ncc: <@UNCRYAV9N> <@U029UHYGRUL> <@U029HP6FVHB> <@U01AK865G10>\n\nEdit: Even with 750 workers, node died sometime after my program is completed."}
{"question": "Hi guys, I have a question regarding support for Windows. How mature is the support for it?"}
{"question": "Hi, I have a ray tune experiment where each trial will launch other remote tasks.\nI am using placement group factory to specify the trial resources and a `pack` strategy. Though, it seems the remote tasks that's being launched inside the trial is not respecting the `pack` strategy. According to the doc, one can use `options(scheduling_strategy=PlacementGroupSchedulingStrategy(placement_group=pg))` to scheduler tasks to placement group. I wonder how can I access the `pg` being assigned to the trial?"}
{"question": "Hi, Is there a way to pass\n```scheduling_strategy=PlacementGroupSchedulingStrategy(\n            placement_group=pg, placement_group_capture_child_tasks=True\n        )```\nto ray tune? I want the trial, which spread further remote task to be scheduled on the same node as the child remote tasks\n\nTo give some context, currently I'm having this placement group factory\n```tune.PlacementGroupFactory([{}, {'CPU': num_cpus, 'GPU': num_gpus}])```\nAnd what I observe is that the trial itself and the tasks being launched by the trial are not being scheduled on the same node. I've also tried `STRICT_PACK` , not working too"}
{"question": "how is this possible? the job says it's `SUCCEEDED` but not only did not a _single_ task succeed, but there are even some still running"}
{"question": "also, when I pass\n```runtime_env={\"working_dir\" : \"./\"}```\ninto a jobsubmissionclient `submit_job`, it looks like if I update the code locally and resubmit the job, it won't update on the cluster? is it using some kind of cached code?\n\nif so, that seems like not a good idea; how can I override that caching?"}
{"question": "<@U029HP6FVHB> I understand that the dashboard memory resides on the head node. I\u2019ve noticed that the dashboard tends to get slower if the number of jobs submitted goes somewhere around 100s of jobs. I want to understand how the dashboard (and logs) scale based on the number of jobs submitted. Even if I schedule the jobs to run on worker nodes, the head can run out of memory for the dashboard.\nAnother follow up to that, does the dashboard always show all previous jobs or does it have a cut off on the number of last submitted jobs that is displays?"}
{"question": "HI Folks! I have a simple torch xla based dataparallel trainer code. It works for ray 2.3.1 but not for ray 2.4.0. This process does not quit while using ray 2.4.0 : python3 -m torch_neuronx.distributed._xrt_run_server --port 48805 --pid_to_track 52718  and subsequent runs of the program fails with neuron device busy error.  What changed in ray 2.4.0 and how do i address this issue in the latest release? If I manually kill this process I can rerun the code again successfully. I have attached the code snippet here:"}
{"question": "Hi, is there a way to retrieve an object ref from its id?"}
{"question": "Hello Everyone,\nI tried this example <https://github.com/ray-project/ray/tree/ray-2.2.0/python/ray/train/examples/transformers> on a kubernetes cluster with 2 nodes (with V100S). To benchmark if we get an increase in the training speed.\nBut confusingly we got a decrease in the training speed for a sample job when we changed number of workers from 1 to 2.\nNote:\n\u2022 We made sure 1 head and 1 worker were running on different nodes.\n\u2022 GPU was getting used in both the nodes.\n\u2022 Batchsize was not changed.\nAm i missing something? Or anyone has any cluse why this might be happening?"}
{"question": "```Traceback (most recent call last):\n5  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/serialization.py\", line 63, in pickle_dumps\n6    return pickle.dumps(obj)\n7  File \"/usr/local/lib/python3.10/dist-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n8    cp.dump(obj)\n9  File \"/usr/local/lib/python3.10/dist-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n10    return Pickler.dump(self, obj)\n11TypeError: cannot pickle 'SSLContext' object```\n:confused: this is very frustrating. I'm not even able to read the exceptions being thrown. how can I avoid this?"}
{"question": "Hi all,\nJust a quick question about ray remote functions.\nHere is what I had before:\n\n```import func\n\n@ray.remote\ndef remote_func():\n    return func()```\nand here is what I have now:\n```@ray.remote\ndef remote_func():\n    import func\n    return func()```\nI know it is not very pythonic, but I need to do this because I want to isolate dependencies between two modules.\n\n*For performances issues, should I consider making the import in another remote func like as follow ?*\n```@ray.remote\ndef remote_import():\n    import func\n\n@ray.remote\ndef remote_func():\n    return func()```\nAs remote_func can be called many times in my app, I would like to import this function only once, and then spam remote_func shameless !\nThe potential drawback of this method that I anticipate is that in a ray cluster, it would be necessary to be certain that the import is indeed done on all the workers so that the function can be executed everywhere afterwards. *Any solution to avoid such a problem ?*\n\nThanks"}
{"question": "will submitting lots of jobs (even if they're completed in success/fail states) overwhelm the task scheduler db? yesterday I submitted a bunch of jobs (&lt;100) that had about 1k tasks each.\n\nafter the first dozen or two, I noticed the UI couldn't show me success/fail of tasks anymore. all the tasks should have completed within a few minutes. this morning I noticed that all my worker nodes were still open and I could not connect to the cluster at all, so I had to tear the whole thing down\n\nmy guess is that I ran out of RAM on the head node. is there a way to monitor / prevent this? can the head node be configured to autoscale alongside the workers, or is there some pattern I should avoid to not overwhelm the scheduler?"}
{"question": "I am seeing this message (and subsequent job failures) after running large workloads in Kubernetes (not using RayKube) - `gcs_rpc_client.h:203: Failed to connect to GCS at address &lt;dns name&gt;:6379 within 5 seconds.`   Though I\u2019m still investigating this, is there a way to increase that 5 second timeout value? (Ray 2.3.1)"}
{"question": "Has there been any work done to integrate <https://github.com/NVIDIA/FasterTransformer|NVIDIA FasterTransformer> with Ray?"}
{"question": "hey, my cluster is still not autoscaling down. the node timeout is set to 5min and there are nodes which have been there hours. there are no client connections open or tasks running, but it's leaving about 100 ec2 nodes running. why might this be? I don't want to have to be constantly tearing down the whole cluster"}
{"question": "Hi,  I'm trying to do some wild hpo experiment with ray, where I have a cluster of 8 m5.24xlarge machines (700 + vcpus), and launching 960 trials, where each trial will train 8 models in parallel with ray tasks (meaning train around 8000 models in total). From my observation, the tune job is only utilizing around 100 - 200 vcpus in the cluster and I can see a lot of \"unaccounted\" jobs, whose number keeps increasing. I wonder is there any way to avoid that?"}
{"question": "Hello! I\u2019m trying to create a ray cluster in AWS. However it seems to run into some errors whilst pulling the docker image(I assume that\u2019s the problem), could someone give possible pointers I can look into further?\nAll my subnets are set to public right now and the attached security group allows all egress (I later wish to make atleast all worker nodes in private subnets, but that\u2019s for another day)"}
{"question": "Hi, is there a career job board or channel or forum category(or would it make sense to have one) for job postings catering to Ray? A place where companies using Ray can post jobs for developers interested/having experience working with Ray."}
{"question": "wanted to add a coworker to this workspace, its currently gated on admin approval, can someone please add `<mailto:sam.huang@snorkel.ai|sam.huang@snorkel.ai>`?"}
{"question": "Hello! We have workloads on ray that are writing out files locally on the ray worker nodes in /tmp and then uploading them to another service. I wanted to understand what best practice is for this kind of workload as the local directory eventually will run out of space. Is there guidance on where these should be written? Is there some sort of garbage collection that cleans these up once a ray task completes?"}
{"question": "re: Ray workflow, any chance I could get some color on if supporting Azure as one of its backends would be on the roadmap? Thanks!"}
{"question": "Hello I'm ray lover...! but I'm having hard time using share object. I have collection of numpy array like: `[array([0, 1, 2]), array([5,7,2,4,7]). ...]`\nwhen I put that variable to object store(using `.put()`) and call `remote()`, it seems that ray worker copy  variable to their own memory...(`RES` is very high and `SHR` is low in `top` command)\nhow to utilize 'zero-copy' feature ray provide for that variable?"}
{"question": "Hi guys - I'm running into serialization problem when using `torch.compile` :\n\n```from ray import cloudpickle as pkl\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = torch.nn.Linear(100, 10)\n\n    def forward(self, x):\n        return torch.nn.functional.relu(self.lin(x))\n\nmod = MyModule()\nopt_mod = torch.compile(mod)\n_ = pkl.dumps(opt_mod)```\nthe above code would give `TypeError: cannot pickle 'ConfigModuleInstance' object`\nI raised a tkt here: <https://github.com/ray-project/ray/issues/35001>\n\nIs the fix on this WIP ? (tried raybot w/ no luck :))\nThanks"}
{"question": "Does actor = Actor.remote() return singleton object by default ? if not then is it best practice to create a singleton Actor  ?"}
{"question": "Sorry for the newbie question, still learning Ray.\n\nFor the supervised actor pattern, can we keep the Worker Actor up if the Supervisor passes a reference to the Actor to another Actor, to allow the worker actor to remain even on Supervisor / Driver failure?"}
{"question": "Hi!\n\nI am really new Ray (RLLib), I just started to use it few days ago. I try to restructure my training algorithm (MARL - 4 agents, DQN) using RLLib for VSL purposes using a custom environment.\nMy trainer looks like this right now:\n```from myEnvironment import myEnvironment\nimport ray\nfrom ray.tune.registry import register_env\nfrom ray.rllib.algorithms.dqn import DQN\n\n\nray.init()\nregister_env(\"myEnv\", lambda config: myEnvironment())\nenv = myEnvironment()\n\nconfig = {\n    'environment' : \"myEnv\",\n    'observation_space' : env.observation_space,\n    'action_space' : env.action_space,\n    \"framework\": \"torch\",\n    \"timesteps_per_iteration\": 1000,\n\n}\n\n# Create the RLlib agent\nagent = DQN(config=config)\nobservation = env.reset()\nfor iteration in range(100):\n    result = agent.train()\n\n    # Print training progress\n    print(f\"Iteration {iteration}: {result}\")\n\n    # Save a checkpoint every 10 iterations\n    if iteration % 10 == 0:\n        checkpoint = agent.save()\n        print(f\"Checkpoint saved at iteration {iteration}: {checkpoint}\")```\nThe action space and the observation space are defined like this:\n```occupancy_low = 0\noccupancy_high = 1\nspeed_low = 0\nspeed_high = 37\nspeed_observation_space = spaces.Box(low=speed_low, high=speed_high, shape=(4,), dtype=np.float32)\noccupancy_observation_space = spaces.Box(low=occupancy_low, high=occupancy_high, shape=(4,), dtype=np.float32)\nbit_observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\nobservation_space = spaces.Tuple((speed_observation_space, occupancy_observation_space, bit_observation_space))\naction_space = gym.spaces.Discrete(3)```\n```class myEnvironment(MultiAgentEnv):\n    def __init__(self, gui = True):\n        self.observation_space = spaces.Dict({\n            \"agent_0\": observation_space,\n            \"agent_1\": observation_space,\n            \"agent_2\": observation_space,\n            \"agent_3\": observation_space,\n        })\n        self.action_space = action_space```\nAnd of course the basic gym structure in the environment. I got this error:\nValueError: RolloutWorker has no `input_reader` object! Cannot call `sample()`. You can try setting `create_env_on_driver` to True.\n\nI am not even sure I use the correct format for the spaces. Can somebody help me what would be the problem? Thank you in advance."}
{"question": "Hello. Is there any hope for RNNSAC?"}
{"question": "Hi, I\u2019m deploying a ML model with Ray Serve on AWS, but the initial setup time is way too high. Can someone tell me if this is normal?"}
{"question": "Does ray-air work fine with Pytorch 2.0? Does a Pytorch1.x ray air code need to change in anyway for migration to Pytorch2.0 ?"}
{"question": "Has anyone figured out a way to have a monorepo with multiple modules be packaged correctly via runtime_env?"}
{"question": "Hey all, is anyone else running into issues building Ray? I've been seeing a build error related to spdlog using a reserved keyword from the C++20 standard: <https://github.com/ray-project/ray/issues/35200>"}
{"question": "Hmm -- we're running into this occasionally with our batch jobs: <https://github.com/ray-project/ray/issues/18053>  Is the workaround still to manually specify everything, or is there a better fix in (or on the way?)"}
{"question": "Howdy -- any advice from Ray core folks on diagnosing f.remote() calls that take really long to return?\n\nI've posted in the discuss.ray forum\n<https://discuss.ray.io/t/f-remote-calls-taking-a-while-to-return/10574>\nsince that's formally correct place for this.\nFigured it wouldn't hurt to repeat on Slack as well :slightly_smiling_face:\ncc <@U038M1HMGR5> <@U03SNV32NCC>"}
{"question": "Hi\nI am trying to tune PyTorch Geometric model using ray tune but it looks like it is not parallelizing the trials as it is getting more time to finish when compared to Optuna alone, here is the code snippet:\n`search_config={`\n        `\"hidden_channels\": tune.randint(search_space.hidden_channels[0], search_space.hidden_channels[1]),`\n        `\"lr_rate\": tune.loguniform(<http://search_space.lr|search_space.lr>_rate[0], <http://search_space.lr|search_space.lr>_rate[1]),`\n        `\"n_hidden_layers\": tune.randint(search_space.n_hidden_layers[0], search_space.n_hidden_layers[1]),`\n        `\"weight_decay\": tune.loguniform(search_space.weight_decay[0], search_space.weight_decay[1]),`\n        `\"n_trials\": search_space.n_trials,`\n        `\"data\": data,`\n        `\"model\": model,`\n        `\"metagraph\": metagraph,`\n        `\"classification_node_type\": classification_node_type,`\n        `\"sizes\": sizes,`\n        `\"batch_size\": batch_size,`\n        `\"epochs\": epochs`\n    `}`\n\n    `if len(metagraph[\"vertexCollections\"]) &gt; 1:  # type: ignore`\n        `print(\"Heterogeneous Graph Detected...........\")`\n        `#train_on_data_partial = partial(`\n        `#    train_hetero,`\n        `#    config=search_config`\n        `#)`\n        `search_alg = OptunaSearch(`\n        `sampler=optuna.samplers.TPESampler(seed=42))`\n\n        `analysis = tune.run(`\n        `train_hetero,`\n        `resources_per_trial={\"cpu\":8},`\n        `config= search_config,`\n        `metric=\"accuracy\",`\n        `mode=\"max\",`\n        `num_samples=search_config[\"n_trials\"],`\n        `search_alg=search_alg)`\n\n`def train_hetero(`\n    `config: dict`\n`) -&gt; None:`\n    `val_score = 0.0`\n    \n    `data = config[\"data\"]`\n    `classification_node_type = config[\"classification_node_type\"]`\n    `sizes = config[\"sizes\"]`\n    `batch_size = config[\"batch_size\"]`\n    `epochs = config[\"epochs\"]`\n    `metagraph = config[\"metagraph\"]`\n\n    `# type cast node features to float`\n    `for vertex in metagraph[\"vertexCollections\"].keys():  # type: ignore`\n        `data[vertex].x = data[vertex].x.float()`\n\n    `train_loader = hetero_batch_loader(`\n        `data, classification_node_type, sizes, batch_size`\n    `)`\n\n    `hidden_channels = config[\"hidden_channels\"]`\n    `lr_rate = config[\"lr_rate\"]`\n    `n_hidden_layers = config[\"n_hidden_layers\"]`\n    `weight_decay = config[\"weight_decay\"]`\n\n    `for epoch in range(epochs):`\n        `if torch.max(data[classification_node_type].y).item() == 1:`\n            `model = HeteroSage(n_hidden_layers, hidden_channels, 1)`\n            `model = to_hetero(model, data.metadata(), aggr=\"sum\")`\n            `<http://data.to|data.to>(device)`\n            `<http://model.to|model.to>(device)`\n            `optimizer = torch.optim.Adam(`\n                `model.parameters(), lr=lr_rate, weight_decay=weight_decay`\n            `)`\n            `loss = train_binary_hetero(`\n                `model, train_loader, optimizer, classification_node_type`\n            `)`\n            `print(f\"Epoch {epoch:02d}, Loss: {loss:.4f}\")`\n            `train_score, val_score, test_score = test_binary_hetero(`\n                `model, data, classification_node_type`\n            `)`\n            `print(`\n                `f\"Epoch {epoch:02d}, Train_auc: {train_score:.4f},\"`\n                `f\"Val_auc: {val_score:.4f}, Test_auc: {test_score:.4f}\"`\n            `)`\n        `else:`\n            `model = HeteroSage(`\n                `n_hidden_layers,`\n                `hidden_channels,`\n                `torch.unique(data[classification_node_type].y).size(0),`\n            `)`\n            `model = to_hetero(model, data.metadata(), aggr=\"sum\")`\n            `<http://data.to|data.to>(device)`\n            `<http://model.to|model.to>(device)`\n            `optimizer = torch.optim.Adam(`\n                `model.parameters(), lr=lr_rate, weight_decay=weight_decay`\n            `)`\n            `loss = train_multiclass_hetero(`\n                `model, train_loader, optimizer, classification_node_type`\n            `)`\n            `print(f\"Epoch {epoch:02d}, Loss: {loss:.4f}\")`\n            `train_score, val_score, test_score = test_multiclass_hetero(`\n                `model, data, classification_node_type`\n            `)`\n            `print(`\n                `f\"Epoch {epoch:02d}, Train_Acc: {train_score:.4f},\"`\n                `f\"Val_acc: {val_score:.4f}, Test_acc: {test_score:.4f}\"`\n            `)`\n        `tune.report(accuracy=val_score)` \n\nAny help here?"}
{"question": "Hi everyone,\n\nI am running the simplecorridor example on Ray, and when I load a model checkpoint and run the `compute_single_action` function, it gives me the same result no matter what the input is. Even if I exceed the corridor length, the prediction remains the same. I have the same issue with my actual environment, wherein I am using MultiDiscrete actions, and when I load a checkpoint after training, the predicted action is always the same. Is there something that I am doing wrong in terms of the logic? Here is the example that I am running:\n\n```import logging\n\nimport gym\nimport ray\nfrom ray import air\nfrom ray import tune\nfrom ray.rllib.algorithms.a2c import A2C\nfrom ray.tune import register_env\n\nlogging.basicConfig(format='%(levelname)s:%(name)s: %(message)s (%(asctime)s; %(filename)s:%(lineno)d)',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=<http://logging.INFO|logging.INFO>)\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass SimpleCorridor(gym.Env):\n    \"\"\"Corridor in which an agent must learn to move right to reach the exit.\n\n    ---------------------\n    | S | 1 | 2 | 3 | G |   S=start; G=goal; corridor_length=5\n    ---------------------\n\n    Possible actions to chose from are: 0=left; 1=right\n    Observations are floats indicating the current field index, e.g. 0.0 for\n    starting position, 1.0 for the field next to the starting position, etc..\n    Rewards are -0.1 for all steps, except when reaching the goal (+1.0).\n    \"\"\"\n\n    def __init__(self, config):\n        LOGGER.warning(f'Initializing the corridor of length {config[\"corridor_length\"]}')\n        self.end_pos = config[\"corridor_length\"]\n        self.cur_pos = 0\n        self.action_space = gym.spaces.Discrete(2)  # left and right\n        self.observation_space = gym.spaces.Box(0.0, self.end_pos, shape=(1,))\n\n    def reset(self, **kwargs):\n        \"\"\"Resets the episode and returns the initial observation of the new one.\"\"\"\n        LOGGER.warning(f'Resetting current position to the beginning')\n        self.cur_pos = 0\n        return [self.cur_pos]\n\n    def step(self, action):\n        \"\"\"Takes a single step in the episode given `action`\n\n        Returns:\n            New observation, reward, done-flag, warning-dict (empty).\n        \"\"\"\n        # Walk left.\n        LOGGER.warning(f'inside step function')\n        if action == 0 and self.cur_pos &gt; 0:\n            self.cur_pos -= 1\n        # Walk right.\n        elif action == 1:\n            self.cur_pos += 1\n        # Set `done` flag when end of corridor (goal) reached.\n        done = self.cur_pos &gt;= self.end_pos\n        # +1 when goal reached, otherwise -1.\n        reward = 1.0 if done else -1.0\n        LOGGER.warning(f'Episode end: {self.cur_pos}, {reward}, {done}')\n        return [self.cur_pos], reward, done, {}\n\n\n<http://LOGGER.info|LOGGER.info>(f'Registering env')\nenvironment = SimpleCorridor(config={'corridor_length': 5})\n\nLOGGER.critical(f'If using a remote ray cluster, it should be initialized before the environment is registered,'\n                f'otherwise the remote ray cluster never realizes the name of the registered environment')\n\nregister_env('simplecorridor', lambda env_config: environment)\n\nagent_params = {\n    \"num_workers\": 1,\n    \"horizon\": 10,\n    \"model\": {\n        [32,32]\n    },\n    \"explore\": True,\n    \"lr\": 0.001,\n    \"entropy_coeff\": 0.01,\n    \"vf_loss_coeff\": 0.25,\n}\n\nmetric_dict = {\n    \"metric\": \"episode_reward_mean\",\n    \"mode\": \"max\"\n}\n\nexp_name = 'test-experiment'\nlocal_dir = 'tmp/ray2/simplecorridor'\n\ntrainer_config = agent_params\ntrainer_config['env'] = 'simplecorridor'\n\n# concerned with metrics and stuff\ntune_config = tune.TuneConfig(\n    mode=metric_dict['mode'],\n    metric=metric_dict['metric']\n)\n\n# concerned with the stopping and the checkpointing logic\nrun_config = air.RunConfig(\n    name=exp_name,\n    local_dir=local_dir,\n    stop={\n        \"timesteps_total\": 100\n    },\n    checkpoint_config=air.CheckpointConfig(checkpoint_at_end=True, checkpoint_frequency=5,\n                                           num_to_keep=5, checkpoint_score_attribute='episode_reward_mean')\n)\n\nexperiment_tuner = tune.Tuner(\n    trainable=A2C,\n    param_space=agent_params,\n    tune_config=tune_config,\n    run_config=run_config,\n)\n\nray.init()\nresult_grid = experiment_tuner.fit()\n<http://LOGGER.info|LOGGER.info>(f'The training is now finished, the best checkpoint as per the metric will be found and loaded. For the'\n            f' loading back, we will be using the same metric as we passed into the tuner config')\n\n# no need to pass a metric or mode, default picked from the experiment_tuner config\nbest_result = result_grid.get_best_result()\nbest_checkpoint = best_result.checkpoint\n\n<http://LOGGER.info|LOGGER.info>(f'The best checkpoint path is {str(best_checkpoint)}')\n<http://LOGGER.info|LOGGER.info>(f'The directory of the best checkpoint for the current experiment run is {best_checkpoint._local_path}')\n\n<http://LOGGER.info|LOGGER.info>(f'Let us now load back the trainer from the best checkpoint from the path we saw above')\n\n<http://LOGGER.info|LOGGER.info>(f'Checking the path of the best checkpoint available from the trainer object')\n\n# this also restores the configuration that the best checkpoint had\nnew_trainer_for_inference = A2C.from_checkpoint(best_checkpoint)\n\n# we also have to check the method to resume training if continuous training flag is enabled via Airflow\n<http://LOGGER.info|LOGGER.info>(f'Make a prediction to check if the model works from the loaded checkpoint')\n\n# the predictions are always the same no matter what input observation you pass, be it 6 or 6000\npredictions = new_trainer_for_inference.compute_single_action([6])\n\n<http://LOGGER.info|LOGGER.info>(f'The predictions are now working')```\nBasically, no matter what the input observation is to the `compute_single_action` method, the prediction is always the same. What am I doing wrong here?\n\nThanks for the help"}
{"question": "Hi team, is there a way to suppress FutureWarning specifically?"}
{"question": "In processing some large files, I am starting to run into errors around object store exhaustion.  Is there a way to more aggressively garbage collect object store objects that are no longer referenced?\n```(raylet) [2023-05-11 18:49:21,362 E 640 831] (raylet) <http://file_system_monitor.cc:105|file_system_monitor.cc:105>: /tmp/ray/session_2023-05-11_18-14-57_361455_7 is over 95% full, available space: 3975639040; capacity: 104591454208. Object creation will fail if spilling is required.```"}
{"question": "I see `ray start`'s `--memory` <https://github.com/ray-project/ray/blob/ray-2.4.0/python/ray/scripts/scripts.py#L354-L361|switch here> is hidden and help message says `The amount of memory (in bytes) to make available to workers`. Does this mean this switch has no effect on the head node when that node is not used as a worker (i.e. started with `ray start --head --num-cpus=0`)?"}
{"question": "Just wondering if there is a way to specify ECR locations for Docker images in the config.yaml file instead of Dockerhub? Thank you"}
{"question": "Is there a way to get the private ip of the head node in a ray cluster?"}
{"question": "I use ray-ml:latest-py38-gpu docker image in yaml, but when I ssh into docker container, I can't run nvidia-smi. What's going on here?"}
{"question": "is there much overhead per `ray.put()/get()` call? is it better to store fewer larger objects than lots of smaller objects?"}
{"question": "hi, does anyone know how to open ray dashboard on jupyter notebook?"}
{"question": "Hi, does ray tune with 2.4 new LightningTrainer supports distributed HPO + each trial using multiple gpus? This was possible with the `ray-lightning` plugin but not sure if it\u2019s still the case with the substitution"}
{"question": "Hey everyone,\n\nI was running <https://github.com/microsoft/AMRL-ICLR2020/blob/main/experiments/maze_runner.py|this> script which builds a Python List of <https://docs.ray.io/en/latest/tune/api/doc/ray.tune.Experiment.html|ray.tune.Experiment> on <https://github.com/microsoft/AMRL-ICLR2020/blob/main/experiments/maze_runner.py#L453|line number 453> and calls <https://docs.ray.io/en/latest/tune/api/doc/ray.tune.run_experiments.html#ray-tune-run-experiments|ray.tune.run_experiments()> on <https://github.com/microsoft/AMRL-ICLR2020/blob/main/experiments/maze_runner.py#L574|line number 574>.\n\nBut, while running the script, I am getting the below error from <https://docs.ray.io/en/latest/tune/api/doc/ray.tune.Experiment.from_json.html#ray.tune.Experiment.from_json|ray.tune.Experiment.from_json()> function at <https://github.com/microsoft/AMRL-ICLR2020/blob/main/experiments/maze_runner.py#L356|line number 356>.\n```Warning: grid search will not be performed; using lr and optimizer from config...\nMetal device set to: Apple M1 Pro\n\nsystemMemory: 16.00 GB\nmaxCacheSize: 5.33 GB\n\n\nUsing config:\n\nrun: PPO\ncheckpoint_freq: 5\ncheckpoint_at_end: True\nstop\n    timesteps_total: 250000\nconfig\n    opt_type: adam\n    lr: 0.0005\n    gamma: 0.98\n    num_workers: 5\n    num_envs_per_worker: 2\n    num_gpus: 1\n    observation_filter: NoFilter\n    sample_batch_size: 205\n    train_batch_size: 4000\n    sgd_minibatch_size: 200\n    num_sgd_iter: 30\n    batch_mode: truncate_episodes\n    fetch_lstm_gates: False\n    model\n        use_lstm: True\n        max_seq_len: 10000\n        slot1: lstm\n        slot2: avg\n        sum_instead: False\n        max_instead: True\n        straight_through: True\n    env_config\n        check_up: 1\n        check_down: -1\n        maze_length_upper_bound: None\n        pos_enc: False\n        wave_encoding_len: None\n        task_switch_after: None\n        intermediate_checks: False\n        intermediate_indicators: True\n        reset_intermediate_indicators: True\n        per_step_reset: True\n        num_indicators_components: 1\n        frac_correct_components_for_check: 1\n        final_intermediate_indicator: True\n        check_reward: 0.1\n        reward_per_correct_component: False\n        allow_left: False\n        force_final_decision: True\n        force_right: True\n        timeout: 150\n        timeout_reward: 0\n        maze_length: 100\n        indicator_pos: 0\n        flipped_indicator_pos: None\n        correlated_indicator_pos: None\n        success_reward: 4.0\n        fail_reward: -3.0\n        persistent_reward: 0.0\n    callbacks\n        on_episode_end: &lt;function prep_for_saving_config_and_callback_data.&lt;locals&gt;.&lt;lambda&gt; at 0x2a6972790&gt;\nTrial_Name: T-LN_EXAMPLE_model-AMRL-Max_RUN1\nlocal_dir: /Users/sid/rl_attention/AMRL_results/data\nenv: tmaze-v0\nTraceback (most recent call last):\n  File \"/Users/sid/miniconda3/envs/rl/lib/python3.9/site-packages/ray/tune/experiment/experiment.py\", line 304, in from_json\n    exp = cls(name, run_value, **spec)\nTypeError: __init__() got an unexpected keyword argument 'checkpoint_freq'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/sid/rl_attention/maze_runner.py\", line 529, in &lt;module&gt;\n    main()\n  File \"/Users/sid/rl_attention/maze_runner.py\", line 523, in main\n    all_experiments = get_experiments(yaml_path_env_path_tups, save_dirs_successful, args)\n  File \"/Users/sid/rl_attention/maze_runner.py\", line 455, in get_experiments\n    new_experiment = get_experiment_dict_from_yaml(yaml_path, run_num, clust=args.clust, existing_save_dirs=save_dirs_successful,\n  File \"/Users/sid/rl_attention/maze_runner.py\", line 437, in get_experiment_dict_from_yaml\n    return tune.Experiment.from_json(experiment_name, d)\n  File \"/Users/sid/miniconda3/envs/rl/lib/python3.9/site-packages/ray/tune/experiment/experiment.py\", line 306, in from_json\n    raise TuneError(\nray.tune.error.TuneError: Failed to load the following Tune experiment specification:\n\n {'checkpoint_at_end': True,\n 'checkpoint_freq': 5,\n 'config': {'batch_mode': 'truncate_episodes',\n            'callbacks': {'on_episode_end': &lt;function prep_for_saving_config_and_callback_data.&lt;locals&gt;.&lt;lambda&gt; at 0x2a6972790&gt;},\n            'env': 'tmaze-v0',\n            'env_config': {'allow_left': False,\n                           'check_down': -1,\n                           'check_reward': 0.1,\n                           'check_up': 1,\n                           'correlated_indicator_pos': None,\n                           'fail_reward': -3.0,\n                           'final_intermediate_indicator': True,\n                           'flipped_indicator_pos': None,\n                           'force_final_decision': True,\n                           'force_right': True,\n                           'frac_correct_components_for_check': 1,\n                           'indicator_pos': 0,\n                           'intermediate_checks': False,\n                           'intermediate_indicators': True,\n                           'maze_length': 100,\n                           'maze_length_upper_bound': None,\n                           'num_indicators_components': 1,\n                           'per_step_reset': True,\n                           'persistent_reward': 0.0,\n                           'pos_enc': False,\n                           'reset_intermediate_indicators': True,\n                           'reward_per_correct_component': False,\n                           'success_reward': 4.0,\n                           'task_switch_after': None,\n                           'timeout': 150,\n                           'timeout_reward': 0,\n                           'wave_encoding_len': None},\n            'fetch_lstm_gates': False,\n            'gamma': 0.98,\n            'lr': 0.0005,\n            'model': {'max_instead': True,\n                      'max_seq_len': 10000,\n                      'slot1': 'lstm',\n                      'slot2': 'avg',\n                      'straight_through': True,\n                      'sum_instead': False,\n                      'use_lstm': True},\n            'num_envs_per_worker': 2,\n            'num_gpus': 1,\n            'num_sgd_iter': 30,\n            'num_workers': 5,\n            'observation_filter': 'NoFilter',\n            'opt_type': 'adam',\n            'sample_batch_size': 205,\n            'sgd_minibatch_size': 200,\n            'train_batch_size': 4000},\n 'local_dir': '/Users/sid/rl_attention/AMRL_results/data',\n 'stop': {'timesteps_total': 250000},\n 'trial_name_creator': &lt;function get_experiment_dict_from_yaml.&lt;locals&gt;.&lt;lambda&gt; at 0x2a6972670&gt;}.\n\nPlease check that the arguments are valid. Experiment creation failed with the following error:\n __init__() got an unexpected keyword argument 'checkpoint_freq'```\nI also logged the Python Dictionary I am passing as `spec` parameter passed to the `ray.tune.Experiment.from_json()` function. Please find below the same.\n```{'run': 'PPO', 'checkpoint_freq': 5, 'checkpoint_at_end': True, 'stop': {'timesteps_total': 250000}, 'config': {'opt_type': 'adam', 'lr': 0.0005, 'gamma': 0.98, 'num_workers': 5, 'num_envs_per_worker': 2, 'num_gpus': 0, 'observation_filter': 'NoFilter', 'sample_batch_size': 205, 'train_batch_size': 4000, 'sgd_minibatch_size': 200, 'num_sgd_iter': 30, 'batch_mode': 'truncate_episodes', 'fetch_lstm_gates': False, 'model': {'use_lstm': True, 'max_seq_len': 10000, 'slot1': 'lstm', 'slot2': 'avg', 'sum_instead': False, 'max_instead': True, 'straight_through': True}, 'env_config': {'check_up': 1, 'check_down': -1, 'maze_length_upper_bound': None, 'pos_enc': False, 'wave_encoding_len': None, 'task_switch_after': None, 'intermediate_checks': False, 'intermediate_indicators': True, 'reset_intermediate_indicators': True, 'per_step_reset': True, 'num_indicators_components': 1, 'frac_correct_components_for_check': 1, 'final_intermediate_indicator': True, 'check_reward': 0.1, 'reward_per_correct_component': False, 'allow_left': False, 'force_final_decision': True, 'force_right': True, 'timeout': 150, 'timeout_reward': 0, 'maze_length': 100, 'indicator_pos': 0, 'flipped_indicator_pos': None, 'correlated_indicator_pos': None, 'success_reward': 4.0, 'fail_reward': -3.0, 'persistent_reward': 0.0}}, 'local_dir': '/home/siddhantsahu/Desktop/rl/POMDP-RL/AMRL_results/data', 'env': 'tmaze-v0'}```\nI was wondering what might be missing here? What is correct way to create an object of  `ray.tune.Experiment` with the help of it's `from_json()` method?"}
{"question": "Hello everyone, unfortunately I couldn't find a straightforward answer to my question in Ray discussions and other popular online forums so maybe I can get some clarity here.\n\nHere is my situation: I have a very large numpy array which is stored in Ray Object Store Memory.  I have multiple Ray workers (using joblib with Ray as backend) which are using *only subsets* of that numpy array and are not changing anything in the original numpy array(even if they could).\nI am passing each worker numpy array as ObjectRef and then inside the worker I have something like:\n`data = ray.get(data_ref)[:, col_idx]`\n\nMy question: is there a way to just grab a subset/part of numpy array from Object Store Memory to avoid moving large amounts of unnecessary data to worker heap memory?\n\nMy idea was to store numpy array as an Actor attribute and implement a getter method for that Actor to return only a subset of numpy array it is holding. On the other hand, I am not sure if it is a smart idea to store large objects in Actors.\n\nWhat would be the best approach to reduce memory footprint of my run? Thank you!"}
{"question": "Is there a timeout or retry setting for long a worker will wait / retry to make an initial connection to the head node?  In our environment, ray head and worker pods are kicked off in k8s but due to auto-scaling their actual scheduling / start times vary widely."}
{"question": "Hello- qq about this page <https://docs.ray.io/en/latest/data/dataset.html#datasets-for-parallel-compute>\n\nThey provide a higher-level API for Ray tasks and actors for such embarrassingly parallel compute, internally handling operations like batching, pipelining, and memory management.\n\n^ My coworkers and I are wondering what does _embarrassingly_ parallel compute mean?"}
{"question": "Hello, is there an example of how I can run open-source LLM with an API endpoint using Ray and AWS Spot instances (either EC2 or Fargate spot)?\n<https://github.com/go-skynet/LocalAI> I would like to deploy LocalAI not sure how Ray fits here"}
{"question": "Hey Folks! My training job was running for 2+ days using AWS VM but then  it ran into this error:\n```ERROR trainable.py:597 -- Could not upload checkpoint even after 3 retries.Please check if the credentials expired and that the remote filesystem is supported.. For large checkpoints, consider increasing `SyncConfig(sync_timeout)` (current value: 1800 seconds). Checkpoint URI: <s3://a9vs-photon-fsx-data-repository/sync-with-fsx/Results/parisexp/TorchTrainer_2023-05-15_22-41-59/TorchTrainer_b3daa_00000_0_2023-05-15_22-42-00/checkpoint_000002>```\nThe job is still running in some way but I do not see logs from ray any more indicating stuff like trial name, status and stuff. The VM is using IAM role which still has good s3 permissions. Any idea on how to fix it? This will become a blocker for training long running jobs. My sync config looks like:\n```SyncConfig(upload_dir=\"<s3://a9vs-photon-fsx-data-repository/sync-with-fsx/Results/parisexp>\")```"}
{"question": "Hi All...... am installing a Raycluster in Kubernetes....... is there any specific setup for the GCS or autoscaler for TLS?  I keep seeing the following error in the autoscaler logs:\n```Traceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/gcs_utils.py\", line 124, in check_health\n    resp = stub.CheckAlive(req, timeout=timeout)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/grpc/_channel.py\", line 946, in __call__\n    return _end_unary_response_blocking(state, call, False, None)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/grpc/_channel.py\", line 849, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)\ngrpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:x.x.x.x:6379: Socket closed\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNAVAILABLE: ipv4:x.x.x.x:6379: Socket closed {grpc_status:14, created_time:\"2023-05-18T13:59:54.630853143-07:00\"}\"```"}
{"question": "Hi All :wave:, has anyone successfully registered and used a custom gym environment with rllib on databricks using databricks notebooks (instead of the package structure specified, or at least using .nby instead of .py files: <https://www.gymlibrary.dev/content/environment_creation/#creating-a-package>)?\n\nNot sure if this is possible... I unfortunately have a constraint where I can only run code with .nby files, and wondering if anyone has tried to do this and been successful?  Thank you in advance!"}
{"question": "Hi Folks, is there a roadmap for Python 3.11 support? I see that docs show as \u201cExperimental\u201d for linux at the moment."}
{"question": "Hello chat!\n\nI'm playing around with ray jobs. In one of the scripts I have there is blocking call (it can be short polling) to 3rd party api which might hang for a long time. I was wondering is it possible somehow release resources while waiting for response? Let's say my job has 4 cpu and 8 gb mem resource requirements, but because of this blocking call I'm not using them at all."}
{"question": "Hi, this might be the wrong channel for this, but I am wondering if it would be possible to attend Ray summit remotely this year? Could not find any information about it on the page, but we in my team will not be able to attend but are very interested in some of the talks!"}
{"question": "What is the difference between the HuggingFacePredictor and BatchPredictor? Is there an incentive for me to use one over the other?"}
{"question": "Hey folks! Why are you sunsetting `local_mode`? We\u2019ve been using it for our testing pipeline to minimize memory footprint, which has been awesome."}
{"question": "Hi! Somebody knows if apple M2 is support by Ray?\nWhen I run a default basic app it seems not to exploit my GPUs in my apple M2 Max\nThank you in advance for any answer"}
{"question": "I see the <https://docs.ray.io/en/latest/cluster/running-applications/job-submission/doc/ray.job_submission.JobSubmissionClient.html|ray.job_submission.JobSubmissionClient> in Ray jobs python sdk has `cookies`  and `headers` arguments. How do we pass cookies and headers using `ray job` cli?"}
{"question": "Hi! I\u2019m trying to get a Flask app that calls some ray\u2019s processes through ray.remote to run in Apache. However, I get this error: *OSError: Apache/mod_wsgi log object is not associated with a file descriptor*. Any tips as to how to sort this issue? Thanks!"}
{"question": "Hi Please is there a way to stop the Kuberay Operator from restarting ever so frequently?"}
{"question": "Hello, I\u2019m seeing a weird issue where my cluster mem utilization stays high (30%) even without any jobs running. Any ideas on what would cause ray to hold on to memory here?"}
{"question": "Is there a way to not reuse python processes on worker nodes?"}
{"question": "Hi! I have an observation_space of variable length (ray.rllib.utils.spaces.repeated.Repeated) and I want to create an action_space with also a variable_length that adopts in each step the length of the observation space. That is, in step 1, the action_space can generate [1,0,1] and in the next step, as the new observation_space has length 5, the action_space will generate [1,0,1,1,0] How can I do it?"}
{"question": "Hi all!\nWe have been struggling with slowdowns of ray tune with big experiments (as in many trials) and have been trying to pin down the issue. We followed a lot of the general advice, but have found something we can't quite explain.\n\nThere is a test for bookkeeping overhead here:\n\u2022 <https://github.com/ray-project/ray/blob/master/release/tune_tests/scalability_tests/workloads/test_bookkeeping_overhead.py>\nI did rerun this test on our local infrastructure with more CPUs and it finished in about 500 seconds, so pretty much as expected (theoretical optimum would have been around 400).\nI then reran the very same test but with 200k instead of 10k samples, everything else being equal. There is a massive slowdown around 100k trials:\n\u2022 After about 1h of runtime, about 100k experiments had been reached\n\u2022 After about 4h of runtime, about 130k experiments have been reached\n\u2022 After about 8h of runtime, the full 200k were reached\nThis is roughly in line with what we saw in our production code: Tehre is some magic number of experiments after which experiments slow down to a crawl. Our intuition was that this is due to IO being slow with large files being written. In our production, this also happens when most logging is turned off via\n```TUNE_GLOBAL_CHECKPOINT_S=1e33 \\\nTUNE_DISABLE_AUTO_CALLBACK_LOGGERS=1```\nAny idea what's going on?\n\nI ran the test in today's git HEAD using today's nighly.\n\nThanks :slightly_smiling_face:"}
{"question": "hey, I'm calling a c++ binary in parallel tasks on my ray cluster with the following pattern\n\n```@ray.remote(num_cpus=1, memory=25e9)\ndef foo(x):\n    cmd = [my, command, here]\n    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\nray.get([foo.remote(x) for x in x_list])```\nthese are running on `m5.4xlarge` which have `64gb` so I would expect to pack two tasks per node. this is true when not redirecting stdout, stderr, but it seems when I redirect to `DEVNULL` then now it will only set one task per node, so I am using twice as many nodes as needed which adds cost. any ideas what causes this or how to mitigate?"}
{"question": "Hi everyone!\n\nDoes anybody know if it's possible to check how many memory/object store memory is left on a specific node in a Ray Cluster? I can check the whole cluster with `ray.available_resources()` but not a specific node before doing `ray.put(data)`. Can somebody help me?\n\nThank you in advance."}
{"question": "Hi, did someone run into this? <https://github.com/ray-project/ray/issues/35537> I believe most development is using ray client with Mac vs Linux k8s setup.\nMaybe an intermediate solution?"}
{"question": "Hello guys, any resources on how can we utilize M2 and the Apple nural engine from within ray ? "}
{"question": "Hello\ndoes any one has pointer to bake application logic in container and run on Ray cluster ?\ni get my app logic working with Ray Actor along with conda runtime , Runtime is expensive to create on each job submission , since its having 500 library . any pointers /tutorials will be helpful, Thanks!"}
{"question": "Hi all,\n\nDo people know of a roadmap for Ray development that is shown publicaly?\nI am thinking of set of features and what the forthcoming version releases aim to include in terms of improved functionality.\n\nMany thanks"}
{"question": "Is there a way to programatically retrieve the address that\u2019s passed to `ray.init`  ?"}
{"question": "How do I get useful information displayed for the disk metrics on the dashboard?"}
{"question": "Hi Folks! I would like to bring your attention to this issue where multiple network interfaces (like EFA)  cannot be attached to an instance in a distributed training setting. This will lead to performance impact <https://github.com/ray-project/ray/issues/33586>. Could you folks address this as a priority ?"}
{"question": "Is it possible to make RLLib use CUDA instead of CPU?  Also when will the documentation be back up?"}
{"question": "Hi,\nI am trying to update Ray from 2.1.0 to 2.4.0. I am using `rayproject/ray:2.4.0-py38` as the `operatorImage`.\nBut operator node (k8s pod) fails to start. The events on the pod is as follows -\n```Normal   Scheduled            5m26s                default-scheduler  Successfully assigned ray-ml-us-east-1-qa-01-v3/ray-operator-76bbfc6c89-vf9tw to ip-10-50-52-87.ec2.internal\nNormal   Pulled               4m41s                kubelet            Successfully pulled image \"rayproject/ray:2.4.0-py38\" in 43.697886902s (43.697895951s including waiting)\nWarning  Failed               2m41s                kubelet            Error: context deadline exceeded\nNormal   Pulled               2m26s                kubelet            Successfully pulled image \"rayproject/ray:2.4.0-py38\" in 14.041232263s (14.041238392s including waiting)\nWarning  Failed               83s                  kubelet            Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to create new parent process: namespace path: lstat /proc/0/ns/ipc: no such file or directory: unknown\nNormal   Pulled               83s                  kubelet            Successfully pulled image \"rayproject/ray:2.4.0-py38\" in 103.755666ms (103.769559ms including waiting)\nNormal   Created              65s (x4 over 4m41s)  kubelet            Created container ray\nNormal   Pulling              65s (x4 over 5m25s)  kubelet            Pulling image \"rayproject/ray:2.4.0-py38\"\nNormal   Pulled               65s                  kubelet            Successfully pulled image \"rayproject/ray:2.4.0-py38\" in 158.679691ms (158.690317ms including waiting)\nWarning  Failed               32s (x2 over 82s)    kubelet            Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"ray-operator\": executable file not found in $PATH: unknown\nWarning  BackOff              32s (x3 over 81s)    kubelet            Back-off restarting failed container\nWarning  Evicted              22s                  kubelet            The node was low on resource: ephemeral-storage.\nWarning  ExceededGracePeriod  12s                  kubelet            Container runtime did not kill the pod within specified grace period.```\nHas anyone faced this?"}
{"question": "Simple question regarding (anti-)patterns; when using Ray Actors, is it considered bad practice to use global variables for _reading definitions_ (not sharing state, just e.g. a configuration object that\u2019s identical for all actors)?"}
{"question": "Hey - I have started experiencing the following logs:\n```Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/ray/_private/worker.py\", line 2182, in connect\n    node.check_version_info()\n  File \"/usr/local/lib/python3.8/site-packages/ray/_private/node.py\", line 339, in check_version_info\n    cluster_metadata = ray_usage_lib.get_cluster_metadata(self.get_gcs_client())\n  File \"/usr/local/lib/python3.8/site-packages/ray/_private/usage/usage_lib.py\", line 711, in get_cluster_metadata\n    gcs_client.internal_kv_get(\n  File \"python/ray/_raylet.pyx\", line 2167, in ray._raylet._auto_reconnect.wrapper\n  File \"python/ray/_raylet.pyx\", line 2155, in ray._raylet._auto_reconnect.wrapper\n  File \"python/ray/_raylet.pyx\", line 2220, in ray._raylet.GcsClient.internal_kv_get\n  File \"python/ray/_raylet.pyx\", line 466, in ray._raylet.check_status\nray.exceptions.RpcError: failed to connect to all addresses\n(raylet, ip=10.250.144.216) Failed to connect to GCS. Please check `gcs_server.out` for more details.```\nWhat is it about? Do you have any ideas about what is causing it? Or if it has a negative effect on performance?\nI am currently training, using the Tensorflow Training, on a Ray cluster, with streaming datasets. There is quite a lot of throughput, about 1000 rows a second, split across a 4-stage data pipeline, ending up with about 50 actors on the same worker. Also, currently using Ray nightly.\nThnx"}
{"question": "Is there an ETA on 2.5.0?"}
{"question": "Trying to parallelize feature preparation task with Prefect and Ray. If I use Prefect\u2019s RayTaskScheduler, everything runs fine, except I have very little control on the scheduling itself.\nSo I\u2019m trying to use Ray actors for the same purpose, but that keeps crashing with OOM (though in the dashboard I never see that happening, so I assume it\u2019s a peak memory consumption).\n\nWhy is prefect able to run these tasks on Ray but Ray alone can\u2019t? In both cases I set the `num_cpus=1` to ensure only one task is run at a time (to better understand this issue)."}
{"question": "Hi. Please what is new with Ray 2.5.0?"}
{"question": "Does anyone use Ray to run batch jobs on ephemeral clusters using AWS resources? For example, if I have a bunch of data to process, I'd love a workflow that (1) spins up a cluster with parameters that might depend on the input (e.g. number or size of input files) (2) runs the job and (3) reliably tears down the cluster. Any tips on best practices to implement this approach?"}
{"question": "HI Folks! How can I use a pytorch .pth checkpoint with ray Checkpoint ? How do I load it ?"}
{"question": "Is there a recommended way to use pybind11 objects with ray? I\u2019m sending an object to a remote, but when I try to use it from c++ I get 2 errors:\n\u2022 from pybind11: RuntimeError: Unable to cast from non-held to held instance (T&amp; to Holder&lt;T&gt;) \n\u2022 from ray: ValueError: An application is trying to access a Ray object whose owner is unknown(c8ef45ccd0112571ffffffffffffffffffffffff0100000001000000 )"}
{"question": "I\u2019ve set up a Ray cluster on AWS. I\u2019m running the same Ray version (latest, 2.5.0) both on the cluster and locally, and both instances use the same image and same Python version.\n\nWhen I run my `ray.init(address=\"ray://&lt;ip&gt;:10001\", runtime_env=dict(pip=[\u2026]))`, I get the following error:\n```AttributeError: module 'lib' has no attribute 'OpenSSL_add_all_algorithms'```\nI\u2019ve tried downgrading `cryptography` (as some SO thread suggests), and upgrading it and openssl, etc. Same end result. Am I missing some configuration for the head node perhaps?\n\nRunning without docker, if that matters."}
{"question": "I\u2019m trying to use ray with an on-prem cluster, and there are so many crashes. Is there a way to print out the ssh command that ray is running to access nodes?"}
{"question": "I guess aiohttp should be added to ray deps?"}
{"question": "Running Ray Cluster on AWS using Docker (`rayproject/ray:nightly-py310-cpu` image). I\u2019ve tried also with the default config (`full_example_config.yaml`) - i.e. without subnet or security groups specified - but I keep getting:\n```ERROR utils.py:1391 -- Failed to connect to GCS. Please check `gcs_server.out` for more details.\nWARNING utils.py:1397 -- Unable to connect to GCS at :6379. Check that (1) Ray GCS with matching version started successfully at the specified address, and (2) there is no firewall setting preventing access.```\nAre there some additional AWS-specific configuration settings that are missing? Something in the Iam role or similar?"}
{"question": "Hello Team, any thoughts on the below error ?\n```ConnectionError: Request can't be sent because the Ray client has already been disconnected due to an error. Last exception: &lt;_MultiThreadedRendezvous of RPC that terminated with:\nstatus = StatusCode.NOT_FOUND\ndetails = \"Attempted to reconnect a session that has already been cleaned up\"\ndebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.21.65.139:10001 {grpc_message:\"Attempted to reconnect a session that has already been cleaned up\", grpc_status:5, created_time:\"2023-06-13T15:46:50.165786191+00:00\"}\"```"}
{"question": "I\u2019m trying to train via rllib, and I find that if I don\u2019t specify anything about the resources, it discovers my GPU and does training on the GPU, but is bottlenecked by doing sample on a single thread.  If I specify rollouts(num_rollout_workers=num_cpus) it runs much faster as it now does the sampling on multiple threads, but it no longer uses my GPU for training and does that on the CPU instead.  Also I think there\u2019s a non-neglible overhead from spawning all of these ray processes.\n\n\nI have tried setting num_cpus_for_local_worker but that doesn\u2019t seem to work.  Is there some way to get it to sample in parallel using CPU but still use the GPU for training?"}
{"question": "Hi, are there any plans for NodeJS Javascript/Typescript interop?"}
{"question": "Hey guys. I\u2019m having a problem implementing a custom attention network. Here\u2019s the context. I\u2019m working on adapting a custom attention network based on the paper: FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting\nLink: <https://arxiv.org/abs/2202.07125|[2202.07125] Transformers in Time Series: A Survey>\nGithub: <https://github.com/MAZiqing/FEDformer|GitHub - MAZiqing/FEDformer>\nThe issue I\u2019m having is that the trial is super slow when using this attention model. In comparison, when using the default Rllib LSTM or Attention models with a fully connected layers of size [[1024,2048, 2048, 4096, 4096, 2048, 2048,1024,512]] (150 million parameters), the time per iteration is 300 seconds on average when the train_batch size is 250. However, using this model (50 million parameters) takes 90 minutes per iteration on average.\nSo I have some questions:\n\n1. What part of the adaptation could be causing the trial to take long per iteration?\n2. What determines how long the time per iteration in Rllib? Here's more details about my issue: <https://discuss.ray.io/t/time-per-iteration-is-high-when-using-a-custom-model/11017>"}
{"question": "Hi, everyone. I was looking at the example config file: <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/example-full.yaml>. Here m5.large is used which has no GPU. Does this mean setting `num_gpus=1` will not have any effect?"}
{"question": "Hi, I have a question about accessing attribute on ray actors. In order to access an attribute on an actor, you need to create a method for that like that get_counter in the following example.\n```import ray\n\n@ray.remote\nclass Counter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\n    def get_counter(self):\n        return self.value\n\n# Create an actor from this class.\ncounter = Counter.remote()```\nHowever my use case is this, I want to get an attribute on an arg that is passed to my class, any idea why this is not working?\n```# my_module.py\n@ray.remote\nclass MyClass:\n    def __init__(self, dependent: DependentClass):\n        self.dependent = dependent\n    def update_dependent(self):\n        self.dependent.attribute = 5\n    def get_dependent_attribute(self):\n        return self.dependent.attribute\n    ```\nAnd in the my driver script:\n```from my_module import MyClass, DependentClass\nmy_remote_class = MyClass(dependent=DependentClass())\n\nresult_ref = my_remote_class.update_dependent.remote()\nray.get(result_ref)\n\nattribute_ref = my_remote_class.get_dependent_attribute.remote()\nattribute = ray.get(attribute_ref)\n\nprint(attribute)  # The attribute is not updated ```"}
{"question": "Is there a way to catch failure in ray (OOM, worker died, etc) and have the run continue and retry that task, rather than crashing the entire job?"}
{"question": "I\u2019m reading this example, which is a very basic one with workers, and then wondering, ok, how do I collect back the data that\u2019s been sharded out at end? (not the case with training but other dataset manipulation) <https://docs.ray.io/en/latest/data/iterating-over-data.html>"}
{"question": "In terms of ray logging/metrics, can one add custom files? For example, we\u2019d like to generate and keep an updated map of ongoing GIS operations distributed using Ray actors."}
{"question": "Hello everyone,\nIs it possible to set the `checkpoint_score_attribute` in the `CheckpointConfig` to be an evaluation metric?\n\nI tried but I get the `Result dict has no key` error. Could it be because the eval metrics are present  once every `evaluation_interval` iteration while checkpoints are taken every `checkpoint_frequency` ?"}
{"question": "We're looking into the concept of setting up a permanent RayCluster hosted on kubernetes, but one thing that i havent found on documentation was the concept of access control -&gt; how do you limit who can call the cluster?"}
{"question": "In <https://docs.ray.io/en/latest/ray-observability/user-guides/configure-logging.html#customizing-prefixes-for-actor-logs|customizing the prefix for actors>, the example suggests one can use a dynamic attribute for the prefix. It seems however that the representation is captured only once and never refreshed.\nFor example, we reutilize workers so the example in the documentation could be updated to:\n```@ray.remote\nclass MyActor:\n    def __init__(self, index):\n        self.index = index\n\n    def foo(self, index=None):\n        if index is not None:\n            self.index = index\n        print(\"hello there\")\n\n    def __repr__(self):\n        return f\"MyActor(index={self.index})\"\n\na = MyActor.remote(1)\nb = MyActor.remote(2)\nray.get(a.foo.remote())\nray.get(b.foo.remote())\nray.get(b.foo.remote(3))\n\n# output will be\n# (MyActor(index=2) pid=482120) hello there\n# (MyActor(index=1) pid=482119) hello there\n# (MyActor(index=2) pid=482120) hello there  # &lt;--- at this point we would like this to be (MyActor(index=3) ...)```\nIs there some way to refresh the representation? If it\u2019s static, perhaps the documentation should at least mention it?"}
{"question": "Does <https://docs.ray.io/en/latest/ray-observability/user-guides/ray-tracing.html#ray-tracing|Ray Tracing> work with Java actors?"}
{"question": "Hey, Guyz Can anyone help me Setting Up Ray Project in my machine?"}
{"question": "I want to use ray jobs to manage a bunch of gpus, so that users run `ray job submit` and ray assigns them a GPU, but if I submit multiple jobs at once, the order of execution isn't in any deterministic order. Is there a way to enforce FCFS/Queue behavior for jobs?"}
{"question": "Has anyone here ever tried to serialize an `ObjectRef`  to disk and then deserialize it elsewhere?Is it possible?"}
{"question": "Can somebody do me a favour? I need a benchmark of the policy server example found at <https://docs.ray.io/en/latest/rllib/rllib-env.html#external-agents-and-applications>\n\nPlease run the server.py and client.py scripts and tell me how long a request takes to be processed, as well as what environment/hardware you're running. It's a constant 2 seconds for me, whether I log_returns or get_action"}
{"question": "We seem to be running into <https://github.com/ray-project/ray/issues/7724> -- this is an engineer running in a Linux container under Docker for Desktop on MacOS (long story).  We're trying to use `_temp_dir`  in ray.init() to overcome it, and wondering:\n\u2022 Is that a good idea?\n\u2022 How should we select a directory?\n\u2022 Any word on a fix -- looks like a longstanding issue that's been updated a few weeks ago..."}
{"question": "Ray seems to busy-wait a lot, even when there are no jobs running. Is this a known issue?"}
{"question": "Hi everyone,\nI am deploying and testing Ray Cluster in k8S for my company and I have some questions because I have some issues.\n\nI  am trying to train a model with the ray_train.py script below (based on <https://docs.ray.io/en/latest/train/train.html>). in the head node, I launch the script with `python ray_train.py`\nI got this error :\n```RuntimeError: Trying to shard data for 2 actors, but the maximum number of shards is 1. If you want to shard the dataset by rows, consider centralized loading by passing `distributed=False` to the `RayDMatrix`. Otherwise consider using fewer actors or re-partitioning your data.```\nSo I switch to 10 partititons by changing :\n```dataset = ray.data.read_csv(\"<s3://anonymous@air-example-data/breast_cancer.csv>\")```\nto :\n```dataset = ray.data.read_csv(\"<s3://anonymous@air-example-data/breast_cancer.csv>\").repartition(10)```\n\n*Is it the good way to handle this ?*\n*if yes, how can I determine the number of partition ?*\n\nBy doing repartition, it takes a lot of time compare to train with 1 worker and 1 shard but it works with 2 workers and 10 partitions.\n\nWhen I increased the num of workers to 4 and 10 partitions :\n\n```f\"Trying to shard data for {num_actors} actors, but the \"\nRuntimeError: Trying to shard data for 4 actors, but the maximum number of shards is 3. If you want to shard the dataset by rows, consider centralized loading by passing `distributed=False` to the `RayDMatrix`. Otherwise consider using fewer actors or re-partitioning your data.```\nIn comments, my K8S config and Ray Cluster config\n\nThank your for your help."}
{"question": "`ray rsync-up cluster_config.yaml` isn't syncing `file_mounts` config to workers. Why? This is apparently the recommended approach, according the the warning received when using `--all-nodes`."}
{"question": "Hi i encountered an error of the sort:\n```ay.exceptions.OwnerDiedError: Failed to retrieve object 00e933709b9d591effffffffffffffffffffffff0200000002000000. To see information about where this ObjectRef was created in Python,```\nGiven a ref, is there a clean way to figure out whether the owner is still alive?"}
{"question": "Hey! I'm trying to run a Flyte workflow with a Ray task. I can see the RayCluster and RayJob CRs being created. Cluster creation is successful, but the job fails due to some application errors. Running `kubectl describe rayjob ...` only outputs a truncated error stack. How can I view the complete error stack for a `RayJob`?"}
{"question": "Hey all. I'm trying to figure out how to use the `ray job submit` command to submit a bazel built python target. Does anyone have experience doing this?"}
{"question": "HI Folks! I don't see reference to TensoryArray in the main links of the docs anymore . Is that all tensors are now supposed to be returned as numpy arrays ? <https://docs.ray.io/en/latest/data/working-with-tensors.html#tensor-data-representation>"}
{"question": "im not sure what this error is:\n```10168[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m 2023-06-24 00:11:14,327\tERROR worker.py:844 -- Worker exits with an exit code None. The worker may have exceeded K8s pod memory limits.\n10169[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m  Traceback (most recent call last):\n10170[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m   File \"python/ray/_raylet.pyx\", line 1197, in ray._raylet.task_execution_handler\n10171[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m   File \"python/ray/_raylet.pyx\", line 1100, in ray._raylet.execute_task_with_cancellation_handler\n10172[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m   File \"python/ray/_raylet.pyx\", line 759, in ray._raylet.execute_task\n10173[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m AttributeError: 'dict' object has no attribute 'function_name'\n10174[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m An unexpected internal error occurred while the worker was executing a task.\n10175[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m Traceback (most recent call last):10176[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m   File \"python/ray/_raylet.pyx\", line 1197, in ray._raylet.task_execution_handler\n10177[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m   File \"python/ray/_raylet.pyx\", line 1100, in ray._raylet.execute_task_with_cancellation_handler\n10178[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m   File \"python/ray/_raylet.pyx\", line 759, in ray._raylet.execute_task\n10179[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m AttributeError: 'dict' object has no attribute 'function_name'\n10180[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m During handling of the above exception, another exception occurred:\n10181[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m Traceback (most recent call last):10182[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m   File \"python/ray/_raylet.pyx\", line 1238, in ray._raylet.task_execution_handler\n10183[2m[36m(ScanProvider pid=371, ip=10.17.3.7)[0m SystemExit```\nAny ideas what may cause this?"}
{"question": "Hey everyone, I'm comparing Ray vs Dask performance for an identical many-model type train/inference workloads for 1000 models and 4k data sources (I am also writing a blog post about this). In both Ray and Dask, I am using the same number of CPU and GPU workers/resource management. After quite a few tests and configurations, I came to a conclusion that the worker threading capabilities of Dask are enabling **much** higher performance (more than an order of magnitude faster than Ray). This conclusion stems from observing that reducing the allowed threads per Dask worker to 1 thread each, Dask slows down to almost the same performance as Ray.\n\nMy setup is relatively basic, we build our list of remotes, which includes a parameterization across functions like `train(city)` and `predict(city)`, collect the `task_refs` and then call `get` on the final one. Pretty much the same as the Batch training example <https://docs.ray.io/en/latest/ray-core/examples/batch_training.html>.\n\nI am a bit confused, this high-frequency task switching is something that Ray sells itself for [many model training/inference](<https://www.anyscale.com/blog/training-one-million-machine-learning-models-in-record-time-with-ray>), so am I missing something?\n\nKeep in mind that although Dask provides multiple threads per worker, it still respects the resource restrictions. So in this case, we have 6 GPUs and 128 physical CPUs and only 6 models are ever being trained at any given time in Dask or Ray. It is just that Dask seems to be much faster at task switching/preparation by using the worker threads.\n\nThe main reason I began converting to Ray was to take advantage of the object store, which would theoretically enable faster loading times of the trained model objects. But it would appear that the time spent task switching in Ray is dominating this many model configuration beyond any savings that the object store might provide.\n\nMy question is this: what is the best practice to improve Ray performance in a many model setting? Can we somehow enable threading for the Ray workers, similar to how Dask is providing threading? Am I missing something in how to properly setup Ray?"}
{"question": "Hey folks! Are the logs generated by the workers ever rotated on a VM based setup ? Or they have the potential of filling up the disk for a long running cluster ?"}
{"question": "Hey! I got test_advanced_9.py failed in <https://github.com/ray-project/ray/pull/36153|PR>.\nThe error message is:\n&gt; E                       ray.exceptions.RayTaskError(AssertionError): ray::f() (pid=19313, ip=172.16.16.3)\n&gt; E                         File \"/root/.cache/bazel/_bazel_root/5fe90af4e7d1ed9fcf52f59e39e126f5/execroot/com_github_ray_project_ray/bazel-out/k8-opt/bin/python/ray/tests/test_advanced_9.runfiles/com_github_ray_project_ray/python/ray/tests/test_advanced_9.py\", line 430, in f\n&gt; E                           assert pool_info[\"num_threads\"] == 2\n&gt; E                       AssertionError: assert 4 == 2\nFull message link: <https://buildkite.com/ray-project/oss-ci-build-pr/builds/24776#01889515-7d05-4850-8a52-1e4bfa2c7e34|CI ERROR>\nI'm trying to find where `num_threads`  is set to 4, and I printed a lot about `OMP_NUM_THREADS`  but still cannot find it.\nSo how can we find where it has been set? Besides, this failed test seems unrelated to my PR.. Any help is appreciated!"}
{"question": "Hi, I've successfully tested the GCS FT for Ray actor, the following are the scripts I used. Furthermore, can Ray support GCS FT and resume training for job when using Ray on k8s? When I submit a job to run and then kill the gcs_serve process, I hope the program can continue training after GCS fault tolerance, but it breaks directly. Does Ray currently support head node failure recovery of jobs?\n*Ray actor test*\n\u2022 kubectl apply -f <https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml>\n\u2022 kubectl port-forward --address 0.0.0.0 service/service-ray-cluster 8265:8265\n\u2022 ray job submit --working-dir my_working_dir --address <http://localhost:8265> -- python test_detached_actor_1.py\n```import ray\n\nray.init(address='<ray://127.0.0.1:10001>', namespace=\"detached_actor_ns\")\n\n@ray.remote\nclass TestCounter:\n    def __init__(self):\n        self.value = 0\n\n    def increment(self):\n        self.value += 1\n        return self.value\n\ntc = TestCounter.options(name=\"testCounter\", lifetime=\"detached\", max_restarts=-1).remote()\nval1 = ray.get(tc.increment.remote())\nval2 = ray.get(tc.increment.remote())\nprint(f\"val1: {val1}, val2: {val2}\")\n\nassert(val1 == 1)\nassert(val2 == 2)```\n\u2022 kubectl exec -it ${HEAD_POD} -- pkill gcs_server\n\u2022 ray job submit --working-dir my_working_dir --address <http://127.0.0.1:8265> -- python test_detached_actor_2.py\n```import ray\n\nprint(\"Try to connect to Ray cluster.\")\nray.init(address='<ray://127.0.0.1:10001>', namespace=\"detached_actor_ns\")\n\n# Get TestCounter actor\nprint(\"Get TestCounter actor.\")\ntc = ray.get_actor(\"testCounter\")\n\nprint(\"Try to call remote function \\'increment\\'.\")\nval = ray.get(tc.increment.remote())\nprint(f\"val: {val}\")```\n*The code I want to be able to continue training after GCS FT*\n\u2022 ray job submit  --address <http://127.0.0.1:8265> -- python /home/ray/test/ray_minist.py\n\u2022 kubectl exec -it ${HEAD_POD} -- pkill gcs_server\n\u2022 The job will break directly:\n```Traceback (most recent call last):\n  File \"/home/ykp/miniconda3/envs/test_k8s/bin/ray\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/ray/scripts/scripts.py\", line 2448, in main\n    return cli()\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/click/core.py\", line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/click/core.py\", line 1055, in main\n    rv = self.invoke(ctx)\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/click/core.py\", line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/click/core.py\", line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py\", line 856, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/ray/dashboard/modules/job/cli.py\", line 243, in submit\n    get_or_create_event_loop().run_until_complete(_tail_logs(client, job_id))\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/asyncio/base_events.py\", line 587, in run_until_complete\n    return future.result()\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/ray/dashboard/modules/job/cli.py\", line 65, in _tail_logs\n    _log_job_status(client, job_id)\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/ray/dashboard/modules/job/cli.py\", line 45, in _log_job_status\n    info = client.get_job_info(job_id)\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/ray/dashboard/modules/job/sdk.py\", line 331, in get_job_info\n    self._raise_error(r)\n  File \"/home/ykp/miniconda3/envs/test_k8s/lib/python3.7/site-packages/ray/dashboard/modules/dashboard_sdk.py\", line 264, in _raise_error\n    f\"Request failed with status code {r.status_code}: {r.text}.\"\nRuntimeError: Request failed with status code 500: {\"result\": false, \"msg\": \"Traceback (most recent call last):\\n  File \\\"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/optional_utils.py\\\", line 95, in _handler_route\\n    return await handler(bind_info.instance, req)\\n  File \\\"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/job_head.py\\\", line 386, in get_job_info\\n    job_or_submission_id,\\n  File \\\"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/utils.py\\\", line 209, in find_job_by_ids\\n    driver_jobs, submission_job_drivers = await get_driver_jobs(gcs_aio_client)\\n  File \\\"/home/ray/anaconda3/lib/python3.7/site-packages/ray/dashboard/modules/job/utils.py\\\", line 154, in get_driver_jobs\\n    reply = await gcs_aio_client.get_all_job_info(timeout=timeout)\\n  File \\\"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/gcs_utils.py\\\", line 167, in wrapper\\n    return await f(self, *args, **kwargs)\\n  File \\\"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/gcs_utils.py\\\", line 403, in get_all_job_info\\n    reply = await self._job_info_stub.GetAllJobInfo(req, timeout=timeout)\\n  File \\\"/home/ray/anaconda3/lib/python3.7/site-packages/grpc/aio/_call.py\\\", line 291, in __await__\\n    self._cython_call._status)\\ngrpc.aio._call.AioRpcError: &lt;AioRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.UNAVAILABLE\\n\\tdetails = \\\"failed to connect to all addresses; last error: UNKNOWN: ipv4:10.244.1.161:6380: Failed to connect to remote host: Connection refused\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:10.244.1.161:6380: Failed to connect to remote host: Connection refused {created_time:\\\"2023-06-25T10:11:52.684794628-07:00\\\", grpc_status:14}\\\"\\n&gt;\\n\", \"data\": {}}.```\n\u2022 code\n```import argparse\nfrom typing import Dict\nfrom ray.air import session\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\nimport ray.train as train\nfrom ray.train.torch import TorchTrainer\nfrom ray.air.config import ScalingConfig\nfrom ray.air import RunConfig, FailureConfig\nfrom ray.air.checkpoint import Checkpoint\nfrom datetime import datetime\nimport ray\nfrom ray.tune import Experiment\n\n\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"/home/ray/test\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"/home/ray/test\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\ndef train_epoch(epoch, dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset) // session.get_world_size()\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"epoch: {epoch} loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef validate_epoch(dataloader, model, loss_fn):\n    size = len(dataloader.dataset) // session.get_world_size()\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(\n        f\"Test Error: \\n \"\n        f\"Accuracy: {(100 * correct):&gt;0.1f}%, \"\n        f\"Avg loss: {test_loss:&gt;8f} \\n\"\n    )\n    return test_loss\n\ndef train_func(config: Dict):\n    batch_size = config[\"batch_size\"]\n    lr = config[\"lr\"]\n    epochs = config[\"epochs\"]\n\n    worker_batch_size = batch_size // session.get_world_size()\n\n    # Create data loaders.\n    train_dataloader = DataLoader(training_data, batch_size=worker_batch_size)\n    test_dataloader = DataLoader(test_data, batch_size=worker_batch_size)\n\n    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n    test_dataloader = train.torch.prepare_data_loader(test_dataloader)\n\n    # Create model.\n    model = NeuralNetwork()\n    # Note that `prepare_model` needs to be called before setting optimizer.\n    if not session.get_checkpoint():  # fresh start\n        model = train.torch.prepare_model(model)\n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n\n    starting_epoch = 0\n    if session.get_checkpoint():\n        checkpoint_dict = session.get_checkpoint().to_dict()\n        print(\"pass here:\", checkpoint_dict.keys())\n        model_state = checkpoint_dict[\"model\"]\n        model.load_state_dict(model_state)\n        model = train.torch.prepare_model(model)\n        # Load in optimizer\n        optimizer_state = checkpoint_dict[\"optimizer_state_dict\"]\n        optimizer.load_state_dict(optimizer_state)\n\n        # The current epoch increments the loaded epoch by 1\n        checkpoint_epoch = checkpoint_dict[\"epoch\"]\n        starting_epoch = checkpoint_epoch + 1\n\n    for epoch in range(starting_epoch, epochs):\n        train_epoch(epoch, train_dataloader, model, loss_fn, optimizer)\n        loss = validate_epoch(test_dataloader, model, loss_fn)\n        checkpoint = Checkpoint.from_dict(\n            {\n                \"epoch\": epoch,\n                \"model\": model.state_dict(),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n            }\n        )\n        session.report(dict(epoch=epoch, loss=loss), checkpoint=checkpoint)\n\n\ndef train_fashion_mnist(num_workers=2, use_gpu=False):\n    # ckt_pth = \"/home/ykp/ray_results/TorchTrainer_2023-03-27_14-56-26/TorchTrainer_7e195_00000_0_2023-03-27_14-56-26/checkpoint_000002\"\n    # checkpoint = Checkpoint.from_directory(ckt_pth)\n    trainer = TorchTrainer(\n        train_loop_per_worker=train_func,\n        train_loop_config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 20},\n        scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu, placement_strategy=\"SPREAD\"),\n        run_config=RunConfig(failure_config=FailureConfig(max_failures=-1)),\n        # resume_from_checkpoint=checkpoint,\n    )\n    result = trainer.fit()\n    print(f\"Last result: {result.metrics}\")\n\n\nif __name__ == \"__main__\":\n    ray.init(address=\"<ray://127.0.0.1:10001>\")\n    train_fashion_mnist(num_workers=2)```"}
{"question": "Any recommendations on LLMs with high token limits and not absurdly expensive?"}
{"question": "I have a ray cluster running in our on-prem k8s cluster. To connect to the head node from outside the cluster, should It suffice to just port forward to the head node:client port?"}
{"question": "Is it possible to deploy a ray VM cluster with some nodes using docker and others not?\n\nE.g., defining the docker config only for certain node types, and leave it empty for others in the yaml file?"}
{"question": "Hi, I\u2019m seeing some weird error produced by ray tune after it runs sometimes (around 7 mins):\n```ray.tune.error._AbortTrialExecution: Trainable runner reuse requires reset_config() to be implemented and return True.```\nAny idea? I\u2019m not passing anything special to the ray tune call"}
{"question": "So I\u2019m just starting up a ray cluster in AWS using the instructions in various tutorials. My cluster is configured as in <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/example-full.yaml>. When I try to run some basic commands, I see a ton of this in the logs:\n```The node with node id: 6896b6310e8317265194ea3128bd233dc6d4cc32b3a3fe019b204263 and address: 172.31.14.230 and node name: 172.31.14.230 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a \t(1) raylet crashes unexpectedly (OOM, preempted node, etc.) \n\t(2) raylet has lagging heartbeats due to slow network or busy workload.\n(raylet, ip=172.31.14.230) [2023-06-28 17:31:44,629 C 82 82] (raylet) <http://node_manager.cc:1036|node_manager.cc:1036>: [Timeout] Exiting because this node manager has mistakenly been marked as dead by the GCS: GCS failed to check the health of this node for 5 times. This is likely because the machine or raylet has become overloaded.\n(raylet, ip=172.31.14.230) *** StackTrace Information ***\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x5361ba) [0x564ae51841ba] ray::operator&lt;&lt;()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x537ba2) [0x564ae5185ba2] ray::SpdLogMessage::Flush()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x537eb7) [0x564ae5185eb7] ray::RayLog::~RayLog()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x29bb13) [0x564ae4ee9b13] ray::raylet::NodeManager::NodeRemoved()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x40b0a7) [0x564ae50590a7] ray::gcs::NodeInfoAccessor::HandleNotification()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x515166) [0x564ae5163166] EventTracker::RecordExecution()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x4a262e) [0x564ae50f062e] std::_Function_handler&lt;&gt;::_M_invoke()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x4a2b86) [0x564ae50f0b86] boost::asio::detail::completion_handler&lt;&gt;::do_complete()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0xa97cfb) [0x564ae56e5cfb] boost::asio::detail::scheduler::do_run_one()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0xa9a289) [0x564ae56e8289] boost::asio::detail::scheduler::run()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0xa9a742) [0x564ae56e8742] boost::asio::io_context::run()\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x168ff2) [0x564ae4db6ff2] main\n(raylet, ip=172.31.14.230) /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f52d4eff083] __libc_start_main\n(raylet, ip=172.31.14.230) /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x1b0567) [0x564ae4dfe567]\n(raylet, ip=172.31.14.230) \nRaylet is terminated: ip=172.31.14.230, id=6896b6310e8317265194ea3128bd233dc6d4cc32b3a3fe019b204263. Termination is unexpected. Possible reasons include: (1) SIGKILL by the user or system OOM killer, (2) Invalid memory access from Raylet causing SIGSEGV or SIGBUS, (3) Other termination signals. Last 20 lines of the Raylet logs:\n    [2023-06-28 17:31:15,879 I 82 82] (raylet) <http://agent_manager.cc:40|agent_manager.cc:40>: HandleRegisterAgent, ip: 172.31.14.230, port: 54600, id: 424238335\n    [2023-06-28 17:31:16,795 I 82 82] (raylet) <http://accessor.cc:611|accessor.cc:611>: Received notification for node id = 0f7b6082229c596356184b276a9c07b1489fd2e28e44f640859b40fe, IsAlive = 1\n    [2023-06-28 17:31:44,583 I 82 82] (raylet) <http://accessor.cc:611|accessor.cc:611>: Received notification for node id = 6896b6310e8317265194ea3128bd233dc6d4cc32b3a3fe019b204263, IsAlive = 0\n    [2023-06-28 17:31:44,629 C 82 82] (raylet) <http://node_manager.cc:1036|node_manager.cc:1036>: [Timeout] Exiting because this node manager has mistakenly been marked as dead by the GCS: GCS failed to check the health of this node for 5 times. This is likely because the machine or raylet has become overloaded.\n    *** StackTrace Information ***\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x5361ba) [0x564ae51841ba] ray::operator&lt;&lt;()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x537ba2) [0x564ae5185ba2] ray::SpdLogMessage::Flush()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x537eb7) [0x564ae5185eb7] ray::RayLog::~RayLog()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x29bb13) [0x564ae4ee9b13] ray::raylet::NodeManager::NodeRemoved()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x40b0a7) [0x564ae50590a7] ray::gcs::NodeInfoAccessor::HandleNotification()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x515166) [0x564ae5163166] EventTracker::RecordExecution()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x4a262e) [0x564ae50f062e] std::_Function_handler&lt;&gt;::_M_invoke()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x4a2b86) [0x564ae50f0b86] boost::asio::detail::completion_handler&lt;&gt;::do_complete()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0xa97cfb) [0x564ae56e5cfb] boost::asio::detail::scheduler::do_run_one()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0xa9a289) [0x564ae56e8289] boost::asio::detail::scheduler::run()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0xa9a742) [0x564ae56e8742] boost::asio::io_context::run()\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x168ff2) [0x564ae4db6ff2] main\n    /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f52d4eff083] __libc_start_main\n    /home/ray/anaconda3/lib/python3.10/site-packages/ray/core/src/ray/raylet/raylet(+0x1b0567) [0x564ae4dfe567]\n    ```\nbut literally nothing is happening on the cluster - I ran a few of the tutorial commands but this started happening 5-10 minutes later.\n\nThen I tried to use the cluster to do tuning on an xgboost model and got this error:\n```RaySystemError: System error: Ray has not been started yet. You can start Ray with 'ray.init()'.```\nI definitely already ran `ray.init(\"<ray://my.server.ip:10001>\")`, and if I run it again I get the expected error:\n```RuntimeError: Ray Client is already connected. Maybe you called ray.init(\"ray://&lt;address&gt;\") twice by accident?```\nBut `ray status --address my.server.ip:6379` says everything is fine:\n```======== Autoscaler status: 2023-06-28 20:40:58.445155 ========\nNode status\n---------------------------------------------------------------\nHealthy:\n 1 ray.head.default\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/2.0 CPU\n 0B/4.35GiB memory\n 579.27MiB/2.18GiB object_store_memory\n\nDemands:\n (no resource demands)```\nwhereas `ray status` with no arguments does not work:\n```ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.```\nAny thoughts on to what\u2019s happening here? Why do the nodes keep getting restarted?"}
{"question": "I\u2019m struggling to find the correct way to restore a tuner and resume training. Currently, my code follows\n```    config = APPOConfig().{various setup functions}\n    tuner = tune.Tuner(\n        \"APPO\",\n        run_config=air.RunConfig(\n            stop={\"timesteps_total\": total_timesteps},\n            local_dir=output_directory,\n            name=model_name,\n            checkpoint_config=air.CheckpointConfig(\n                checkpoint_at_end=True,\n                checkpoint_frequency=checkpoint_frequency,\n            ),\n        ),\n        param_space=config,\n    )\n    tuner.fit()```\nIf tuner.fit() is interrupted, the directory `output_directory/model_name` has an `experiment_state` and `basic_variant_state` both timestampped with the start time, as well as a directory `APPO_EnvName_timestamp` that contains checkpoint directories. The documentation implies that all I should do is add\n```    tuner.restore(\n        \"output_directory/model_name\",\n        \"APPO\",\n    )```\nbefore to tuner.fit(), but while this does not error, it restarts training from scratch. One example script shows how to restore the policy for deployment, but doesn\u2019t show if/how it\u2019s possible to load the entire tuner object state:\n```# load and restore model\nagent = ppo.PPO(env=env_name)\nagent.restore(checkpoint_path)\nprint(f\"Agent loaded from saved model at {checkpoint_path}\")```\n What is the correct way to resume training from a terminated tuner?"}
{"question": "So I\u2019m still struggling with getting ray running a basic AWS cluster to work after following the tutorials for running Ray locally, which worked fine. I used the example cluster config for a simple AWS cluster, and am trying to run some basic dataset preprocessing + XGBoost training from the tutorials. `ray status --address my.ip:6379`  shows a running cluster, and I can navigate to the dashboard which is up.\n```======== Autoscaler status: 2023-06-30 17:49:38.011926 ========\nNode status\n---------------------------------------------------------------\nHealthy:\n 1 ray.head.default\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/4.0 CPU\n 0B/9.13GiB memory\n 289.64MiB/4.57GiB object_store_memory\n\nDemands:\n (no resource demands)```\nHowever, when I try to run any work it says \u201cRay has not been started yet\u201d\n```import ray\nray.init(\"<ray://my.ip:10001>\")\ndataset = ray.data.read_parquet(\n    \"<s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet>\"\n)\ndataset = dataset.repartition(10)\ntrain_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\nprint(train_dataset.groupby('passenger_count').mean('fare_amount').show())```\noutputs:\n```2023-06-30 17:58:24,061\tWARNING dataset.py:253 -- Important: Ray Data requires schemas for all datasets in Ray 2.5. This means that standalone Python objects are no longer supported. In addition, the default batch format is fixed to NumPy. To revert to legacy behavior temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes.\n\nLearn more here: <https://docs.ray.io/en/master/data/faq.html#migrating-to-strict-mode>\n2023-06-30 17:58:24,192\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -&gt; TaskPoolMapOperator[ReadParquet] -&gt; AllToAllOperator[Repartition]\n2023-06-30 17:58:24,193\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n2023-06-30 17:58:24,193\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n- Repartition: 0 active, 0 queued, 0.0 MiB objects, 0 output: 0%\n0/10 [00:03&lt;?, ?it/s]\n*- Repartition: 0%\n0/10 [00:03&lt;?, ?it/s]\nRunning: 0.0/4.0 CPU, 0.0/0.0 GPU, 147.04 MiB/1.14 GiB object_store_memory: 0%\n0/10 [00:03&lt;?, ?it/s]\n2023-06-30 17:58:27,311\tINFO streaming_executor.py:149 -- Shutting down &lt;StreamingExecutor(Thread-14, stopped daemon 123145760382976)&gt;.```\nfollowed by a deep stack trace, ending in:\n```RaySystemError: System error: Ray has not been started yet. You can start Ray with 'ray.init()'.```\nif I call `ray.init(my.ip:10001)`  again, I get:\n```RuntimeError: Ray Client is already connected. Maybe you called ray.init(\"ray://&lt;address&gt;\") twice by accident?```\nIf I restart the jupyter to try `ray.init(my.ip:10001)` from scratch, it connects to the cluster and shows the nice little \u201cI\u2019m up\u201d picture.\n\nAm I trying to connect to the remote cluster completely wrong? Why can I connect to the cluster with `ray.init` but as soon as I try to use it it fails?"}
{"question": "Why do processes in the head node attempt to connect to the GCS server via the node's IP address instead of e.g. `localhost` or the DNS name, and is there any way of forcing the head node to connect to the GCS via localhost?\n\nI'm asking specifically because this makes it complicated to set up TLS for our Ray cluster. We can configure the cert to provide TLS over `localhost` and the DNS name (k8s service name), which works fine for the workers (which connect over the server name), but it breaks for the head node which tries to access the GCS via the node's IP address (and not `localhost` nor the DNS name). This means we have to dynamically configure the certificate on boot via an init container (see the Ray<https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.tls.yaml| example scripts here> taken from <https://docs.ray.io/en/latest/ray-core/configure.html#tls-authentication|this documentation>), which is much more painful than just being able to configure the cert once to work over dns / localhost.\nAn example of the error we're seeing is this\n```D0703 21:16:23.880569575    2451 <http://security_handshaker.cc:181]|security_handshaker.cc:181]> Security handshake failed: {\"created\":\"@1688418983.880562702\",\"description\":\"Peer name 10.21.6.60 is not in peer certificate\",\"file\":\"src/core/lib/security/security_connector/ssl/ssl_security_connector.cc\",\"file_line\":57}\nI0703 21:16:23.880615501    2451 <http://handshaker.cc:89]|handshaker.cc:89]>           handshake_manager 0x563a7567c530: error={\"created\":\"@1688418983.880562702\",\"description\":\"Peer name 10.21.6.60 is not in peer certificate\",\"file\":\"src/core/lib/security/security_connector/ssl/ssl_security_connector.cc\",\"file_line\":57} shutdown=0 index=2, args={endpoint=(nil), args=(nil) {size=0: }, read_buffer=(nil) (length=0), exit_early=0}```\nWhere `10.21.6.60` is the head node's IP address."}
{"question": "A quick question, does `rllib.models.modelv2` supports `gym.spaces.Dict`  as `obs_space` or it's up to individual algorithm? thanks"}
{"question": "Hi folks, I need some help about running example file.\n\nThis is the problem. I ran \"ray\\tune\\examples\\pb2_examples.py\" like the way I attached.\nHow can I resolve this error? I don't have any sense...\n(I already installed visual studio and GPy library and I run this code through the anaconda powershell prompt with the administrator rights.)"}
{"question": "has anyone tried to use Ray to train a LLM from scratch? wondering how much would it cost"}
{"question": "I was building *`ray-0.6.2`* from source with *Bazel version `6.2.1`* and getting *error when downloading Boost*. Please see bellow the snippet for the log from the build. The full build log can also be found here <https://gist.github.com/iamsiddhantsahu/18cdf4843d9e87df0f9b448a398cf216>\n\n```[ 21%] Built target gen_gcs_fbs\n    -- Using src='<http://dl.bintray.com/boostorg/release/1.68.0/source/boost_1_68_0.tar.gz>'\n    CMake Error at boost_ep-stamp/download-boost_ep.cmake:170 (message):\n      Each download failed!\n\n        error: downloading '<http://dl.bintray.com/boostorg/release/1.68.0/source/boost_1_68_0.tar.gz>' failed\n              status_code: 6\n              status_string: \"Couldn't resolve host name\"\n              log:\n              --- LOG BEGIN ---\n              getaddrinfo(3) failed for <http://dl.bintray.com:80|dl.bintray.com:80>\n\n      Couldn't resolve host '<http://dl.bintray.com|dl.bintray.com>'\n\n      Closing connection 0\n\n\n\n              --- LOG END ---\n              error: downloading '<http://dl.bintray.com/boostorg/release/1.68.0/source/boost_1_68_0.tar.gz>' failed\n              status_code: 6\n              status_string: \"Couldn't resolve host name\"\n              log:\n              --- LOG BEGIN ---\n              getaddrinfo(3) failed for <http://dl.bintray.com:80|dl.bintray.com:80>\n\n      Couldn't resolve host '<http://dl.bintray.com|dl.bintray.com>'\n\n      Closing connection 0\n\n\n\n              --- LOG END ---```\n*Steps to reproduce the error*:\n1. `git clone <https://github.com/ray-project/ray.git>`\n2. `cd ray`\n3. `git checkout ray-0.6.2` \n4. `cd python`\n5. `pip install -e . --verbose` \nQuick googling, I came across this exact same error message from GitHub's Issue (*<https://github.com/ray-project/ray/issues/4705>*).\n\nBut, I am still stuck at this error and can't manage to build `ray-0.6.2` . Does anyone know or have an idea on how to resolve it? Would be very thankful if I am able to build `ray-0.6.2` successfully."}
{"question": "if I run a remote job with `num_cpus=1`, but internally use `subprocess.Popen` to spawn two tasks, what happens? the `num_cpus` is only for logical bin-packing right? so the tasks can attempt to run on multiple cores?"}
{"question": "Hey everyone, I am testing storing and moving models with cluster memory versus simply saving/loading to/from distributed storage (minio/s3). I see that cluster memory is underperforming saving/loading to S3. Let me elaborate:\n\nEssentially I have a set of tasks that each trains a unique model and then `put`s to cluster memory or saves to distributed storage. (Note the `put` is done in the individual task, but it is done using the `_owner` argument which is set to a detached actor that holds all the object refs).\n\nThen I have another set of tasks that load the models with `get` or from distributed storage, and then run an inference on them. (Note that the `get`  first gets the objectref from the detached actor and then calls `get` on the objectref to get the model).\n\nI am using two nodes, Node 1 has GPUs and trains all the models, Node2 is only CPUs. Train must be done with GPU but Inference can be performed on CPUs. This means that inference of a task on Node2 calls `get` on the cluster memory which forces Node1 object store to start replicating the models to its local object store.\n\nSide by side I see that saving/loading to/from S3 is significantly faster than using `put` and `get`. Watching the Ray dashboard, I see that the node can read/write 100Mb/s when using S3, but it is closer to 10 MB/s when using `put` and `get`.\n\nI also read a similar conclusion was made by someone comparing plasma vs S3 here <https://www.telesens.co/2022/04/23/data-transfer-speed-comparison-ray-plasma-store-vs-s3/>\n\nAm I missing something? Is cluster memory object replication causing performance decreases?"}
{"question": "What's the best way to load a checkpoint into Tune.tuner? I'm training a model with policy gradients. I can load my checkpoint with Algorithm.from_checkpoint() but I'm not sure how to make it usable for the tuner"}
{"question": "From where can I download the Wheel file `.whl` file for Ray version `0.6.2` or version `0.6.6` ?"}
{"question": "We are currently doing a flash sale for the Ray Summit, if you haven't signed up yet, now is the time -- <https://www.myeventi.events/raysummit23/attendee/?code=SUMMERSAVINGS&amp;utm_campaign=RaySummit2023&amp;utm_source=slack_promo> (the code \"SUMMERSAVINGS\" for the flash sale is part of the link, make sure to click \"APPLY PROMO CODE\" at the bottom) :ray: :mountain:"}
{"question": "*Hi everyone. I am trying to deploy an agent. Has anybody write an operator to control the agent in Kubernetes? Is the Kopf suitable for this purpose?*"}
{"question": "This is the last chance to sign up for the Ray Summit :ray: :mountain: with the \"SUMMERSAVINGS\" discount code, it will only be available till today midnight PST: <https://www.myeventi.events/raysummit23/attendee/?code=SUMMERSAVINGS&amp;utm_campaign=RaySummit2023&amp;utm_source=slack_promo> -- I'm looking forward to seeing you all at the summit :tada:"}
{"question": "Can someone walk me through the sequence of events following connecting to a remote ray-head with ray.init(&lt;remote_host&gt;:10001&gt;)?\nMy ray-head is behind a reverse proxy and I can\u2019t seem to connect with Ray.\nTCP connections on the ports mentioned above all work.\nI\u2019m assuming there\u2019s something else at play - either a port I missed, or something else.\nBut the issue is definitely the reverse proxy."}
{"question": "Hey folks - Had a question out of curiosity , has anyone had a ray cluster setup with head and other worker nodes being in different geographical regions? Does it affect ray performance due to network data transfer or networking overhead? Say even if we could essentially have nodes on a single VPN - how would that affect performance of say a fsdp training for a DNN. Would love to talk to anyone who's tried such a setup or has a clue about this !"}
{"question": "Starting to use the job CLI. Is it compatible with Python scripts installed using  [project.scripts] (used on pyproject.toml) ?"}
{"question": "Starting to use the job CLI #2. I usually use nodes with multiple NIC's and due to this fact  I start the cluster using --node-ip-address, for both head and workers nodes and for the workers the --address that they use is the --node-ip-address of the head node. The cluster startup nicely, but when trying to use the ray job CLI I am stuck in a problem  (the script just use ray.init())  If I do not pass the --address parameter of ray job submit its complain about multiple raylets If I pass the address it fails to connect\n\nAny ideas ?"}
{"question": "Hey guys, I found out that xgboost_ray deosn\u2019t really support multi-output models, while xgboost does, any suggestions?\nSee details here: <https://discuss.ray.io/t/does-xgboost-ray-supports-multi-output-many-y-labels/11383/2?u=y_c>"}
{"question": "When is the next planned Ray release?\n\n`grpcio` contains a few new high-severity vulnerabilities and we need to upgrade for compliance, but <https://github.com/ray-project/ray/commit/9aa133ed9c7d8c0072698c887ba5ff163729bb73> is not released yet."}
{"question": "I am running in to some issues trying to use ray to distribution RL training in the gymnasium mingrid/babyai environment and looking to discuss that with more knowledgable people. Where is the best place to do this? TypeError: unhashable type:\u2018EnvContext\u2019 is the issue the I cannot seem to get past. I have code that I can share if needed as well. Thank you!"}
{"question": "hey, in our research group people are using Ray, but everyone on their own cluster (which is usually a few nodes). Given that very soon we'll get more GPUs, we'd like to have a setup of one big cluster shared by everyone, to improve the utilization.\n\nWhat are the best practices / recommendations / docs about it?  I saw some previous forum threads mentioning that Ray isn't really intended for such use-case, but I'm not sure what does it mean in practice...\n\nFor example, if people A &amp; B share the same cluster with 100 GPUs, and person A sends 400 tasks (1 GPU each),  then B submits 50-task job (1 GPU each), does it mean that person A has priority and all 400 tasks need to execute before B's task?\n\nThanks a lot for pointers!"}
{"question": "and does anyone know how companies like OpenAI (who are using Ray) handle this? :eyes:  is this sth like Vulacon that <@U05831BNH4Y> mentioned?"}
{"question": "Hi, what's the best way to use map or map_batches remotely? I'm trying to give map a remote actor as a parameter, but i'm getting an error saying that isn't callable"}
{"question": "Hey y'all, kinda new to Ray and to distributed ML in general. Anyone have experience with loading files from a GitHub repo to all workers in a cluster and then pushing the result to the same repo?"}
{"question": "Hello all,\n\nWe are investigating Ray for distributed training. We will likely be leveraging kuberay in the future, but need a stop gap solution to getting Ray up and running without the need for Kubernetes. \n\nWondering if it would be feasible to have Ray on top of AWS Batch multi node parallel jobs?"}
{"question": "Hello, I recently downloaded the ray-ml:2.5.1-py310-cpu image and it came out to be ~13GB. is it right that this image is supposed to be this large?"}
{"question": "Hi all, Ray 2.6.0 just came out. What is new with this version? Are there changes we need to note?"}
{"question": "Hi,\nI'm new to ray. been trying to figure out how to gracefully free-up resources taken by an Actor after it finishes a particular job. When using exist_actor() the job shows up as having failed which makes it hard to distinguish from real failures.\nThis seems to have been mentioned here: <https://github.com/ray-project/ray/issues/2641> , my current solution is to just let the actor go out of scope .. but that feels suboptimal. Is this by design ? or is there a more graceful &amp; explicit way to tell ray to free-up resources from a live actor or to kill an actor without causing the dashboard to show failures ?\nThanks"}
{"question": "Hi everyone,\n\nI\u2019m attempting to run the example code for Ray on Spark, found here: <https://docs.ray.io/en/latest/cluster/vms/user-guides/community/spark.html#running-a-basic-example>\n\nI\u2019m using Ray 2.5.1 and PySpark 3.3.2.\n\nWhen I run the example, I\u2019m seeing the error below.  Is this an issue with my environment?  I\u2019m trying to understand how we can use Ray and Spark together, so any help with this would be much appreciated.\n\nThanks, Jason\n\n```23/07/13 16:43:18 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\nTraceback (most recent call last):\n  File \"ra_spark_support/test.py\", line 16, in &lt;module&gt;\n    setup_ray_cluster(num_worker_nodes=MAX_NUM_WORKER_NODES)\n  File \"/home/ubuntu/dev/spark-support/venv/lib/python3.8/site-packages/ray/util/spark/cluster_init.py\", line 987, in setup_ray_cluster\n    ) = get_avail_mem_per_ray_worker_node(\n  File \"/home/ubuntu/dev/spark-support/venv/lib/python3.8/site-packages/ray/util/spark/utils.py\", line 344, in get_avail_mem_per_ray_worker_node\n    spark.sparkContext.parallelize([1], 1).map(mapper).collect()[0]\n  File \"/home/ubuntu/dev/spark-support/venv/lib/python3.8/site-packages/pyspark/rdd.py\", line 1197, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/home/ubuntu/dev/spark-support/venv/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/home/ubuntu/dev/spark-support/venv/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n    return f(*a, **kw)\n  File \"/home/ubuntu/dev/spark-support/venv/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (ip-172-31-31-1.ec2.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 668, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'ray.util.spark'```"}
{"question": "Hello everyone!!!\n\nI have being using the Ray API for conducting RL experiments on different RL benchmarks like the <https://github.com/soltoggio/CT-graph.git|CT-graph benchmark> Recently I have been tasks to work with Reinforcement Learning in Robotic scenarios. For these settings I have found out the <https://github.com/vikashplus/robohive.git|RoboHIve API> which has a plethora of different robotic tasks. All the RoboHive environments are OpenAI Gym environments. The API uses OpenAI Gym version 0.13.0 for making the environments. I wanted to use Ray Tune and RLlib for running experiments with their pick and place environment. I have made a notebook where I register the custom RoboHive environment to the Ray API using the _env_register_ function. However when running the method _tune.run_ with either PPO, SAC or A2C I receive the following error message:\n\n\n`2023-07-15 20:01:46,845 ERROR trial_runner.py:773 \u2013 Trial`\n`A2C_RoboHive_Pick_Place_0_095a9_00000: Error processing event.`\nTraceback (most recent call last):\nFile \u201c/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/tune/trial_runner.py\u201d, line 739, in _process_trial\nresults = self.trial_executor.fetch_result(trial)\nFile \u201c/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/tune/ray_trial_executor.py\u201d, line 746, in fetch_result\nresult = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\nFile \u201c/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\u201d, line 82, in wrapper\nreturn func(*args, **kwargs)\nFile \u201c/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/worker.py\u201d, line 1623, in get\nraise value\nray.exceptions.RayActorError: The actor died because of an error raised in its creation task, ray::A2C.`*init*() (pid=14782, ip=172.19.132.220)`\nFile \u201c/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py\u201d, line 136, in *`init`*\n`Trainer.*init*(self, config, env, logger_creator)`\nFile \u201c/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\u201d, line 592, in *`init`*\n`super().*init*(config, logger_creator)`\nFile \u201c/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/tune/trainable.py\u201d, line 103, in *`init`*\n`self.setup(copy.deepcopy(self.config))`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py\", line 146, in setup`\n`super().setup(config)`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 739, in setup`\n`self._init(self.config, self.env_creator)`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/agents/trainer_template.py\", line 170, in _init`\n`self.workers = self._make_workers(`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 821, in _make_workers`\n`return WorkerSet(`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 103, in *init*`\n`self._local_worker = self._make_worker(`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\", line 399, in _make_worker`\n`worker = cls(`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 580, in *init*`\n`self._build_policy_map(`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1375, in _build_policy_map_`\n_self.policy_map.create_policy(name, orig_cls, obs_space, act_space,_\n_File \u201c/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\u201d, line 136, in create_policy_\n_self[policy_id] = class_`(observation_space, action_space,`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/policy/policy_template.py\", line 279, in *init*`\n`self._initialize_loss_from_dummy_batch(`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 746, in _initialize_loss_from_dummy_batch`\n`self._dummy_batch = self._get_dummy_batch_from_view_requirements(`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 875, in _get_dummy_batch_from_view_requirements`\n`ret[view_col] = np.zeros_like([`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 876, in`\n`view_req.space.sample() for _ in range(batch_size)`\n`File \"/home/cocp5/anaconda3/envs/rhRL3906c/lib/python3.9/site-packages/gym/spaces/box.py\", line 42, in sample`\n`return <http://self.np|self.np>_random.uniform(low=self.low, high=high, size=self.shape).astype(self.dtype)`\n`File \"mtrand.pyx\", line 1155, in numpy.random.mtrand.RandomState.uniform`\n`OverflowError: Range exceeds valid bounds`\n\n\n\nThe _*action space*_ of the environment is of type _*Box(9,) [array dtype=float32]*_ and the _*observation space*_ of type *_Box(63,) [numpy.ndarray of numpy.float64]_.*\nFor compatibility with the RoboHive API dependencies I am using Ray version 1.6.0. The numpy version in my system is 1.25.1. I am using OpenAI GYm version 0.13.0 since the RoboHive API specifically is depended to only this version of the Gym API and since there are not any version constraints in the _<https://github.com/ray-project/ray/blob/releases/1.6.0/python/requirements.txt|requirements.txt>_  I am running in a conda environment in Ubuntu 20.04.06 LTS through WSL2 with GPU acceleration setup on Windows11.\n\n\nWhat is the cause of the issue and How could I resolve it?\n\n_Additional question:_ I am posting this question in the correct Slack channel, or should I post it in a different one?\n\nThank you very much in advance for your valuable help and support!!!\nI am always at your disposal for any further queries or clarifications regarding my code and my setup.\nKind regards,\nChristos Peridis"}
{"question": "Hi guys\nI'm running jobs on remote ray AWS cluster w/ *multiple* local object spilling directories, e.g. `$HOME/ray_spill`  and `$HOME/ray_spill2`\nI noticed that, when `ray_spill` is almost full, but `ray_spill2` has plenty disk space, I still get following warnings/errors:\n\n `(raylet, ip=XXX) [2023-07-17 18:05:43,023 E 34954 34969] (raylet) <http://file_system_monitor.cc:111|file_system_monitor.cc:111>: /home/ubuntu/ray_spill is over 99% full, available space: 2662400; capacity: 299853516800. Object creation will fail if spilling is required.`\n\nIs this expected behavior? Or did I mis configure it?\nI see there are objects spilled to `ray_spill2` so it's definitely using multiple directories.\n\nthanks"}
{"question": "Is there a plan to support GPU memory as a resource that\u2019s specifiable rather than just quantity/fractional quantity of a GPU? It\u2019s much easier to specify the memory requirements for a GPU and have the workload be transferrable across different GPU instances.  I understand that I can create a new resource in our own env but this seems like something that would be useful for all GPU workloads in general."}
{"question": "how do i get the id of the current worker?"}
{"question": "Hi guys, I just started evaluating Ray Serve and discovered great HTTP APIs to manage Serve deployments <https://docs.ray.io/en/latest/serve/production-guide/deploy-vm.html>. I have several questions about these APIs.\n1. Are they documented somewhere? The documentation describes `serve` commands but not the actual HTTP APIs. I would like to use the APIs programmatically\n2. Is there any specific reason why these APIs are implemented not on the dashboard port (like jobs APIs), but rather on a different one? The issue for me is that if I want to expose both I now need 2 different ingresses and 2 different endpoints - one for dashboard and jobs and one for serve, which is not the most convenient thing.\nThanks for your help\nBoris"}
{"question": "hi guys, I submit a <https://github.com/ray-project/ray/pull/37467|pull request > that can bring a great programming experience when working with actors. Can you provide some suggestions, please?"}
{"question": "along the same lines of this question I raised...\n<https://ray-distributed.slack.com/archives/C01DLHZHRBJ/p1689617348624289>\n, I tried spilling to remote s3 storage but getting the well known boto3 EntityTooLarge error.\nI've played around w/ `buffer_size` from 10MB to 100MB (recommended from doc), but still running into the same error.\nI understand remote spilling is experimental, but is there anywhere I can pass in boto3 args to avoid the EntityTooLarge issue ?"}
{"question": "Hello there, I have a question regarding invalid action masking. In the examples, I can only find implementations for invalid action masking in the forward pass. However, a paper I've read determined that invalid action masking is a state-dependent differentiable function and can be used effectively during back-propagation as well. Are there any examples of using invalid action masks in this way? I am assuming I would have to overwrite the custom_loss method for the ModelV2 class?\n\nHere's a link to the paper in question: <https://arxiv.org/pdf/2006.14171.pdf>\n\nHere's a link to the example where it is used in the forward pass only: <https://github.com/ray-project/ray/blob/master/rllib/examples/models/parametric_actions_model.py>"}
{"question": "Hi, I'm trying to launch a ray 2.5.1 cluster, it's failing on provisioning the head node with\n```   Running `rsync -e 'docker exec -i' -avz /tmp/ray_tmp_mount/default/~/ray_bootstrap_config.yaml ray_container:/root/ray_bootstrap_config.yaml`\n...\nprotocol version mismatch -- is your shell clean?```\nI built the docker image on top of the ray cpu 2.5.1 image, and I'm running on Ubuntu 22, anyone run into anything similar?"}
{"question": "Hi folks,\na few weeks ago, I tried to read json files from my HDFS database using the `ray.data.read_json()` API. But it was not successful. I checked the source code and it seems to me that it is not supported.\nHas anyone tried it as well? Do you know whether it is supported? Thanks a lot!"}
{"question": "Running Ray in docker on AWS, everything seems to work fine, but suddenly even the simplest functions get:\n```TypeError: Could not serialize the put value &lt;function ff at 0x7fc130488e50&gt;```\nWith e.g.\n```@ray.remote\ndef ff(x):\n    return x*x\n\nfutures = [ff.remote(i) for i in range(4)]```\nAny idea? :thinking_face:\n\nI\u2019ve gone through the troubleshooting and of course there\u2019s nothing that should prevent serialization -\n```from ray.util import inspect_serializability\ninspect_serializability(ff, name=\"ff\")  # (True, set())```\nIt seems to happen when I specify `runtime_env=dict(pip=[\u2026])` in `ray.init`, but that can\u2019t be the case?\nEDIT: Okay, so it\u2019s specifically to do when requesting an old `typing_extensions` in `ray.init`\u2026"}
{"question": "Is it a know issue that ascii control characters (e.g. those emitted by tqdm) aren\u2019t applied correctly in the log in the jobs dashboard?"}
{"question": "only get the worker logs after they complete, not in progress, should I follow <https://stackoverflow.com/questions/55272066/how-can-i-use-the-python-logging-in-ray> to get them in progress?"}
{"question": "<https://gist.github.com/mooreniemi/ed5c468d41505ea8bc454648fe3d9b84> adapted to also get result out, feels like there must be a better way to get progress/logs out of workers in ray tho? i must be missing right docs"}
{"question": "Hello\n[Reg building <https://docs.ray.io/en/latest/ray-contribute/development.html#building-ray|Ray locally>] if I make a change to _raylet.pyx and other py files. How should I rebuild to make the change visible to pyx file?\nAll I run is basic-test _python -m pytest -v -s python/ray/tests/test_basic.py_\n\nThanks"}
{"question": "I\u2019ve seen these error\u2019s a lot lately.\n```Error publishing node physical stats.\nTraceback (most recent call last):\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/dashboard/modules/reporter/reporter_agent.py\", line 1089, in _perform_iteration\n    formatted_status_string = await self._gcs_aio_client.internal_kv_get(\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/gcs_utils.py\", line 167, in wrapper\n    return await f(self, *args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/ray/_private/gcs_utils.py\", line 280, in internal_kv_get\n    reply = await self._kv_stub.InternalKVGet(req, timeout=timeout)\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/grpc/aio/_call.py\", line 290, in __await__\n    raise _create_rpc_error(self._cython_call._initial_metadata,\ngrpc.aio._call.AioRpcError: &lt;AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.DEADLINE_EXCEEDED\n\tdetails = \"Deadline Exceeded\"\n\tdebug_error_string = \"UNKNOWN:Deadline Exceeded {grpc_status:4, created_time:\"2023-07-20T17:10:02.371379717-07:00\"}\"\n&gt;```\nany idea what causes this? What deadline am I exceeding in the calls to GCS?"}
{"question": "Transitioning from the Client SDK to the job submission SDK, what\u2019s a good way to pass along arguments?\nShould everything be provided explicitly in the CLI? :thinking_face: That seems a bit excessive when there are a lot of arguments."}
{"question": "Is there a way to control the session name generated by ray?"}
{"question": "Hi!\n\nIf I know well raylib uses preprocessing for observation.\nMy observation is the following: np.array((8,4,5)). I have some cases, when in the observation sometimes I want to some data to be ignored during training and in these cases I uses ouitlier data value such as -100. Mostly my data is between [-3,3] or [0,36.1] etc, etc. So far from that value. Is it possible to normalize only several columns in the array and which has the value of -100, leave it like that? Thank you."}
{"question": "What's the state of push-based shuffle? I gave it a try on 2.6.1 and I'm seeing an exception :thread:"}
{"question": "Hi! At Golem Factory we are building a ray cluster manager so that ray apps can also be effortlessly run on the decentralised cloud which we create.\n\nWe are thinking about calling it `Ray on Golem` - could you please point me to ray name and logo usage guidelines? I can't find them easily :("}
{"question": "Hi folks, I\u2019m running with ray serve on kubernetes, my problem is my network is not quite fast, then when preparing for the runtime, like installing the `on_pytorch_model.bin`, it often times out, then the pod will restart, it likes a vicious circle, any one knows any approach to solve this? I tried to increase the value of `health_checkout_timeout_s`, it doesn\u2019t take effect. Thanks a lot."}
{"question": "Hi Folks! If using VM based ray cluster, how can  I export more prometheus endpoints listening at a different port on every machine ?"}
{"question": "Hi, \nwe are setting up our cluster in a way that we are forwarding head node ports with ssh tunels so that they are visible on a machine outside the cluster.\n\nWhen the worker starts, it connects to the head (via the ssh tunnel) (and is visible in `ray status`) but after 30-60 seconds disappears with logs saying about GCS health check failing.\n\nWhat could be happening?\nHow to debug it?\n\nDo we understand correctly that GCS health checks try connecting to the same head node port (6379 by default) as new workers (so if worker can connect to the head then the health check also should be able to go through?)?"}
{"question": "Hi all, we are using ray to distribute and schedule  machine learning pipelines for stock predictions.  We are facing a recurrent issue when we run one of our tests:\n\n1. Node with 30 CPUs\n2. An actor task runs in an endless loop that schedules other tasks every X minutes (let's call this SchedulerTask)\n3. After some time SchedulerTask disappears from ray ( cant be queried by the API, CLI and does not appear on the dashboard but stills runs) so we can't decide if kill or re-start the task\n4. When the task disappears some of the tasks that are scheduled constantly present OOM errors\nIs there a limit of how long tasks are keep track by ray? How can we track if a task is still healthy if is a very long run ?"}
{"question": "Is there a maximum number of pending tasks that Ray allows?"}
{"question": "Every once in a while I see my logs get flooded by `Mismatched ActorID`, what's that about? :thread:"}
{"question": "The task and actor of ray can retry if the ray node crashed through some parameters. Does ray job have this ability?"}
{"question": "hi! i'm looking for a way to spawn a ray cluster in a jupyter notebook, then attach to it and run functions cell by cell, effectively sharing a single file with somebody familiar with jupyter who then doesn't need to learn how to deploy things on AWS. is that possible with Ray?"}
{"question": "Hi people, There is a problem with ray tune. When I worked earlier with optuna for tuning it used a set of hyperparameters to see the performance of the model and based on the performance and set of hyperparameters it used an optimization to see how it can improve performance. now ray uses optuna as built in method but instead of running each trail sequentially it run all of them parallel so it can not benefits from past experiment to change the hyperparameters to improve the future trials. what should I do?"}
{"question": "When importing ray in python (the top-level package `import ray`) it by default reconfigures logging in a way that disables any currently active loggers. Is this intentional? It can be removed by setting disable_existing_loggers to False in the dict argument to dictConfig in `ray/_private/log.py`"}
{"question": "Hello Ray Team! Does Ray publish user metrics like kubeflow does here? <https://blog.kubeflow.org/kubeflow-user-survey-2023/>"}
{"question": "I\u2019m setting up grafana integration, and I\u2019m seeing this on the Ray Dashboard for different panels:\n```Panel with id 18 not found```\nDoes Ray create different panels in grafana or do we need to do that manually?"}
{"question": "why RAY_RUNTIME_ENV_TEMPORARY_REFERENCE_EXPIRATION_S is by default set to only 10 minutes? most of autoscaled infra will not be able to scale out in that short of time"}
{"question": "Hi, with application-level metrics, is there a way to NOT add default labels such as \"Component\", \"SessionName\", \"WorkerId\" ?\nI creates separate series which is not wanted and querying difficult."}
{"question": "hey folks, I have a general UI question around intepreting CPU usage. I'm running a node that has 32 CPUs, and I've assigned 15 cpus per task so there are currently two tasks running. One task is at 103% CPU while the other is at 13.8%. The overall CPU shows 4%. How do interpret these CPU loads? I had assumed that each bar is just the percentage of total CPU load on the node, but that seems to not be the case here?"}
{"question": "Hi! When attempting to parallelize `xgboost_ray.train` across tasks for cross-validation, it always results in the connection error shown in the thread. If the same task is run sequentially, it works. Anyone have any suggestions what to try to get parallelization of `xgboost_ray.train` to work?"}
{"question": "hi folks, recently updated to ray 2.6.1 from ray 2.5 and it just crashes me out of python on `ray.init`  .  Things are fine with 2.5.  Error is:\n\n```\n2023-08-01 00:25:56,846 INFO worker.py:1621 -- Started a local Ray instance.\n[2023-08-01 00:25:58,510 E 19932 19932] <http://core_worker.cc:201|core_worker.cc:201>: Failed to register worker 01000000ffffffffffffffffffffffffffffffffffffffffffffffff to Raylet. IOError: [RayletClient] Unable to register worker with raylet. No such file or directory```\nI'm see this on the ubuntu-based container, `pangeo/pytorch-notebook`  , which is on python 3.10.12 from conda-forge.  Any ideas?"}
{"question": "Hey guys, can you cancel a ray job with the UI?"}
{"question": "How does the autoscaler decide that a node can be removed?"}
{"question": "Hi guys!\n\nIs it possible to train with different episode length with rllib PPO? I tried do ran a training, when I have a maximum step size/epsiode, but I also have an exit condition during the training, which make make the episode shorter. In this case I always get an error after several episodes, but if I ignore the exit condition, and use the same length of episode it goes through. Thank you in advance."}
{"question": "Semi-random question: we're trying to upgrade the pydantic in our monorepo to 2.x and it looks like Ray won't play along with that.  Any idea if/when Ray will update to recent pydantic?  Alternately, anybody know how hard it is to use pedantic v2 in v1 mode?  <https://docs.pydantic.dev/latest/migration/#continue-using-pydantic-v1-features>. I've not yet looked at the Ray code to see how big of a deal this might or might not be."}
{"question": "what's the expected latency / overhead that ray adds to await a handle?\n\ni have some code that's like\n\n```#start\nresult_ref = await self.handle.function.remote(blah)\nresult = await result_ref\n#end```\nthe remote call only takes like ~30ms to execute\nbut awaiting in total from start to end varies a lot and can take up to 1second to resolve\n\nthanks!"}
{"question": "The Python 3.7 MacOS x86 wheel seems to be missing for Ray 2.6.1? Is target now dropped?\n<https://pypi.org/project/ray/2.6.1/#files>"}
{"question": "Hi all,  for projects that have multiple groups making use of ray clusters, e.g. innovation teams, development, production, do you typically see companies maintaining separate clusters for each or do you see maintaining a single cluster that serves all? Or do you see both? What is suggested?"}
{"question": "Hi All, I am stuck with some issues with ray serving on ray cluster, it would be great if I could get any lead on the below issue:\n\nI have 2 Virtual Machines on AWS, each having 4 GPUs, I created a cluster using the following commands:\nOn Instance1: ray start --head --dashboard-host=\"0.0.0.0\"\nOn Instance2: ray start --address='&lt;ip_address_of_head&gt;'\n\nThe issue is I have multiple LLM models, so once the cluster is created, I have 8 GPUs in total and I want to deploy one model over 5 GPUs and another model on 3 GPUs, is there a way to do it?\n\nI tried setting num_gpus in ray actor in @serve.deployment but I got the following error while serving:\n*no available node types can fulfill resource request {\u2018gpu\u2019: 5.0},* even though I can see resources available: {\u201cgpu\u201d: 8.0}\n\nPlease let me know if I am missing something?"}
{"question": "i've attached the code reproduction here. it's super simple there are only two files. (server + client): <https://github.com/bryanhpchiang/serve-example>\n\ntrying to wrap my head around the latency that ray injects, i feel like i must be doing something extremely basic wrong.\n\nfrom the readme:\n## Background:\n- `Ingress` has a handle to `Inference`\n- `Ingress` has 1 worker, `Inference` one has 10 workers\n- `client.py` dispatches 10 concurrent requests\n- 1 machine, 16 CPU cores available\n\n## Reproducing\n```python server.py # in one window\npython client.py # in another```\n## Questions:\n- Why are all not all the print calls from insider `Ingress` (denoted with `[WORKER]`) printed?\n- There are a few inference calls that only take ~25ms.\n    - Why do all the ingress timings at least 60ms? Is that extra 30ms of time being added by Ray?\n    - Are multiple requests being routed to the same `Inference` replica?\n    - If so, what's the point of defining `num_replicas` if Ray won't uniformly distribute the load?\n\n\nServer logs:\n```(HTTPProxyActor pid=268697) INFO:     ('127.0.0.1', 45552) - \"WebSocket /\" [accepted]\n(HTTPProxyActor pid=268697) INFO:     connection open\n(ServeReplica:default_Inference pid=268749) INFO 2023-08-02 18:07:03,519 default_Inference default_Inference#mtylLG oQzRSNNGan / default replica.py:723 - __CALL__ OK 62.0ms\n(ServeReplica:default_Inference pid=268749) INFO 2023-08-02 18:07:03,545 default_Inference default_Inference#mtylLG oQzRSNNGan / default replica.py:723 - __CALL__ OK 24.7ms\n(ServeReplica:default_Inference pid=268749) [WORKER] Took 0.024555683135986328 seconds to compute logits. [repeated 51x across cluster]\n(ServeReplica:default_Ingress pid=268756) [INGRESS] Took 0.06375503540039062 seconds to complete request 329934e0-abdc-4887-a54f-92872a2ffb3f.\n(ServeReplica:default_Ingress pid=268756) [INGRESS] Took 0.0649104118347168 seconds to complete request d443f21d-aa7e-417f-a217-a823251d9324.\n(ServeReplica:default_Ingress pid=268756) [INGRESS] Took 0.06409120559692383 seconds to complete request b69f1ede-269f-4a34-8d20-65ab83a7dd81.\n(ServeReplica:default_Ingress pid=268756) [INGRESS] Took 0.06693530082702637 seconds to complete request 6b0b52fe-162f-491a-8283-d25f026a9b69.\n(ServeReplica:default_Ingress pid=268756) [INGRESS] Took 0.06712460517883301 seconds to complete request c980ae01-32b6-4ccc-9653-b6db9773d14e.\n(ServeReplica:default_Ingress pid=268756) [INGRESS] Took 0.06794095039367676 seconds to complete request 62449a33-474f-48d0-917c-4fc5c7e36288.\n(ServeReplica:default_Ingress pid=268756) [INGRESS] Took 0.06844472885131836 seconds to complete request b5304c68-06ca-4b9b-8487-b40193c10886.\n(ServeReplica:default_Ingress pid=268756) [INGRESS] Took 0.09019851684570312 seconds to complete request 6efd2f1c-0378-46c2-9e2f-4e706dde79f6.\n(ServeReplica:default_Inference pid=268753) INFO 2023-08-02 18:07:03,520 default_Inference default_Inference#AVgvoQ oQzRSNNGan / default replica.py:723 - __CALL__ OK 62.9ms\n(ServeReplica:default_Inference pid=268750) INFO 2023-08-02 18:07:03,515 default_Inference default_Inference#rLlVGC oQzRSNNGan / default replica.py:723 - __CALL__ OK 59.4ms\n(ServeReplica:default_Inference pid=268755) INFO 2023-08-02 18:07:03,521 default_Inference default_Inference#IEpFZe oQzRSNNGan / default replica.py:723 - __CALL__ OK 62.7ms\n(ServeReplica:default_Inference pid=268752) INFO 2023-08-02 18:07:03,515 default_Inference default_Inference#LqbCcH oQzRSNNGan / default replica.py:723 - __CALL__ OK 58.6ms\n(ServeReplica:default_Inference pid=268738) INFO 2023-08-02 18:07:03,519 default_Inference default_Inference#bCbldh oQzRSNNGan / default replica.py:723 - __CALL__ OK 62.5ms\n(ServeReplica:default_Inference pid=268748) INFO 2023-08-02 18:07:03,515 default_Inference default_Inference#gqyjPm oQzRSNNGan / default replica.py:723 - __CALL__ OK 58.3ms\n(ServeReplica:default_Inference pid=268748) INFO 2023-08-02 18:07:03,542 default_Inference default_Inference#gqyjPm oQzRSNNGan / default replica.py:723 - __CALL__ OK 25.9ms\n(HTTPProxyActor pid=268697) INFO:     connection closed\n(ServeReplica:default_Inference pid=268749) INFO 2023-08-02 18:07:03,566 default_Inference default_Inference#mtylLG oQzRSNNGan / default replica.py:723 - __CALL__ OK 21.3ms\n(ServeReplica:default_Ingress pid=268756) [INGRESS] Took 0.11423301696777344 seconds to complete request d4bd4f78-ace7-4a1f-926b-8b1b1f5f633e.\n(ServeReplica:default_Ingress pid=268756) [INGRESS] Took 0.11437821388244629 seconds to complete request 286a1803-76c2-4bea-b969-ddff50923239.\n(ServeReplica:default_Ingress pid=268756) Client disconnected.\n(ServeReplica:default_Ingress pid=268756) INFO 2023-08-02 18:07:03,569 default_Ingress default_Ingress#OmwEVs oQzRSNNGan / default replica.py:723 - __CALL__ OK 120.1ms```\nClient logs:\n```Took 0.06707334518432617 seconds for request 329934e0-abdc-4887-a54f-92872a2ffb3f to be fulfilled.\nTook 0.06737732887268066 seconds for request d443f21d-aa7e-417f-a217-a823251d9324 to be fulfilled.\nTook 0.06707978248596191 seconds for request b69f1ede-269f-4a34-8d20-65ab83a7dd81 to be fulfilled.\nTook 0.06934118270874023 seconds for request 6b0b52fe-162f-491a-8283-d25f026a9b69 to be fulfilled.\nTook 0.06924915313720703 seconds for request c980ae01-32b6-4ccc-9653-b6db9773d14e to be fulfilled.\nTook 0.06953263282775879 seconds for request 62449a33-474f-48d0-917c-4fc5c7e36288 to be fulfilled.\nTook 0.0698697566986084 seconds for request b5304c68-06ca-4b9b-8487-b40193c10886 to be fulfilled.\nTook 0.09227132797241211 seconds for request 6efd2f1c-0378-46c2-9e2f-4e706dde79f6 to be fulfilled.\nTook 0.1163175106048584 seconds for request d4bd4f78-ace7-4a1f-926b-8b1b1f5f633e to be fulfilled.\nTook 0.11629104614257812 seconds for request 286a1803-76c2-4bea-b969-ddff50923239 to be fulfilled.\n[{'id': 'd443f21d-aa7e-417f-a217-a823251d9324', 'start': 1691024823.4519286, 'logits': [[-0.11890064179897308, 0.08033183962106705]]}, {'id': '6b0b52fe-162f-491a-8283-d25f026a9b69', 'start': 1691024823.4520264, 'logits': [[-0.02248799428343773, -0.021837597712874413]]}, {'id': 'c980ae01-32b6-4ccc-9653-b6db9773d14e', 'start': 1691024823.45214, 'logits': [[-0.0276297926902771, 0.11271484941244125]]}, {'id': '329934e0-abdc-4887-a54f-92872a2ffb3f', 'start': 1691024823.4521916, 'logits': [[0.018737144768238068, -0.05270552635192871]]}, {'id': 'b69f1ede-269f-4a34-8d20-65ab83a7dd81', 'start': 1691024823.4522426, 'logits': [[-0.11278679221868515, 0.06468745321035385]]}, {'id': '62449a33-474f-48d0-917c-4fc5c7e36288', 'start': 1691024823.4522882, 'logits': [[0.1064518615603447, -0.03943554684519768]]}, {'id': 'd4bd4f78-ace7-4a1f-926b-8b1b1f5f633e', 'start': 1691024823.4523306, 'logits': [[-0.02706400863826275, 0.12094366550445557]]}, {'id': '286a1803-76c2-4bea-b969-ddff50923239', 'start': 1691024823.4523768, 'logits': [[-0.04014109447598457, 0.11138756573200226]]}, {'id': '6efd2f1c-0378-46c2-9e2f-4e706dde79f6', 'start': 1691024823.4524207, 'logits': [[-0.09290292859077454, 0.04749571159482002]]}, {'id': 'b5304c68-06ca-4b9b-8487-b40193c10886', 'start': 1691024823.4524636, 'logits': [[0.0392649807035923, -0.048447370529174805]]}]```"}
{"question": "Hello all, Is it possible to build ray for the RISC-V architecture instead of x86?"}
{"question": "hey guys, I have a question about ray tune multi-tenancy, why is ray not designed to run multiple tune jobs at the same time? It seems counter-intuitive to me. And maybe I\u2019ve always understood it wrong about the concept of ray cluster, a documentation suggests to create a cluster per job instead of a cluster shared by multiple jobs/people. Since ray has auto-scaling, I have designed my ML Platform solution by deploying 1 ray cluster on k8s which is shared by all users. So far it\u2019s working well, what would you suggest for ray tune?  <https://discuss.ray.io/t/ray-tune-multi-tenancy/11669>"}
{"question": "Hello everyone! I have a question. Currently, I have two server running, each with their own ip (ex 172.12.1.88 and 172.12.1.65). I started the ray cluster on 172.12.1.88 with following command:\n```ray start --head --port 1234```\nand it has successfully running (observation from `ray status` and dashboard)\nOn the other server (172.12.1.65), I have make it a worker node with command:\n```ray start --address 172.12.1.88```\nIt seems like this is working (observation from `ray status` and dashboard).\nNow, I wanna connect to this cluster via python scripts on server 172.12.1.65.\n```import ray\n\nray_address = \"172.12.1.88\"\nray.init(address = ray_address)```\nand run the script using:\n```python testing.py \n2023-08-04 15:10:26,326 INFO worker.py:1431 -- Connecting to existing Ray cluster at address: 172.12.1.88:1234...\n[2023-08-04 15:10:26,333 I 691185 691185] <http://global_state_accessor.cc:356|global_state_accessor.cc:356>: This node has an IP address of 172.12.1.65, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.\n2023-08-04 15:10:26,449 INFO worker.py:1612 -- Connected to Ray cluster. View the dashboard at <http://172.12.1.88:8265> \n[2023-08-04 15:10:35,455 E 691185 691198] <http://core_worker_process.cc:216|core_worker_process.cc:216>: Failed to get the system config from raylet because it is dead. Worker will terminate. Status: GrpcUnavailable: RPC Error message: failed to connect to all addresses; RPC Error details:  .Please see `raylet.out` for more details.```\nDid anyone know does this error occur and how to solve it?"}
{"question": "Hi Team,  I have created a ray cluster with 3 node, one of the node has a GPU , but when I try to run the `ray status`   command, it shows no GPU there.\n```ecl@ecl:~$ ray status\n======== Autoscaler status: 2023-08-04 08:52:16.908103 ========\nNode status\n---------------------------------------------------------------\nHealthy:\n 1 node_cb25ef00340d552cb7dab308aa13663d304b754af803adcdfb6d4f4f\n 1 node_cc4a0c2b6ffdc646824845f69539650847c382274cb47666cd70ed48\n 1 node_03e9f087c781efba77c0f294019e79dd3169535b2e794678f5d99420\nPending:\n (no pending nodes)\nRecent failures:\n (no failures)\n\nResources\n---------------------------------------------------------------\nUsage:\n 0.0/5.0 CPU\n 0B/8.91GiB memory\n 0B/3.94GiB object_store_memory\n\nDemands:\n (no resource demands)```\n Here is the output of lshw\n\n```*-display\n       description: VGA compatible controller\n       product: GM107GL [Quadro K1200]\n       vendor: NVIDIA Corporation\n       physical id: 0\n       bus info: pci@0000:0b:00.0\n       version: a2\n       width: 64 bits\n       clock: 33MHz\n       capabilities: pm msi pciexpress vga_controller bus_master cap_list\n       configuration: driver=nouveau latency=64\n       resources: irq:59 memory:fc000000-fcffffff memory:d0000000-dfffffff memory:e4000000-e5ffffff ioport:5000(size=128)```\nAny suggestion for this?"}
{"question": "I'm getting a lot of\n```OSError: Failed to download runtime_env file package <gcs://_ray_pkg_5cd6b21b2efc88a0.zip> from the GCS to the Ray worker node. The package may have prematurely been deleted from the GCS due to a long upload time or a problem with Ray. Try setting the environment variable RAY_RUNTIME_ENV_TEMPORARY_REFERENCE_EXPIRATION_S  to a value larger than the upload time in seconds (the default is 600). If this fails, try re-running after making any change to a file in the file package.```\nafter bumping from `2.5.1` to `2.6.1`. did something change? I'm happy to try to apply the hint in the message, but my runtime env really isn't that big so I'm not sure what the problem is"}
{"question": "Any reason I'd see ray use all threads when installed from pip using system python (ubuntu 22.04) but when I use conda python I it seems confined to  a single cpu?  (it does still spawn a number ray processes that's about equal to my thread count, but each uses only ~ 7% of cpu, while the system python each ray process spawned uses 100% and saturates the processor effectively)"}
{"question": "HI Folks! I am using cloud storage based syncing (s3)  in my DataParallelTrainer through storage path but I am not reporting checkpoints (No session.report calls in my training loop function). It looks like my trial artifacts are not being synced either then. How do I get around this? I want the trial artifacts to get synced."}
{"question": "Hi Folks! I am trying to run my application via ray-client using the following code:\n`ray.init(address=ray_address,runtime_env={\"working_dir\":\"./\",\"excludes\":excludes,\"py_modules\":[\"src/word_autocomplete\"],\"pip\":\"/home/jackson/Next_Word_Autocomplete/word-autocomplete/requirements.txt\"})`\nBut it show error:\n```ValueError: Local directory /tmp/ray/session_2023-08-07_09-42-41_348346_2128188/runtime_resources/pip/be761d619fdc1882001721f8dcc62801c604c6f1 for URI \n<pip://be761d619fdc1882001721f8dcc62801c604c6f1> does not exist on the cluster. Something may have gone wrong while installing the runtime_env `pip` packages.```\nDid anyone know why does this occur?"}
{"question": "Hey all, I have a question about Ray task scheduling strategy and custom resources. I'm seeing the following scheduling behavior in a cluster with a CPU-only node with 4 logical cores and a GPU node - if I schedule 4 tasks that each request 1 CPU, they all get scheduled on the CPU-only node and the 5th task onward gets on the GPU node. This is what I'd expect, as we'd normally want to reserve the GPU node for tasks that require GPU.\nin a cluster with the same CPU-only node and a node with a custom resource specified, the first task lands on the custom resource node and the next 4 go on the CPU-only node. is this expected scheduling behavior under the default task scheduling strategy? is there another way I can indicate that tasks that don't request custom resources should preferentially run on nodes without it in the same way that GPU resources seem to be treated?\n\nthanks in advance!"}
{"question": "Is it good practice to not allocate all cores to tasks on worker node? e.g. If I have a 32 core node, and have 16 tasks that launch with 2 num_cpus, would I trigger a node failure because raylet wouldn\u2019t have enough resources to communicate with the head node?"}
{"question": "Greetings,\n\nI am seeking more information on how to secure my remote server using Ray remote. If a hacker knows the IP and open port to the server, is it possible for the hacker to run any python code on the machine using Ray? What steps are recommended to take in order to secure the server?"}
{"question": "Why do I always get the following error? It occurs randomly whenever I try to submit my job to the Ray cluster...\n```ValueError: Local directory /tmp/ray/session_2023-08-07_09-42-41_348346_2128188/runtime_resources/pip/be761d619fdc1882001721f8dcc62801c604c6f1 for URI \n<pip://be761d619fdc1882001721f8dcc62801c604c6f1> does not exist on the cluster. Something may have gone wrong while installing the runtime_env `pip` packages.```\n"}
{"question": "After I have run 2 jobs, `ray job list` becomes super slow, and takes approximately 2 minutes more than 9 times out of 10. Some rare cases it takes few seconds, but most of the time response time is untolerable.\n\nI'm not sure what has triggered this since I don't recall job list being so slow earlier. I updated Ray recently to 2.6.1. My Ray cluster is managed by Kuberay and I have 1 CPU / 4Gig memory for head. Dashboard nor top does not show anything suspicious in CPU load or in the memory usage.\n\nAny suggestions how to hunt the origin of this behaviour?"}
{"question": "Hi all, I have a question about head node reconnection from worker:\n\nI am operating a private compute platform where kuberay is not directly usable. I have some basic fault tolerance scheme of the head node so the head node can automatically restart and restore states from a Redis server.  However, when head node recovers, it may have a different IP and port.\n\nSo, one question on worker side is how to reconnect to the new head node WITHOUT interrupting running actors and tasks on the worker node. My expected behavior is that, during the head node recovery period, all actors/tasks on all workers can still operate. Once the new head node is available, worker raylet can reconnect to the new head node without effecting all previous running actors/tasks.\n\nI wonder if that\u2019s doable?"}
{"question": "hi all! i'm a roblox employee looking into ray. if i'm running ray on a kubernetes cluster, how can i best check what EC2 instance type(s) my ray work is running on?"}
{"question": "Hi there, is it possible to schedule group of `Task`s to be run by a specific ActorPool?"}
{"question": "Hi All, I\u2019m noticing that ray uses `s3fs`, but when I look at the package, the default and none of the extra appear to show that dependency:\n\n```[package.dependencies]\naiohttp = {version = \"&gt;=3.7\", optional = true, markers = \"extra == \\\"air\\\"\"}\naiohttp-cors = {version = \"*\", optional = true, markers = \"extra == \\\"air\\\"\"}\naiorwlock = {version = \"*\", optional = true, markers = \"extra == \\\"air\\\"\"}\naiosignal = \"*\"\nclick = \"&gt;=7.0\"\ncolorful = {version = \"*\", optional = true, markers = \"extra == \\\"air\\\"\"}\nfastapi = {version = \"*\", optional = true, markers = \"extra == \\\"air\\\"\"}\nfilelock = \"*\"\nfrozenlist = \"*\"\nfsspec = {version = \"*\", optional = true, markers = \"extra == \\\"air\\\"\"}\ngpustat = {version = \"&gt;=1.0.0\", optional = true, markers = \"extra == \\\"air\\\"\"}\ngrpcio = {version = \"&gt;=1.42.0\", markers = \"python_version &gt;= \\\"3.10\\\"\"}\njsonschema = \"*\"\nmsgpack = \"&gt;=1.0.0,&lt;2.0.0\"\nnumpy = [\n    {version = \"&gt;=1.19.3\", markers = \"python_version &gt;= \\\"3.9\\\"\"},\n    {version = \"&gt;=1.20\", optional = true, markers = \"extra == \\\"air\\\"\"},\n]\nopencensus = {version = \"*\", optional = true, markers = \"extra == \\\"air\\\"\"}\npackaging = \"*\"\npandas = {version = \"&gt;=1.3\", optional = true, markers = \"extra == \\\"air\\\"\"}\nprometheus-client = {version = \"&gt;=0.7.1\", optional = true, markers = \"extra == \\\"air\\\"\"}\nprotobuf = \"&gt;=3.15.3,&lt;3.19.5 || &gt;3.19.5\"\npy-spy = {version = \"&gt;=0.2.0\", optional = true, markers = \"extra == \\\"air\\\"\"}\npyarrow = {version = \"&gt;=6.0.1\", optional = true, markers = \"extra == \\\"air\\\"\"}\npydantic = {version = \"&lt;2\", optional = true, markers = \"extra == \\\"air\\\"\"}\npyyaml = \"*\"\nrequests = \"*\"\nsmart-open = {version = \"*\", optional = true, markers = \"extra == \\\"air\\\"\"}\nstarlette = {version = \"*\", optional = true, markers = \"extra == \\\"air\\\"\"}\ntensorboardX = {version = \"&gt;=1.9\", optional = true, markers = \"extra == \\\"air\\\"\"}\nuvicorn = {version = \"*\", optional = true, markers = \"extra == \\\"air\\\"\"}\nvirtualenv = {version = \"&gt;=20.0.24,&lt;20.21.1\", optional = true, markers = \"extra == \\\"air\\\"\"}\n\n[package.extras]\nair = [\"aiohttp (&gt;=3.7)\", \"aiohttp-cors\", \"aiorwlock\", \"colorful\", \"fastapi\", \"fsspec\", \"gpustat (&gt;=1.0.0)\", \"numpy (&gt;=1.20)\", \"opencensus\", \"pandas\", \"pandas (&gt;=1.3)\", \"prometheus-client (&gt;=0.7.1)\", \"py-spy (&gt;=0.2.0)\", \"pyarrow (&gt;=6.0.1)\", \"pydantic (&lt;2)\", \"requests\", \"smart-open\", \"starlette\", \"tensorboardX (&gt;=1.9)\", \"uvicorn\", \"virtualenv (&gt;=20.0.24,&lt;20.21.1)\"]\nall = [\"aiohttp (&gt;=3.7)\", \"aiohttp-cors\", \"aiorwlock\", \"colorful\", \"dm-tree\", \"fastapi\", \"fsspec\", \"gpustat (&gt;=1.0.0)\", \"grpcio (!=1.56.0)\", \"gymnasium (==0.26.3)\", \"lz4\", \"numpy (&gt;=1.20)\", \"opencensus\", \"opentelemetry-api\", \"opentelemetry-exporter-otlp\", \"opentelemetry-sdk\", \"pandas\", \"pandas (&gt;=1.3)\", \"prometheus-client (&gt;=0.7.1)\", \"py-spy (&gt;=0.2.0)\", \"pyarrow (&gt;=6.0.1)\", \"pydantic (&lt;2)\", \"pyyaml\", \"ray-cpp (==2.6.2)\", \"requests\", \"rich\", \"scikit-image\", \"scipy\", \"\n\u21aa smart-open\", \"starlette\", \"tensorboardX (&gt;=1.9)\", \"typer\", \"uvicorn\", \"virtualenv (&gt;=20.0.24,&lt;20.21.1)\"]\nclient = [\"grpcio (!=1.56.0)\"]\ncpp = [\"ray-cpp (==2.6.2)\"]\ndata = [\"fsspec\", \"numpy (&gt;=1.20)\", \"pandas (&gt;=1.3)\", \"pyarrow (&gt;=6.0.1)\"]\ndefault = [\"aiohttp (&gt;=3.7)\", \"aiohttp-cors\", \"colorful\", \"gpustat (&gt;=1.0.0)\", \"opencensus\", \"prometheus-client (&gt;=0.7.1)\", \"py-spy (&gt;=0.2.0)\", \"pydantic (&lt;2)\", \"requests\", \"smart-open\", \"virtualenv (&gt;=20.0.24,&lt;20.21.1)\"]\nobservability = [\"opentelemetry-api\", \"opentelemetry-exporter-otlp\", \"opentelemetry-sdk\"]\nrllib = [\"dm-tree\", \"gymnasium (==0.26.3)\", \"lz4\", \"pandas\", \"pyarrow (&gt;=6.0.1)\", \"pyyaml\", \"requests\", \"rich\", \"scikit-image\", \"scipy\", \"tensorboardX (&gt;=1.9)\", \"typer\"]\nserve = [\"aiohttp (&gt;=3.7)\", \"aiohttp-cors\", \"aiorwlock\", \"colorful\", \"fastapi\", \"gpustat (&gt;=1.0.0)\", \"opencensus\", \"prometheus-client (&gt;=0.7.1)\", \"py-spy (&gt;=0.2.0)\", \"pydantic (&lt;2)\", \"requests\", \"smart-open\", \"starlette\", \"uvicorn\", \"virtualenv (&gt;=20.0.24,&lt;20.21.1)\"]\ntrain = [\"pandas\", \"pyarrow (&gt;=6.0.1)\", \"requests\", \"tensorboardX (&gt;=1.9)\"]\ntune = [\"pandas\", \"pyarrow (&gt;=6.0.1)\", \"requests\", \"tensorboardX (&gt;=1.9)\"]```\nWe notice this causes issues.\n\nThis seems odd because I see it in the ray repository on github (e.g., <https://github.com/ray-project/ray/blob/62fc2293821ce76765b2d82c257c60e86609a06d/python/requirements/ml/core-requirements.txt#L21>). What\u2019s the right way to install ray such that it brings in the s3fs dependency that it needs itself, rather than making us add it as a dependency ourselves?"}
{"question": "Hello, how can I make ray code be able to import code that exists outside the working directory? This code should exist in the image somewhere else"}
{"question": "When we submit a job on a cluster, where does the driver end up living?"}
{"question": "In Ray version &gt;2.5.1 it introduces pydantic 2 version checks with `is_pydantic_2 = pydantic.__version__.startswith(\"2\")`\n\nThis will cause runtime errors if pydantic &lt; 1.9.0 due to earlier versions of pydantic not having the version attribute: <https://github.com/pydantic/pydantic/pull/2573>\n\nThe TODO comment in Ray suggests that this will get fixed at some point at future by different approach?\n\nDo you think that it would be feasible make an issue about this? The fix would be simple `hasattr(pydantic, __version__) and  ...` or would the change in TODO land soon?"}
{"question": "I see the following warning in the documentation:\n&gt; Ray *does not permit dynamic updates of resource capacities after Ray has been started on a node*.\nIs there any workaround for this? I am working with some shared resources and need to be able to adjust the resources available to ray based on what else is happening on a node. It seems like this is probably a common scenario, so I am surprised it's not supported directly."}
{"question": "Is there an estimate of when Ray 2.6.3 or 2.7.0 will be released? Thanks!"}
{"question": "can anyone install ray-rllib 2.6.2 successfully from conda?  I can install ray-core 2.6.2, but attempting to install ray-rllib prompts me to downgrade my ray suite to 2.1.0, which I really don't want to do.  If I use pip I have no problems installing ray[rllib] 2.6.2, just asking about conda here."}
{"question": "Is it possible to update `max_workers` and `min_workers` of a running ray cluster without restarting the head node?"}
{"question": "Hi guys, is there any way of calling ray.get(string(ObjectID))? Once I first call ray.put() to generate some objects somewhere, I expect I can run another python script to call ray.get() to do something with one given Object. But looks ray.get() needs ObjectRef as a good parameter. I was just thinking if I can do something like ray.get(string(ObjectID)). Any suggestions."}
{"question": "Can Ray use named objects? As in: `ray.put(obj, ObjRef=\"my_key\")`  so that I can have a pre-defined an `ObjRef`  and used in another process like `ray.get(\"my_key\")`"}
{"question": "Hi guys , this can be a  simple question  , i am trying to understand the benefits of  ray for  llm fine tuning.  Let say that i have 8 gpu cores machine and i want to fine tune a llm model  such as falcon 7b etc.  And lets say that  at the same time  2  people fine tune the model for specific needs.  Now the model must be loaded 2 times  to gpu memories  right?  Is there any way to use the same memory  for  both  processes?  Thanks"}
{"question": "Hi guys, I'm wondering, I made a custom environment that has ends after about 27K steps as it infers the state from time series data. Not sure how important that is, but when I run it, I get initially nan values for the reward. When I change hyperparameters, it can sometimes only get nan values for reward. Anyone has an idea why this exactly is? I mean of course during training."}
{"question": "Is is possible to have control over an object lifecycle inside the Object store?"}
{"question": "very dumb question, for an offline batch inference use case in ray, what's the difference between a batch and a block? i get that batch is user-facing and block is under the hood, but unsure how ray handles the two"}
{"question": "Hello,\n\nWhen I was setting up cluster on Anyscale. I am getting this issue.\nHow can I solve it ?\n\n*<https://global.discourse-cdn.com/business7/uploads/ray/original/2X/1/12fa59aadcdf5c806c7855522cbfc30c7358cf56.png|image>*\n*<https://global.discourse-cdn.com/business7/uploads/ray/original/2X/1/12fa59aadcdf5c806c7855522cbfc30c7358cf56.png|1062\u00d7172 31 KB>*"}
{"question": "Hey Team,  Is it possible to tailor the Ray, only remain the C++ kernel framework,  and then develop computing applications with C++?"}
{"question": "Hi Team, Greetings!! We are planning to deploy NLP and CV models on Ray Serve using K8s by creating Ray Clusters and Kuberay operator.\nCloud provider = AWS\nRay documentation says - Since <https://ray-project.github.io/kuberay/|KubeRay> is still considered alpha, or experimental, so some APIs may be subject to change. Is it fine to deploy NLP and CV models on Ray serve using the Ray service alpha version? or Could you please share your experience with deploying models on Production?\nThanks"}
{"question": "Hi, I am trying to run Ray on my AWS ec2 instance, I would like to establish my own head node with specified resources and a worker node with specified resource, how can this be done?"}
{"question": "Hello everyone.  Is there any stream processing engine integrated with ray now?  Should the components of the stream processing engine run in ray or just treat it as an independent service and call it through the API?"}
{"question": "Hello I\u2019m trying to use ray for training but seeing that our pytorch training keeps randomly failing with segmentation faults:\n```Fatal Python error: Segmentation fault```\nAny ideas why this might be happening?"}
{"question": "Hello,,\n\nThere was a AWS IAM permission list as json (if I remember true), But after update Anyscale docs I couldnt find it. Is there any aws IAM permission list to when creating aws cloud ?"}
{"question": "Hello,\nI want to apply ray.serve for my LLM model.\n\nIt could be basic problem. But I stuck with this problem.\n\nmy app2.py:\n\n`````\n`import requests`\n`from ray import serve`\n`import starlette`\n\n`@serve.deployment(route_prefix=\"/forecast\")`\n`class Ray_llm:`\n    `async def __call__(self, request: starlette.requests.Request):`\n        `if \"file\" not in requests.files:`\n            `return {\"error\": \"No file part in the request\"}, 400`\n\n        `file = requests.files[\"file\"]`\n\n        `if file.filename == \"\":`\n            `return {\"error\": \"No selected file\"}, 400`\n\n        `query_text = requests.form.get(\"query\", None)`\n        `if not query_text:`\n            `return {\"error\": \"No query text provided\"}, 400`\n\n        `if file and query_text:`\n\n            `response = send_to_llm(file, query_text)`\n            `return response`\n\n`def send_to_llm(file, querry_text):`\n\n    `response = llm_caller(file, querry_text)  # Send to model`\n\n    `return response`\n\n`app = Ray_llm.bind()`\n`serve.run(app, port=8081)`\n`````\n\nDockerfile:\n\n\n`````\n`FROM python:3.8-slim`\n\n`WORKDIR /app`\n\n`COPY requirements.txt .`\n\n`RUN pip install --no-cache-dir -r requirements.txt`\n`RUN pip install \"ray[serve]\"`\n\n`EXPOSE 8081`\n\n`COPY . .`\n\n`CMD [\"python\", \"app2.py\"]`\n`````\n\nBuildAndRun.sh :\n\n`````\n`docker build -t llm_api .`\n\n`docker run -p 8081:5000 llm_api`\n`````\n\n\nwhen I run BuildAndRun.sh  my service does not continue to request. How can I solve this problem?\n\nss:"}
{"question": "I'm implementing a `NodeProvider` and I make some changes to the `config` passed by the upstream `_bootstrap_config` to `bootstrap_config`.\n\nWhat's the best way to make these config changes persist in the underlying ray cluster yaml? Can I somehow get the path of the cluster yaml inside the NodeProvider class?"}
{"question": "I'm using a degenerate Ray cluster on one node, started by the `aio HTTP server` in the code so that Ray takes care of multiprocessing. It was good until we left it running over the weekend, and the memory consumption on the node just kept increasing until it hit the `RAY_memory_usage_threshold` and from that point on nothing worked.  looking at the memory usage charts, it just keeps going up with vey brief periods when it goes downward. Why doesn't Ray garbage collect from shm frequently?"}
{"question": "x-posting from <#CNECXMW22|tune>\n\nQuestion about tune.choice/tune.loguniform: Is there a way to get them to be concrete while logging? I'm assigning them to model parameters, but then when calling something like `model.layer_1_size` I get a different result that what the model was initialized with, which makes it hard to log."}
{"question": "I am creating two different `LightGBMTrainer` and fitting them on the same `Dataset` and passing in the same `random_state` into params, then predicting on same data, but the predicted values are different. how could this happen? is there another source of nondeterminism besides the trainer random state?"}
{"question": "also, why must `num_boost_round &gt; 0` and `num_leaves &gt; 1` ? if I wanted to use a minimal config for a linear fit with `LightGBMTrainer`, why disallow that"}
{"question": "Hey, is there a way to make the autoscaler fail fast and loud,  instead of just a warning `\"The following resource request cannot be scheduled right now\"` ? The load we're running is pretty consistent and we want to know if the cluster is underprovisioned the moment we start the actors"}
{"question": "Hello \nCould anyone please help me set up my cluster over high laytency network?\nI'm having issues whenever node to node network latency reaches a few tens of msec.\nI posted the details of my attempts and debugging info in <#CN2RGCHRR|ray-clusters> but that channel doesn't seem to be active.\nThank you so much!"}
{"question": "Hello, I am trying to use impala algorithm in my server. When I run my code locally, it works well. However, when I run the code at my server by slrum, error below appears.\n```/var/spool/slurmd/job4781601/slurm_script: line 19: 48241 Segmentation fault      (core dumped) python launch.py```\nThis error appears when I just import ray libraries.\n```import ray\nfrom ray import air, tune\nfrom ray.rllib.examples.env.look_and_push import LookAndPush, OneHot\nfrom ray.rllib.examples.env.repeat_after_me_env import RepeatAfterMeEnv\nfrom ray.rllib.examples.env.repeat_initial_obs_env import RepeatInitialObsEnv\nfrom ray.rllib.examples.env.stateless_cartpole import StatelessCartPole\nfrom ray.rllib.utils.framework import try_import_tf\nfrom ray.rllib.utils.test_utils import check_learning_achieved\nfrom ray.tune import registry\nfrom ray.tune.logger import pretty_print```\nI think this is because my code use too much resources in the server but It uses so small resources that I can run it in my laptop.\nI am suffering this problem for a week\u2026 Is there any solution? thank you."}
{"question": "anybody ever struggled / solved using `ray.init` in a pytest fixture on Github Actions? it works fine locally, but as soon as I try to run it in github actions it hangs without ever running the unit tests"}
