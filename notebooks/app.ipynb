{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "154b2d0e-f7ce-453b-b3b7-eda0666a9795",
   "metadata": {},
   "source": [
    "# Rag-based LLM Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de569042-32c7-4bea-a1ef-f0e41e260645",
   "metadata": {},
   "source": [
    "- https://github.com/ray-project/llm-applications\n",
    "- https://endpoints.anyscale.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af14d4-478a-418b-a738-b17012188779",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e538090b-c736-46e7-8427-298ecc3e50f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import ray\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "633996c3-45b4-4ac6-961d-56b0df9156c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys; sys.path.append(\"..\")\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv; load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "67203efd-979e-4ea5-97a2-f4115c02dcad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ray/default/llm-applications\n"
     ]
    }
   ],
   "source": [
    "EFS_DIR = Path(\"/efs/shared_storage/simon\")\n",
    "ROOT_DIR = Path(os.getcwd()).parent\n",
    "print (ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36dc38-f797-4db9-9979-2450764679aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m9s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m14s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m19s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m24s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m29s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m34s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m38s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m43s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m48s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m53s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h23m59s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h24m4s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h24m9s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h24m14s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h24m19s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h24m24s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h24m29s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h24m39s)\u001b[0m Resized to 16 CPUs, 1 GPUs.\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h50m9s)\u001b[0m Adding 1 node(s) of type worker-node-type-0.\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h51m20s)\u001b[0m Resized to 32 CPUs, 2 GPUs.\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h56m25s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h56m30s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h56m36s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h56m41s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h56m46s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h56m51s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h56m56s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m0s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m5s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m10s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m15s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m20s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m26s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m31s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m36s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m41s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m46s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m51s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h57m56s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h58m1s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h58m10s)\u001b[0m Resized to 16 CPUs, 1 GPUs.\n"
     ]
    }
   ],
   "source": [
    "# Credentials\n",
    "ray.init(runtime_env={\"env_vars\": {\n",
    "    \"OPENAI_API_BASE\": os.environ[\"OPENAI_API_BASE\"],\n",
    "    \"OPENAI_API_KEY\": os.environ[\"OPENAI_API_KEY\"], \n",
    "    \"ANYSCALE_API_BASE\": os.environ[\"ANYSCALE_API_BASE\"],\n",
    "    \"ANYSCALE_API_KEY\": os.environ[\"ANYSCALE_API_KEY\"],\n",
    "    \"DB_CONNECTION_STRING\": os.environ[\"DB_CONNECTION_STRING\"],\n",
    "}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c503edd-963a-4ec3-9182-39f7afc44153",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82562f-2b5d-4e8d-9716-b0da8670e8bf",
   "metadata": {},
   "source": [
    "Our data is already ready at `/efs/shared_storage/goku/docs.ray.io/en/master/` (on Staging, `us-east-1`) but if you wanted to load it yourself, run this bash command (change `/desired/output/directory`, but make sure it's on the shared storage,\n",
    "so that it's accessible to the workers):\n",
    "```bash\n",
    "export DOCS_PATH=/efs/shared_storage/simon/docs.ray.io/en/master/\n",
    "wget -e robots=off --recursive --no-clobber --page-requisites \\\n",
    "  --html-extension --convert-links --restrict-file-names=windows \\\n",
    "  --domains docs.ray.io --no-parent --accept=html \\\n",
    "  -P $DOCS_PATH https://docs.ray.io/en/master/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6b43b-ea82-4c21-a885-57178cec3b44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h33m38s)\u001b[0m Adding 1 node(s) of type worker-node-type-0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function StreamingExecutor.__del__ at 0x7f7c04630ca0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/streaming_executor.py\", line 147, in __del__\n",
      "    self.shutdown()\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/data/_internal/execution/streaming_executor.py\", line 160, in shutdown\n",
      "    self.join(timeout=2.0)\n",
      "  File \"/home/ray/anaconda3/lib/python3.9/threading.py\", line 1055, in join\n",
      "    raise RuntimeError(\"cannot join thread before it is started\")\n",
      "RuntimeError: cannot join thread before it is started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h34m54s)\u001b[0m Resized to 32 CPUs, 2 GPUs.\n"
     ]
    }
   ],
   "source": [
    "# Ray dataset\n",
    "docs_path = Path(EFS_DIR, \"docs.ray.io/en/master/\")\n",
    "ds = ray.data.from_items([{\"path\": path} for path in docs_path.rglob(\"*.html\") if not path.is_dir()])\n",
    "print(f\"{ds.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9edff6-6dbf-4037-9675-ae05cd3eb7a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a745a-6445-437c-8f91-92b274c6716f",
   "metadata": {},
   "source": [
    "### Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "45eab701-0238-43e2-a7f1-3816f5e5e54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d17bceaa-a5b0-44b0-89da-5ef4d0581771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_html_file(path):\n",
    "    with open(path) as f:\n",
    "        soup = BeautifulSoup(f.read())\n",
    "    html_tags = [\n",
    "        (\"div\", {\"role\": \"main\"}),\n",
    "        (\"main\", {\"id\": \"main-content\"}),\n",
    "    ]\n",
    "    text = None\n",
    "    for tag, attrs in html_tags:\n",
    "        text = soup.find(tag, attrs)\n",
    "        # if found, break\n",
    "        if text is not None:\n",
    "            break\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0202b625-b581-4242-b51c-1c6c29e42c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaggedStr:\n",
    "    def __init__(self, value, tag):\n",
    "        self.value = value\n",
    "        self.tag = tag\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.value) + f\" [{self.tag}]\" if self.tag else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "234f2d9d-cd88-4352-b35f-dd51ff09fd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_tagged_text(path, element, section=None):\n",
    "    \"Recursively convert a BeautifulSoup element to text, keeping track of sections.\"\n",
    "    results = []\n",
    "    for child in element.children:\n",
    "        if isinstance(child, NavigableString):\n",
    "            results.append(TaggedStr(str(child), section))\n",
    "        elif isinstance(child, Tag):\n",
    "            if child.name == \"section\" and \"id\" in child.attrs:\n",
    "                results.extend(convert_to_tagged_text(path, child, section=child.attrs[\"id\"]))\n",
    "            elif not child.find_all(\"section\"):\n",
    "                results.append(TaggedStr(child.get_text(), section))\n",
    "            else:\n",
    "                results.extend(convert_to_tagged_text(path, child, section))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bcae5532-9e58-46b8-bfe3-b9c133a805ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_tagged_text(chunks):\n",
    "    result = []\n",
    "    for item in chunks:\n",
    "        if result and item.value.strip() == \"\":\n",
    "            result[-1].value += item.value\n",
    "        elif result and item.tag == result[-1].tag:\n",
    "            result[-1].value += item.value\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a9f6dd4b-effa-4a3e-8f6f-770c51d3ec51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_to_uri(path, scheme=\"https://\", domain=\"docs.ray.io\"):\n",
    "    return scheme + domain + path.split(domain)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7ba226d1-eb4d-46a4-9a34-b2217cc5911c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_file(record):\n",
    "    html_content = load_html_file(record[\"path\"])\n",
    "    if not html_content:\n",
    "        return []\n",
    "    parsed_data = [\n",
    "        {\n",
    "            \"source\": path_to_uri(str(record[\"path\"])) + (\"#\" + chunk.tag if chunk.tag else \"\"),\n",
    "            \"text\": chunk.value,\n",
    "        }\n",
    "        for chunk in group_tagged_text(convert_to_tagged_text(record[\"path\"], html_content))\n",
    "    ]\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5befdbb1-92c7-41db-aaf1-6af24631c9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/efs/shared_storage/simon/docs.ray.io/en/master/ray-overview/index.html\n"
     ]
    }
   ],
   "source": [
    "# Paste any URL from https://docs.ray.io/en/master/ (ex. https://docs.ray.io/en/master/train/faq.html)\n",
    "docs_page_url = \"https://docs.ray.io/en/master/ray-overview/index.html\"\n",
    "path = f\"{str(docs_path)}/{docs_page_url.split('docs.ray.io/en/master/')[-1]}\"\n",
    "print (path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0f1289a8-e184-4422-918e-d32529963124",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': 'https://docs.ray.io/en/master/ray-overview/index.html',\n",
      "  'text': '\\n\\n\\n\\n\\n\\n\\n'},\n",
      " {'source': 'https://docs.ray.io/en/master/ray-overview/index.html#overview',\n",
      "  'text': 'Overview#\\n'\n",
      "          'Ray is an open-source unified framework for scaling AI and Python '\n",
      "          'applications like machine learning. It provides the compute layer '\n",
      "          'for parallel processing so that you don’t need to be a distributed '\n",
      "          'systems expert. Ray minimizes the complexity of running your '\n",
      "          'distributed individual and end-to-end machine learning workflows '\n",
      "          'with these components:\\n'\n",
      "          '\\n'\n",
      "          'Scalable libraries for common machine learning tasks such as data '\n",
      "          'preprocessing, distributed training, hyperparameter tuning, '\n",
      "          'reinforcement learning, and model serving.\\n'\n",
      "          'Pythonic distributed computing primitives for parallelizing and '\n",
      "          'scaling Python applications.\\n'\n",
      "          'Integrations and utilities for integrating and deploying a Ray '\n",
      "          'cluster with existing tools and infrastructure such as Kubernetes, '\n",
      "          'AWS, GCP, and Azure.\\n'\n",
      "          '\\n'\n",
      "          'For data scientists and machine learning practitioners, Ray lets '\n",
      "          'you scale jobs without needing infrastructure expertise:\\n'\n",
      "          '\\n'\n",
      "          'Easily parallelize and distribute ML workloads across multiple '\n",
      "          'nodes and GPUs.\\n'\n",
      "          'Leverage the ML ecosystem with native and extensible integrations.\\n'\n",
      "          '\\n'\n",
      "          'For ML platform builders and ML engineers, Ray:\\n'\n",
      "          '\\n'\n",
      "          'Provides compute abstractions for creating a scalable and robust ML '\n",
      "          'platform.\\n'\n",
      "          'Provides a unified ML API that simplifies onboarding and '\n",
      "          'integration with the broader ML ecosystem.\\n'\n",
      "          'Reduces friction between development and production by enabling the '\n",
      "          'same Python code to scale seamlessly from a laptop to a large '\n",
      "          'cluster.\\n'\n",
      "          '\\n'\n",
      "          'For distributed systems engineers, Ray automatically handles key '\n",
      "          'processes:\\n'\n",
      "          '\\n'\n",
      "          'Orchestration–Managing the various components of a distributed '\n",
      "          'system.\\n'\n",
      "          'Scheduling–Coordinating when and where tasks are executed.\\n'\n",
      "          'Fault tolerance–Ensuring tasks complete regardless of inevitable '\n",
      "          'points of failure.\\n'\n",
      "          'Auto-scaling–Adjusting the number of resources allocated to dynamic '\n",
      "          'demand.\\n'\n",
      "          '\\n'\n",
      "          '\\n'},\n",
      " {'source': 'https://docs.ray.io/en/master/ray-overview/index.html#what-you-can-do-with-ray',\n",
      "  'text': 'What you can do with Ray#\\n'\n",
      "          'These are some common ML workloads that individuals, organizations, '\n",
      "          'and companies leverage Ray to build their AI applications:\\n'\n",
      "          '\\n'\n",
      "          'Batch inference on CPUs and GPUs\\n'\n",
      "          'Parallel training\\n'\n",
      "          'Model serving\\n'\n",
      "          'Distributed training of large models\\n'\n",
      "          'Parallel hyperparameter tuning experiments\\n'\n",
      "          'Reinforcement learning\\n'\n",
      "          'ML platform\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "record = {\"path\": path}\n",
    "pprint(parse_file(record)[:3])  # just first few chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae36c4-b683-4331-988d-afdf40c4e842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract sections\n",
    "sections_ds = ds.flat_map(parse_file)\n",
    "sections = sections_ds.take_all()\n",
    "print (len(sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f5d6b1c9-b308-4ab1-9e30-e18e50464f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      8944.000000\n",
       "mean       1317.454271\n",
       "std        6078.479538\n",
       "min           3.000000\n",
       "25%           3.000000\n",
       "50%         238.500000\n",
       "75%         773.250000\n",
       "max      214790.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats summary\n",
    "sections_lengths = [len(section[\"text\"]) for section in sections]\n",
    "series = pd.Series(sections_lengths)\n",
    "series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c010dfe8-eb8a-4d0b-93ba-14dc81777c64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m10s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m15s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m20s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m25s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m30s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m35s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m40s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m45s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m50s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m56s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h41m1s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h41m6s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h41m10s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h41m15s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h41m20s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h41m25s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h41m30s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h41m40s)\u001b[0m Resized to 16 CPUs, 1 GPUs.\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h17m2s)\u001b[0m Adding 1 node(s) of type worker-node-type-0.\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +7h18m7s)\u001b[0m Resized to 32 CPUs, 2 GPUs.\n"
     ]
    }
   ],
   "source": [
    "# there's some super big sections, let's remove those\n",
    "def remove_outliers_using_iqr(data):\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    return [x for x in data if lower_bound <= x <= upper_bound]\n",
    "\n",
    "sections_lengths = remove_outliers_using_iqr(sections_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "06587e19-723f-4777-aefe-d002c79ad2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h39m55s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +6h40m0s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n"
     ]
    }
   ],
   "source": [
    "# Sections lengths\n",
    "plt.figure(figsize=(18, 3))\n",
    "plt.hist(sections_lengths, bins='auto', color='blue', edgecolor='black', alpha=0.7)\n",
    "# plt.semilogy(sections_lengths, marker='o')\n",
    "plt.xlabel('Section')\n",
    "plt.ylabel('# of characters (log scale)')\n",
    "plt.title('Section Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1769855-6138-4015-ae64-dd8169445dd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8d27e-f708-4731-aefb-458d64726775",
   "metadata": {},
   "source": [
    "Some of these sections are very large, let's apply some chunking to improve this so that we can use these sections as context later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0bf7145b-47d4-4b60-96a8-74c8d8315f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6b6337e6-07b2-459d-a666-45de3aa945c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "54966d0f-33ee-4503-9f40-a673710950bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = text_splitter.create_documents(\n",
    "    texts=[section[\"text\"] for section in sections], \n",
    "    metadatas=[{\"source\": section[\"source\"]} for section in sections]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d85f61be-b2ce-4f1e-aaf9-36cd3ec5aed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57050 chunks\n",
      "\n",
      "('ray.tune.search.basic_variant.BasicVariantGenerator.CKPT_FILE_TMPL#\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " \"BasicVariantGenerator.CKPT_FILE_TMPL = 'basic-variant-state-{}.json'#\")\n",
      "\n",
      "metadata:\n",
      "{'source': 'https://docs.ray.io/en/master/tune/api/doc/ray.tune.search.basic_variant.BasicVariantGenerator.CKPT_FILE_TMPL.html#ray-tune-search-basic-variant-basicvariantgenerator-ckpt-file-tmpl'}\n"
     ]
    }
   ],
   "source": [
    "print (f\"{len(chunks)} chunks\\n\")\n",
    "pprint (chunks[0].page_content)  # a few tokens\n",
    "print (f\"\\nmetadata:\\n{chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5be23c62-3108-49be-994d-5aa033162c71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"ray.tune.search.basic_variant.BasicVariantGenerator.CKPT_FILE_TMPL#\\n\\n\\nBasicVariantGenerator.CKPT_FILE_TMPL = 'basic-variant-state-{}.json'#\", 'source': 'https://docs.ray.io/en/master/tune/api/doc/ray.tune.search.basic_variant.BasicVariantGenerator.CKPT_FILE_TMPL.html#ray-tune-search-basic-variant-basicvariantgenerator-ckpt-file-tmpl'}\n"
     ]
    }
   ],
   "source": [
    "# Ray dataset\n",
    "chunks_ds = ray.data.from_items([{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks])\n",
    "chunks_ds.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c23b31-e7b3-4078-abf7-683f448f5b19",
   "metadata": {},
   "source": [
    "## Embed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "712fe08b-fd19-4cb8-94d9-a7570b2dc09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "from ray.data import ActorPoolStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "83b6a5a3-cd2d-4987-838a-be13e9553080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbedChunks:\n",
    "    def __init__(self, model_name):\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "        encode_kwargs = {\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        if model_name == \"text-embedding-ada-002\":\n",
    "            self.embedding_model = OpenAIEmbeddings(\n",
    "                model=model_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                encode_kwargs=encode_kwargs,\n",
    "                openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        else:\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=model_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                encode_kwargs=encode_kwargs)\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        embeddings = self.embedding_model.embed_documents(batch[\"text\"])\n",
    "        return {\"text\": batch[\"text\"], \"source\": batch[\"source\"], \"embeddings\": embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9715a01e-dc67-4342-a0cb-30e770852097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed chunks\n",
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "embedded_chunks = chunks_ds.map_batches(\n",
    "    EmbedChunks,\n",
    "    fn_constructor_kwargs={\"model_name\": embedding_model_name},\n",
    "    batch_size=100, \n",
    "    num_gpus=1,\n",
    "    compute=ActorPoolStrategy(size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "67dffa1f-19a3-4411-af3f-b161a47ee164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 16:25:41,927\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(EmbedChunks)]\n",
      "2023-08-29 16:25:41,927\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-29 16:25:41,928\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-08-29 16:25:41,940\tINFO actor_pool_map_operator.py:117 -- MapBatches(EmbedChunks): Waiting for 1 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size: 768\n",
      "('ray.tune.search.basic_variant.BasicVariantGenerator.CKPT_FILE_TMPL#\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " \"BasicVariantGenerator.CKPT_FILE_TMPL = 'basic-variant-state-{}.json'#\")\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "sample = embedded_chunks.take(5)\n",
    "print (\"embedding size:\", len(sample[0][\"embeddings\"]))\n",
    "pprint(sample[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "01e88205-cded-472f-bdfa-91fcf92e5d87",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 16:25:51,897\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(EmbedChunks)]\n",
      "2023-08-29 16:25:51,897\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-29 16:25:51,898\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-08-29 16:25:51,909\tINFO actor_pool_map_operator.py:117 -- MapBatches(EmbedChunks): Waiting for 1 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'text': \"ray.tune.search.basic_variant.BasicVariantGenerator.CKPT_FILE_TMPL#\\n\\n\\nBasicVariantGenerator.CKPT_FILE_TMPL = 'basic-variant-state-{}.json'#\",\n",
       "  'source': 'https://docs.ray.io/en/master/tune/api/doc/ray.tune.search.basic_variant.BasicVariantGenerator.CKPT_FILE_TMPL.html#ray-tune-search-basic-variant-basicvariantgenerator-ckpt-file-tmpl',\n",
       "  'embeddings': [-0.01591426506638527,\n",
       "   -0.0022834097035229206,\n",
       "   0.011015957221388817,\n",
       "   0.02575925551354885,\n",
       "   0.06943940371274948,\n",
       "   0.017953861504793167,\n",
       "   0.06783413887023926,\n",
       "   0.02732197195291519,\n",
       "   -0.037638455629348755,\n",
       "   -0.052934594452381134,\n",
       "   -0.023640058934688568,\n",
       "   -0.0232948400080204,\n",
       "   -0.08555987477302551,\n",
       "   0.051743555814027786,\n",
       "   -0.02073810249567032,\n",
       "   0.07852020114660263,\n",
       "   0.06010095030069351,\n",
       "   -0.02123989723622799,\n",
       "   0.01979942061007023,\n",
       "   0.016987843438982964,\n",
       "   0.003351390827447176,\n",
       "   -0.004886355251073837,\n",
       "   0.0072355312295258045,\n",
       "   0.00748998811468482,\n",
       "   -0.013953156769275665,\n",
       "   -0.014453297480940819,\n",
       "   0.005017587915062904,\n",
       "   0.013419992290437222,\n",
       "   -0.07609155774116516,\n",
       "   -0.0028951491694897413,\n",
       "   -0.027865685522556305,\n",
       "   -0.02689374051988125,\n",
       "   -0.016439691185951233,\n",
       "   0.003062741830945015,\n",
       "   -0.040956560522317886,\n",
       "   -0.01022771093994379,\n",
       "   -0.03139886632561684,\n",
       "   -0.03901054710149765,\n",
       "   0.01214607898145914,\n",
       "   -0.0010967958951368928,\n",
       "   -0.019936606287956238,\n",
       "   -0.0162027720361948,\n",
       "   0.0005587052437476814,\n",
       "   0.0011326009407639503,\n",
       "   -0.027441080659627914,\n",
       "   -0.018380794674158096,\n",
       "   -0.022861896082758904,\n",
       "   0.03998994454741478,\n",
       "   -0.01712460070848465,\n",
       "   -0.03322455659508705,\n",
       "   -0.03404640033841133,\n",
       "   0.03931581974029541,\n",
       "   -0.017234383150935173,\n",
       "   0.0008572324877604842,\n",
       "   0.015734873712062836,\n",
       "   0.054112888872623444,\n",
       "   0.010688813403248787,\n",
       "   -0.014241993427276611,\n",
       "   -0.018929675221443176,\n",
       "   -0.0600813589990139,\n",
       "   0.015956396237015724,\n",
       "   -0.014084450900554657,\n",
       "   0.043620988726615906,\n",
       "   0.013997280970215797,\n",
       "   0.059789616614580154,\n",
       "   0.02201417088508606,\n",
       "   0.018422627821564674,\n",
       "   0.07191682606935501,\n",
       "   -0.050575170665979385,\n",
       "   -0.04428582265973091,\n",
       "   -0.0342642180621624,\n",
       "   0.027279069647192955,\n",
       "   0.004186834208667278,\n",
       "   -0.0032865027897059917,\n",
       "   0.03967949002981186,\n",
       "   -0.030874399468302727,\n",
       "   -0.0018893268425017595,\n",
       "   0.026011105626821518,\n",
       "   0.052476875483989716,\n",
       "   0.038530152291059494,\n",
       "   0.0224874597042799,\n",
       "   0.04191674664616585,\n",
       "   0.016283979639410973,\n",
       "   0.05906952545046806,\n",
       "   0.012897000648081303,\n",
       "   -0.04844633489847183,\n",
       "   -0.0247188713401556,\n",
       "   0.05017995089292526,\n",
       "   -0.062028102576732635,\n",
       "   0.0754372701048851,\n",
       "   0.034442733973264694,\n",
       "   -0.030612796545028687,\n",
       "   0.03492254018783569,\n",
       "   0.04692312330007553,\n",
       "   -0.009915100410580635,\n",
       "   -0.05480056628584862,\n",
       "   0.04600003361701965,\n",
       "   -0.019926678389310837,\n",
       "   0.01837562769651413,\n",
       "   0.008636238984763622,\n",
       "   -0.017550837248563766,\n",
       "   -0.03831571340560913,\n",
       "   0.022562772035598755,\n",
       "   -0.00917466077953577,\n",
       "   -0.1243676021695137,\n",
       "   -0.02914111502468586,\n",
       "   -0.001169421710073948,\n",
       "   0.019527427852153778,\n",
       "   0.0024996153078973293,\n",
       "   0.03010738082230091,\n",
       "   -0.02644738182425499,\n",
       "   -0.011685111559927464,\n",
       "   -0.017000770196318626,\n",
       "   -0.0011843382380902767,\n",
       "   -0.07395178079605103,\n",
       "   0.052744925022125244,\n",
       "   0.029259203001856804,\n",
       "   -0.013409251347184181,\n",
       "   0.02511308155953884,\n",
       "   0.028517385944724083,\n",
       "   0.04070912301540375,\n",
       "   0.03195900470018387,\n",
       "   0.005822272505611181,\n",
       "   0.09309142827987671,\n",
       "   0.046102963387966156,\n",
       "   0.045363880693912506,\n",
       "   0.008828622289001942,\n",
       "   0.05991638824343681,\n",
       "   -0.02507583051919937,\n",
       "   -0.060750123113393784,\n",
       "   0.0059825945645570755,\n",
       "   0.03484799712896347,\n",
       "   0.02016613818705082,\n",
       "   -0.015531719662249088,\n",
       "   -0.00015078607248142362,\n",
       "   -0.005152090918272734,\n",
       "   -0.0031506770756095648,\n",
       "   -0.014064570888876915,\n",
       "   0.006549949757754803,\n",
       "   0.0002779175410978496,\n",
       "   -0.006690835114568472,\n",
       "   -0.01081139501184225,\n",
       "   -0.00817690510302782,\n",
       "   0.007243886590003967,\n",
       "   0.04229675233364105,\n",
       "   -0.0001843618811108172,\n",
       "   -0.034831516444683075,\n",
       "   -0.04425271600484848,\n",
       "   -0.006685037165880203,\n",
       "   0.0744735598564148,\n",
       "   0.004836583975702524,\n",
       "   0.021075710654258728,\n",
       "   0.044211700558662415,\n",
       "   -0.024377498775720596,\n",
       "   -0.006581888534128666,\n",
       "   0.041655030101537704,\n",
       "   -0.01835458166897297,\n",
       "   0.011481028981506824,\n",
       "   -0.035384587943553925,\n",
       "   -0.0004068802809342742,\n",
       "   0.04845699667930603,\n",
       "   0.03448296710848808,\n",
       "   -0.008542228490114212,\n",
       "   0.011030655354261398,\n",
       "   -0.0022074810694903135,\n",
       "   0.004972105845808983,\n",
       "   0.04454226419329643,\n",
       "   0.010056606493890285,\n",
       "   -0.0010017944732680917,\n",
       "   0.005659114569425583,\n",
       "   -0.048988062888383865,\n",
       "   -0.0067527941428124905,\n",
       "   0.05019921436905861,\n",
       "   0.005931532941758633,\n",
       "   0.0016758200945332646,\n",
       "   0.015412870794534683,\n",
       "   0.06865479052066803,\n",
       "   -0.0025194119662046432,\n",
       "   0.025059832260012627,\n",
       "   -0.013146419078111649,\n",
       "   -0.10052873939275742,\n",
       "   0.040929485112428665,\n",
       "   -0.006380573846399784,\n",
       "   0.00720864487811923,\n",
       "   -0.028925593942403793,\n",
       "   -0.04633978009223938,\n",
       "   0.09066612273454666,\n",
       "   0.01903470605611801,\n",
       "   0.02718217670917511,\n",
       "   -0.011247984133660793,\n",
       "   -0.0634986087679863,\n",
       "   -0.06117052212357521,\n",
       "   -0.016891933977603912,\n",
       "   -0.007235310971736908,\n",
       "   0.09212737530469894,\n",
       "   -0.012094315141439438,\n",
       "   -0.046213556081056595,\n",
       "   0.013417202048003674,\n",
       "   -0.0020726972725242376,\n",
       "   0.07849264144897461,\n",
       "   0.01970549300312996,\n",
       "   -0.02336702309548855,\n",
       "   0.039053477346897125,\n",
       "   -0.03934627026319504,\n",
       "   -0.061508018523454666,\n",
       "   0.002625375986099243,\n",
       "   0.024477604776620865,\n",
       "   -0.008296956308186054,\n",
       "   0.040379490703344345,\n",
       "   0.04605964198708534,\n",
       "   -0.01733355224132538,\n",
       "   -0.012563196010887623,\n",
       "   0.029983583837747574,\n",
       "   -0.013735750690102577,\n",
       "   0.011655177921056747,\n",
       "   0.019124215468764305,\n",
       "   -0.012878126464784145,\n",
       "   -0.032525431364774704,\n",
       "   0.02713564597070217,\n",
       "   -0.0533006452023983,\n",
       "   0.023325785994529724,\n",
       "   -0.013038982637226582,\n",
       "   -0.010891521349549294,\n",
       "   0.007949644699692726,\n",
       "   -0.0525907538831234,\n",
       "   0.11426574736833572,\n",
       "   0.07072925567626953,\n",
       "   -0.051606275141239166,\n",
       "   0.0034940538462251425,\n",
       "   0.024612756446003914,\n",
       "   -0.003385218558833003,\n",
       "   -0.00048082511057145894,\n",
       "   0.005429661832749844,\n",
       "   0.010173211805522442,\n",
       "   -0.009425360709428787,\n",
       "   -0.0042553856037557125,\n",
       "   -0.0130669130012393,\n",
       "   0.004887956660240889,\n",
       "   0.05013923719525337,\n",
       "   -0.0709042102098465,\n",
       "   -0.0007641863194294274,\n",
       "   0.050115250051021576,\n",
       "   -0.0449688546359539,\n",
       "   0.0698724165558815,\n",
       "   0.02160658873617649,\n",
       "   -0.019090579822659492,\n",
       "   0.006541488226503134,\n",
       "   -0.023452913388609886,\n",
       "   -0.06276732683181763,\n",
       "   -0.035876836627721786,\n",
       "   -0.034877751022577286,\n",
       "   -0.024968314915895462,\n",
       "   -0.016403093934059143,\n",
       "   -0.004695059731602669,\n",
       "   -0.0041039553470909595,\n",
       "   -0.04686789959669113,\n",
       "   0.0030368382576853037,\n",
       "   -0.008015505969524384,\n",
       "   0.0505184605717659,\n",
       "   0.048578791320323944,\n",
       "   -0.020891336724162102,\n",
       "   0.015416888520121574,\n",
       "   0.020415326580405235,\n",
       "   -0.02728918381035328,\n",
       "   -0.025241652503609657,\n",
       "   -0.06827818602323532,\n",
       "   -0.03618783876299858,\n",
       "   0.008642042055726051,\n",
       "   0.022264955565333366,\n",
       "   0.013311141170561314,\n",
       "   0.02270345762372017,\n",
       "   0.027420807629823685,\n",
       "   -0.026770444586873055,\n",
       "   0.009257804602384567,\n",
       "   -0.04124610498547554,\n",
       "   0.0035177648533135653,\n",
       "   0.05924464389681816,\n",
       "   0.035295069217681885,\n",
       "   -0.0167391374707222,\n",
       "   -0.03067518025636673,\n",
       "   0.0013720294227823615,\n",
       "   0.04651748389005661,\n",
       "   -0.025873543694615364,\n",
       "   -0.045682623982429504,\n",
       "   0.004498533438891172,\n",
       "   -0.07770375162363052,\n",
       "   0.008687164634466171,\n",
       "   -0.045724138617515564,\n",
       "   -0.03939751908183098,\n",
       "   0.003385154763236642,\n",
       "   -0.018265899270772934,\n",
       "   0.022076740860939026,\n",
       "   -0.015212876722216606,\n",
       "   0.013177050277590752,\n",
       "   0.05377433821558952,\n",
       "   -0.01723376289010048,\n",
       "   0.02495625428855419,\n",
       "   0.0588870532810688,\n",
       "   0.015237534418702126,\n",
       "   -0.017833108082413673,\n",
       "   0.013809560798108578,\n",
       "   0.008144861087203026,\n",
       "   0.004486022517085075,\n",
       "   0.009784952737390995,\n",
       "   -0.028346896171569824,\n",
       "   0.0037746336311101913,\n",
       "   -0.030834253877401352,\n",
       "   -0.07332493364810944,\n",
       "   -0.255465567111969,\n",
       "   -0.0050636171363294125,\n",
       "   -0.00013504734670277685,\n",
       "   -0.050382986664772034,\n",
       "   0.08713319897651672,\n",
       "   0.02320169284939766,\n",
       "   0.01879926212131977,\n",
       "   -0.05093234032392502,\n",
       "   -0.016530552878975868,\n",
       "   0.03340296819806099,\n",
       "   -0.01285319309681654,\n",
       "   0.020986715331673622,\n",
       "   0.005487397778779268,\n",
       "   0.05486898124217987,\n",
       "   0.020562078803777695,\n",
       "   -0.005269928835332394,\n",
       "   -0.011682135052978992,\n",
       "   -0.009233287535607815,\n",
       "   0.010681270621716976,\n",
       "   0.03592003136873245,\n",
       "   -0.0005000439705327153,\n",
       "   -0.06974316388368607,\n",
       "   -0.0049618883058428764,\n",
       "   0.019344225525856018,\n",
       "   0.0027127054054290056,\n",
       "   0.04636145010590553,\n",
       "   -0.05513792484998703,\n",
       "   0.011354834772646427,\n",
       "   -0.05938330665230751,\n",
       "   -0.01963472180068493,\n",
       "   -0.006274169776588678,\n",
       "   -0.021806200966238976,\n",
       "   0.017971761524677277,\n",
       "   -0.010202990844845772,\n",
       "   0.00947303231805563,\n",
       "   -0.02412833832204342,\n",
       "   0.030338818207383156,\n",
       "   0.0010021358029916883,\n",
       "   0.008103817701339722,\n",
       "   -0.02692382037639618,\n",
       "   -0.022306708618998528,\n",
       "   -0.02626735158264637,\n",
       "   -0.039691559970378876,\n",
       "   -0.005534673109650612,\n",
       "   0.05672219395637512,\n",
       "   -0.00815504789352417,\n",
       "   -0.03569212183356285,\n",
       "   -0.01846843771636486,\n",
       "   -0.04561406373977661,\n",
       "   0.06639180332422256,\n",
       "   0.019327029585838318,\n",
       "   -0.005322842393070459,\n",
       "   0.012567572295665741,\n",
       "   0.03333701938390732,\n",
       "   -0.034578464925289154,\n",
       "   -0.0430406853556633,\n",
       "   0.028421668335795403,\n",
       "   0.02254907600581646,\n",
       "   -0.009136504493653774,\n",
       "   -0.01192398089915514,\n",
       "   0.010016064159572124,\n",
       "   -0.04353942349553108,\n",
       "   -0.038015540689229965,\n",
       "   -0.027288198471069336,\n",
       "   -0.006745761260390282,\n",
       "   -0.0630478709936142,\n",
       "   -0.03821586072444916,\n",
       "   -0.060198742896318436,\n",
       "   0.05160883814096451,\n",
       "   0.03830566629767418,\n",
       "   -0.03971285745501518,\n",
       "   -0.01689722016453743,\n",
       "   -0.03255763649940491,\n",
       "   -0.07421503216028214,\n",
       "   -0.009669745340943336,\n",
       "   -0.038515958935022354,\n",
       "   -0.007685419637709856,\n",
       "   -0.017364710569381714,\n",
       "   -0.002548441756516695,\n",
       "   0.01617910899221897,\n",
       "   -0.04437381774187088,\n",
       "   -0.032635241746902466,\n",
       "   0.03604559600353241,\n",
       "   0.03767376393079758,\n",
       "   0.0062651378102600574,\n",
       "   -0.05166175588965416,\n",
       "   -0.008879516273736954,\n",
       "   -0.03783949464559555,\n",
       "   -0.02186572551727295,\n",
       "   -0.054571766406297684,\n",
       "   0.06331076472997665,\n",
       "   -0.05448701232671738,\n",
       "   -0.0012388628674671054,\n",
       "   -0.001295784255489707,\n",
       "   0.04004346579313278,\n",
       "   0.007310531567782164,\n",
       "   0.020994240418076515,\n",
       "   -0.016561167314648628,\n",
       "   0.011609269306063652,\n",
       "   0.01827763020992279,\n",
       "   0.05713409185409546,\n",
       "   -0.07919694483280182,\n",
       "   -0.01035190187394619,\n",
       "   -0.029354939237236977,\n",
       "   -0.02523103356361389,\n",
       "   -0.006825807970017195,\n",
       "   -0.07720207422971725,\n",
       "   0.0029902991373091936,\n",
       "   0.03478039428591728,\n",
       "   -0.0027631039265543222,\n",
       "   0.029745040461421013,\n",
       "   -0.015126886777579784,\n",
       "   0.04015259072184563,\n",
       "   -0.046979743987321854,\n",
       "   -0.002069754060357809,\n",
       "   -0.04721011593937874,\n",
       "   0.05187717080116272,\n",
       "   0.030056796967983246,\n",
       "   0.006137298885732889,\n",
       "   -0.008417265489697456,\n",
       "   -0.029569977894425392,\n",
       "   0.051384180784225464,\n",
       "   -0.08980463445186615,\n",
       "   -0.01961468532681465,\n",
       "   -0.07429180294275284,\n",
       "   -0.03078598715364933,\n",
       "   -0.017260044813156128,\n",
       "   0.004853073973208666,\n",
       "   0.006769784260541201,\n",
       "   -0.0021599759347736835,\n",
       "   0.008588121272623539,\n",
       "   0.017570747062563896,\n",
       "   0.027171628549695015,\n",
       "   0.0058860634453594685,\n",
       "   0.004535316489636898,\n",
       "   0.011301378719508648,\n",
       "   -0.006407694425433874,\n",
       "   -0.033173784613609314,\n",
       "   -0.0006486641359515488,\n",
       "   -0.03440578654408455,\n",
       "   0.016638675704598427,\n",
       "   -0.009841266088187695,\n",
       "   0.02170134335756302,\n",
       "   0.0078969681635499,\n",
       "   0.02378387376666069,\n",
       "   0.03201421722769737,\n",
       "   0.01223149336874485,\n",
       "   0.01051829569041729,\n",
       "   -0.02166878432035446,\n",
       "   0.006513160187751055,\n",
       "   0.029721323400735855,\n",
       "   -0.0456865020096302,\n",
       "   0.0236471239477396,\n",
       "   -0.03906579315662384,\n",
       "   -0.035987500101327896,\n",
       "   -0.03618721291422844,\n",
       "   0.05562267079949379,\n",
       "   -0.003425936447456479,\n",
       "   0.02224167063832283,\n",
       "   -0.030127691105008125,\n",
       "   0.004664915148168802,\n",
       "   -0.07464947551488876,\n",
       "   -0.010984805412590504,\n",
       "   -0.02231065183877945,\n",
       "   -0.007250359747558832,\n",
       "   0.06946773082017899,\n",
       "   0.012221169658005238,\n",
       "   -0.007215634919703007,\n",
       "   -0.012006187811493874,\n",
       "   0.026706775650382042,\n",
       "   -0.008571523241698742,\n",
       "   -0.03548523038625717,\n",
       "   -0.0047775194980204105,\n",
       "   0.01754777505993843,\n",
       "   -0.010840429924428463,\n",
       "   0.028112290427088737,\n",
       "   8.049832104006782e-05,\n",
       "   -0.003592358436435461,\n",
       "   0.0186273455619812,\n",
       "   0.03291208669543266,\n",
       "   0.016118502244353294,\n",
       "   0.003565211547538638,\n",
       "   -0.014883709140121937,\n",
       "   0.018940968438982964,\n",
       "   0.07986192405223846,\n",
       "   -0.040600769221782684,\n",
       "   -0.03388745337724686,\n",
       "   -0.049844734370708466,\n",
       "   0.007785969879478216,\n",
       "   0.0057970681227743626,\n",
       "   0.018851621076464653,\n",
       "   0.031036751344799995,\n",
       "   -0.013546356000006199,\n",
       "   -0.016193892806768417,\n",
       "   -0.052029356360435486,\n",
       "   -0.07243775576353073,\n",
       "   -0.007663033902645111,\n",
       "   0.05966705456376076,\n",
       "   0.0063241105526685715,\n",
       "   0.009762833826243877,\n",
       "   0.007074866443872452,\n",
       "   0.022578192874789238,\n",
       "   0.0035433864686638117,\n",
       "   -0.03877907991409302,\n",
       "   0.048253122717142105,\n",
       "   -0.07141374051570892,\n",
       "   0.026940148323774338,\n",
       "   0.007063599769026041,\n",
       "   -0.02747626043856144,\n",
       "   -0.0010274284286424518,\n",
       "   -0.0037467465735971928,\n",
       "   -0.04195184260606766,\n",
       "   -0.010010790079832077,\n",
       "   -0.004368448629975319,\n",
       "   -0.024477940052747726,\n",
       "   -0.07869827747344971,\n",
       "   -0.014458153396844864,\n",
       "   -0.012033993378281593,\n",
       "   0.035773687064647675,\n",
       "   -0.04614992067217827,\n",
       "   -0.004372635390609503,\n",
       "   -0.013252614997327328,\n",
       "   0.017381519079208374,\n",
       "   0.001372858532704413,\n",
       "   -0.03168217092752457,\n",
       "   0.01545304898172617,\n",
       "   -0.016396716237068176,\n",
       "   0.008641812950372696,\n",
       "   0.02238485962152481,\n",
       "   -0.02515519969165325,\n",
       "   0.03345654904842377,\n",
       "   -0.05793384090065956,\n",
       "   0.026662016287446022,\n",
       "   0.08027160167694092,\n",
       "   -0.032769281417131424,\n",
       "   -0.003698150161653757,\n",
       "   -0.0164411012083292,\n",
       "   0.01974734477698803,\n",
       "   0.0339270755648613,\n",
       "   0.025102879852056503,\n",
       "   0.01567184552550316,\n",
       "   -0.00853898748755455,\n",
       "   -0.04256215691566467,\n",
       "   -0.03698403760790825,\n",
       "   0.009965682402253151,\n",
       "   -0.0010706038447096944,\n",
       "   -0.016519974917173386,\n",
       "   -0.0163558479398489,\n",
       "   -0.018243936821818352,\n",
       "   0.07120157778263092,\n",
       "   -0.010444923304021358,\n",
       "   0.006127802189439535,\n",
       "   -0.024128155782818794,\n",
       "   -0.05092844367027283,\n",
       "   0.04482026398181915,\n",
       "   -0.05101660639047623,\n",
       "   -0.043440546840429306,\n",
       "   -0.0022303536534309387,\n",
       "   0.0021040807478129864,\n",
       "   0.03999096527695656,\n",
       "   0.025669271126389503,\n",
       "   0.011868416331708431,\n",
       "   -0.02628035470843315,\n",
       "   -0.006560732610523701,\n",
       "   0.0412275530397892,\n",
       "   0.005992512684315443,\n",
       "   -0.04080161824822426,\n",
       "   -0.03364299610257149,\n",
       "   0.025759747251868248,\n",
       "   -0.030129190534353256,\n",
       "   0.019845876842737198,\n",
       "   -0.07855642586946487,\n",
       "   0.019400741904973984,\n",
       "   -0.0024328352883458138,\n",
       "   0.0012739256490021944,\n",
       "   -0.01247438881546259,\n",
       "   -0.021002179011702538,\n",
       "   -0.01779724843800068,\n",
       "   0.02309924177825451,\n",
       "   -0.034356337040662766,\n",
       "   -0.046976491808891296,\n",
       "   0.05178355053067207,\n",
       "   -0.010606630705296993,\n",
       "   -0.003669256577268243,\n",
       "   0.04134446009993553,\n",
       "   -0.06508567184209824,\n",
       "   0.012609656900167465,\n",
       "   0.00022629280283581465,\n",
       "   -0.04305971413850784,\n",
       "   0.005140031687915325,\n",
       "   0.014652577228844166,\n",
       "   0.06623618304729462,\n",
       "   -0.007881083525717258,\n",
       "   0.015584563836455345,\n",
       "   -0.0055386717431247234,\n",
       "   0.011242556385695934,\n",
       "   0.023053018376231194,\n",
       "   0.07429587095975876,\n",
       "   -0.008914963342249393,\n",
       "   0.036951273679733276,\n",
       "   -0.05922260880470276,\n",
       "   0.057433001697063446,\n",
       "   0.06839826703071594,\n",
       "   -0.020561253651976585,\n",
       "   -0.01668848656117916,\n",
       "   0.013808193616569042,\n",
       "   0.02530485764145851,\n",
       "   -0.04938120394945145,\n",
       "   -0.0019957844633609056,\n",
       "   0.042601577937603,\n",
       "   0.012331699952483177,\n",
       "   -0.04563155770301819,\n",
       "   0.053991638123989105,\n",
       "   0.03681417182087898,\n",
       "   -0.04264022409915924,\n",
       "   0.0056575289927423,\n",
       "   0.02027289941906929,\n",
       "   -0.057202305644750595,\n",
       "   -0.015013099648058414,\n",
       "   0.009964309632778168,\n",
       "   0.03850524500012398,\n",
       "   -0.037920307368040085,\n",
       "   0.05942792072892189,\n",
       "   0.056135013699531555,\n",
       "   0.0075446441769599915,\n",
       "   0.0628824234008789,\n",
       "   -0.04385068267583847,\n",
       "   0.029883302748203278,\n",
       "   0.02025170624256134,\n",
       "   0.04640636965632439,\n",
       "   0.05458913370966911,\n",
       "   0.0026176818646490574,\n",
       "   0.008193657733500004,\n",
       "   0.06981911510229111,\n",
       "   -0.0018135320860892534,\n",
       "   -0.07125028967857361,\n",
       "   0.03180023655295372,\n",
       "   0.03331734985113144,\n",
       "   -0.004297605715692043,\n",
       "   0.03086072951555252,\n",
       "   -0.011952696368098259,\n",
       "   0.054467104375362396,\n",
       "   0.034926559776067734,\n",
       "   0.05266745761036873,\n",
       "   0.03487245365977287,\n",
       "   0.015790406614542007,\n",
       "   0.016973411664366722,\n",
       "   -0.009693610481917858,\n",
       "   0.023243781179189682,\n",
       "   0.0058201937936246395,\n",
       "   0.03624658286571503,\n",
       "   0.009860311634838581,\n",
       "   -0.02461194433271885,\n",
       "   -0.05886823311448097,\n",
       "   0.037570688873529434,\n",
       "   0.011213140562176704,\n",
       "   -0.0127267399802804,\n",
       "   -0.015807969495654106,\n",
       "   -0.04804886132478714,\n",
       "   0.002800749847665429,\n",
       "   -0.034236595034599304,\n",
       "   -0.013551627285778522,\n",
       "   0.08845528960227966,\n",
       "   0.02504913881421089,\n",
       "   -0.043030302971601486,\n",
       "   0.0006428362685255706,\n",
       "   0.04574371874332428,\n",
       "   -0.017918771132826805,\n",
       "   0.013595817610621452,\n",
       "   -0.028060415759682655,\n",
       "   0.004090143833309412,\n",
       "   -0.004836526699364185,\n",
       "   0.0031654820777475834,\n",
       "   0.023560410365462303,\n",
       "   -0.0161840058863163,\n",
       "   0.02095567062497139,\n",
       "   0.041158512234687805,\n",
       "   0.007646515965461731,\n",
       "   0.011963287368416786,\n",
       "   0.027177372947335243,\n",
       "   0.04304363951086998,\n",
       "   -0.028532758355140686,\n",
       "   -0.07348530739545822,\n",
       "   -0.07522999495267868,\n",
       "   -0.026229584589600563,\n",
       "   -0.05644895136356354,\n",
       "   0.010896188206970692,\n",
       "   0.0355607271194458,\n",
       "   -0.020583290606737137,\n",
       "   -0.03366444259881973,\n",
       "   -0.01096569374203682,\n",
       "   -0.038661517202854156,\n",
       "   0.025282029062509537,\n",
       "   -0.004624227061867714,\n",
       "   -0.06405976414680481,\n",
       "   -0.0272865891456604,\n",
       "   0.013656587339937687,\n",
       "   0.02123495750129223,\n",
       "   0.03508128970861435,\n",
       "   0.024115027859807014,\n",
       "   0.07309431582689285,\n",
       "   -0.007023971527814865,\n",
       "   -0.02189263142645359,\n",
       "   0.0071328869089484215,\n",
       "   0.0044199931435287,\n",
       "   0.03731822967529297,\n",
       "   -0.0038280817680060863,\n",
       "   0.03678692877292633,\n",
       "   -0.08664587140083313,\n",
       "   0.049321115016937256,\n",
       "   0.026077207177877426,\n",
       "   0.015047593973577023,\n",
       "   -0.08157666027545929,\n",
       "   0.035536084324121475,\n",
       "   -0.02091999351978302,\n",
       "   -0.022796642035245895,\n",
       "   0.07189715653657913,\n",
       "   0.015972834080457687,\n",
       "   0.006498204078525305,\n",
       "   -0.04886772856116295,\n",
       "   -0.012509828433394432,\n",
       "   -0.001647296128794551,\n",
       "   0.013409710489213467,\n",
       "   0.059494972229003906,\n",
       "   -0.06398051977157593,\n",
       "   0.05356115475296974,\n",
       "   0.042179498821496964,\n",
       "   0.038380254060029984,\n",
       "   -0.04226621612906456,\n",
       "   0.03669295832514763,\n",
       "   0.008923842571675777,\n",
       "   0.010288434103131294,\n",
       "   -0.0366658940911293,\n",
       "   -0.005945742130279541,\n",
       "   -0.03129289299249649,\n",
       "   -0.08502526581287384,\n",
       "   -0.006765337660908699,\n",
       "   0.026763582602143288,\n",
       "   -0.008717427030205727,\n",
       "   -0.06011757627129555,\n",
       "   -0.017247166484594345,\n",
       "   3.655358887044713e-05,\n",
       "   0.006092320661991835,\n",
       "   0.0019891574047505856,\n",
       "   -0.002971830079331994,\n",
       "   -0.019582150503993034,\n",
       "   -0.02312145195901394,\n",
       "   -0.010916602797806263,\n",
       "   -0.05457712337374687,\n",
       "   0.014068362303078175,\n",
       "   0.023388667032122612,\n",
       "   0.010142160579562187,\n",
       "   -0.008812053129076958,\n",
       "   -0.04258941859006882,\n",
       "   0.03422655910253525,\n",
       "   -0.0613219253718853,\n",
       "   -0.0013970925938338041,\n",
       "   0.04282331466674805,\n",
       "   -0.021469349041581154,\n",
       "   -0.0009407760226167738]}]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chunks.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09187588-e1dc-44c5-b88b-cc8ebe3f9c48",
   "metadata": {},
   "source": [
    "## Index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bb6bf17c-da04-4bd8-af2f-dc79e4e27913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pgvector.psycopg import register_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9eed21d9-4309-4fa1-bc21-51fd35b33e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dbname=postgres user=postgres host=localhost password=postgres'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"DB_CONNECTION_STRING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "031a6487-c757-46bc-bc20-1e6a135fee6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StoreResults:\n",
    "    def __call__(self, batch):\n",
    "        with psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"]) as conn:\n",
    "            register_vector(conn)\n",
    "            with conn.cursor() as cur:\n",
    "                for text, source, embedding in zip(batch[\"text\"], batch[\"source\"], batch[\"embeddings\"]):\n",
    "                    cur.execute(\"INSERT INTO document (text, source, embedding) VALUES (%s, %s, %s)\", (text, source, embedding,),)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c7ae20ca-1371-48d6-bc84-6d11a425c3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set up pgvector\n",
    "# bash ../setup-pgvector.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "806b47aa-f3c1-44d3-b041-a3a4f0867653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE\n",
      "CREATE TABLE\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Drop current vector DB (if any)\n",
    "sudo -u postgres psql \"$DB_CONNECTION_STRING\" -c \"DROP TABLE document;\"\n",
    "sudo -u postgres psql -f ../migrations/vector-768.sql  # \"thenlper/gte-base\" dimensino is 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9c4582d9-40ba-4a94-81ac-259b3851f837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 16:29:24,665\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(EmbedChunks)] -> ActorPoolMapOperator[MapBatches(StoreResults)]\n",
      "2023-08-29 16:29:24,666\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-29 16:29:24,666\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-08-29 16:29:24,677\tINFO actor_pool_map_operator.py:117 -- MapBatches(EmbedChunks): Waiting for 1 pool actors to start...\n",
      "2023-08-29 16:29:32,299\tINFO actor_pool_map_operator.py:117 -- MapBatches(StoreResults): Waiting for 15 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 16:31:17,630\tWARNING actor_pool_map_operator.py:267 -- To ensure full parallelization across an actor pool of size 15, the specified batch size should be at most 0. Your configured batch size for this operator was 128.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index data\n",
    "embedded_chunks.map_batches(\n",
    "    StoreResults,\n",
    "    batch_size=128,\n",
    "    num_cpus=1,\n",
    "    compute=ActorPoolStrategy(size=15),\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "04fbfb53-f121-435b-8e26-77715d5ffe38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " count \n",
      "-------\n",
      " 61882\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check number of rows\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"SELECT count(*) FROM document;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "40925cd6-41e8-4651-9692-aeb399b68af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h8m48s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h8m52s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h8m57s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m2s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m7s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m12s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m17s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m22s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m27s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m32s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m38s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m43s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m48s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m53s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h9m58s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h10m3s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(autoscaler +5h10m12s)\u001b[0m Resized to 16 CPUs, 1 GPUs.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Save index\n",
    "export SQL_DUMP_FP=\"/efs/shared_storage/simon/sql_dumps/gte-base_300_50.sql\"\n",
    "sudo -u postgres pg_dump -c > $SQL_DUMP_FP  # save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318798c3-d119-4eb5-ad81-b2834516151a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "480d4c49-5870-471e-a617-86f7d3fa13d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4827efb7-487b-4368-8880-c7480511c8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "conn = psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"])\n",
    "register_vector(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "39c3f410-89a2-4992-8cd1-63aca5bf9936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed query\n",
    "query = \"What is the default batch size for map_batches?\"\n",
    "embedding = np.array(embedding_model.embed_query(query))\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cc9b25ed-fa16-48df-9e1b-a94c33891a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT * FROM document ORDER BY embedding <-> %s LIMIT 5\", (embedding,))\n",
    "    rows = cur.fetchall()\n",
    "    context = [{\"text\": row[1], \"source\": row[2]} for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b0197e98-5fee-4be0-bdf5-2d975216aa4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.ray.io/en/master/data/api/doc/ray.data.Dataset.map_batches.html#ray-data-dataset-map-batches\n",
      "entire blocks as batches (blocks may contain different numbers of rows).\n",
      "The actual size of the batch provided to fn may be smaller than\n",
      "batch_size if batch_size doesn’t evenly divide the block(s) sent\n",
      "to a given map task. Default batch_size is 4096 with “default”.\n",
      "\n",
      "https://docs.ray.io/en/master/_modules/ray/data/dataset.html\n",
      "``batch_size`` if ``batch_size`` doesn't evenly divide the block(s) sent\n",
      "                to a given map task. Default batch_size is 4096 with \"default\".\n",
      "            compute: Either \"tasks\" (default) to use Ray Tasks or an\n",
      "\n",
      "https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-size\n",
      "Note\n",
      "The default batch size depends on your resource type. If you’re using CPUs,\n",
      "the default batch size is 4096. If you’re using GPUs, you must specify an explicit\n",
      "batch size.\n",
      "\n",
      "https://docs.ray.io/en/master/_modules/ray/data/context.html\n",
      "# Default batch size for batch transformations.\n",
      "DEFAULT_BATCH_SIZE = 4096\n",
      "\n",
      "# Default batch size for batch transformations in strict mode.\n",
      "STRICT_MODE_DEFAULT_BATCH_SIZE = 1024\n",
      "\n",
      "https://docs.ray.io/en/master/data/batch_inference.html#configuring-batch-size\n",
      "Configuring Batch Size#\n",
      "Configure the size of the input batch that’s passed to __call__ by setting the batch_size argument for ds.map_batches()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in context:\n",
    "    print (item[\"source\"])\n",
    "    print (item[\"text\"])\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5738d23-91e3-4016-826e-716872f76b62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b55cc1d7-e110-4d9d-abc4-36576db25f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "697e65e8-4d69-4870-9f09-0e128008b94e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    llm, temperature=0.0, \n",
    "    system_content=\"\", assistant_content=\"\", user_content=\"\", \n",
    "    max_retries=3, retry_interval=60):\n",
    "    \"\"\"Generate response from an LLM.\"\"\"\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=llm,\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "                    {\"role\": \"user\", \"content\": user_content},\n",
    "                ],\n",
    "            )\n",
    "            return response[\"choices\"][-1][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(retry_interval)  # default is per-minute rate limits\n",
    "            retry_count += 1\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b76e2b44-dfdf-4f98-ba01-dda13e7c1677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credentials\n",
    "openai.api_base = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "openai.api_key = os.environ[\"ANYSCALE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9de3685b-6839-445f-9baa-68a5e863562a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The default batch size for map_batches is 4096. This is mentioned in multiple sources, including the Ray documentation for Dataset.map_batches, which states that the default batch size is 4096 with the \"default\" argument. Additionally, the Ray documentation for configuring batch size mentions that the default batch size for batch transformations is also 4096.'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate response\n",
    "generate_response(\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    temperature=0.0,\n",
    "    system_content=\"Answer the {query} using the provided {context}\",\n",
    "    user_content=f\"query: {query}, context: {context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b39d3-83f9-497d-960d-8756068af7f2",
   "metadata": {},
   "source": [
    "### Query Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad51191-19d1-40d5-bbc8-72245fafd154",
   "metadata": {},
   "source": [
    "Let's combine the context retrieval and response generation together into a conventient query agent that we can use to easily generate our responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ffcc892a-ceee-487a-aecf-db116f53e89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QueryAgent:\n",
    "    def __init__(self, embedding_model_name=\"thenlper/gte-base\",\n",
    "                 llm=\"meta-llama/Llama-2-70b-chat-hf\", \n",
    "                 temperature=0.0, max_context_length=4096,\n",
    "                 system_content=\"\", assistant_content=\"\"):\n",
    "        \n",
    "        # Embedding model\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "        encode_kwargs = {\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        if embedding_model_name == \"text-embedding-ada-002\":\n",
    "            self.embedding_model = OpenAIEmbeddings(\n",
    "                model=embedding_model_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                encode_kwargs=encode_kwargs,\n",
    "                openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        else:\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=embedding_model_name,\n",
    "                model_kwargs=model_kwargs,\n",
    "                encode_kwargs=encode_kwargs)\n",
    "        \n",
    "        # LLM\n",
    "        self.llm = llm\n",
    "        self.temperature = temperature\n",
    "        self.context_length = max_context_length - len(system_content + assistant_content)\n",
    "        self.system_content = system_content\n",
    "        self.assistant_content = assistant_content\n",
    "\n",
    "        # VectorDB connection\n",
    "        self.conn = psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"])\n",
    "        register_vector(self.conn)\n",
    "\n",
    "    def __call__(self, query, num_chunks=5):\n",
    "        # Get context\n",
    "        embedding = np.array(self.embedding_model.embed_query(query))\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT * FROM document ORDER BY embedding <-> %s LIMIT %s\", (embedding, num_chunks))\n",
    "            rows = cur.fetchall()\n",
    "            context = [{\"text\": row[1]} for row in rows]\n",
    "            sources = [row[2] for row in rows]\n",
    "\n",
    "        # Generate response\n",
    "        user_content = f\"query: {query}, context: {context}\"\n",
    "        answer = generate_response(\n",
    "            llm=self.llm,\n",
    "            temperature=self.temperature,\n",
    "            system_content=self.system_content,\n",
    "            assistant_content=self.assistant_content,\n",
    "            user_content=user_content[: self.context_length],\n",
    "        )\n",
    "\n",
    "        # Result\n",
    "        result = {\n",
    "            \"question\": query,\n",
    "            \"sources\": sources,\n",
    "            \"answer\": answer,\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ad3b1224-922b-40b5-9979-9075c6ef100e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"What is the default batch size for map_batches?\",\n",
      "  \"sources\": [\n",
      "    \"https://docs.ray.io/en/master/data/api/doc/ray.data.Dataset.map_batches.html#ray-data-dataset-map-batches\",\n",
      "    \"https://docs.ray.io/en/master/_modules/ray/data/dataset.html\",\n",
      "    \"https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-size\",\n",
      "    \"https://docs.ray.io/en/master/_modules/ray/data/context.html\",\n",
      "    \"https://docs.ray.io/en/master/data/batch_inference.html#configuring-batch-size\"\n",
      "  ],\n",
      "  \"answer\": \"The default batch size for `map_batches` is 4096, as specified in the context provided. However, this default batch size can be adjusted by specifying a different value for the `batch_size` argument when calling `map_batches`. Additionally, the default batch size may vary depending on the resource type being used, with CPUs using a default batch size of 4096 and GPUs requiring an explicit batch size to be specified.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the default batch size for map_batches?\"\n",
    "system_content = \"Your job is to answer a question using the additional context provided.\"\n",
    "agent = QueryAgent(\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    max_context_length=4096,\n",
    "    system_content=system_content,\n",
    ")\n",
    "result = agent(query=query)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7a068-fc77-4928-bf58-52321616f9d5",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03acb7e0-7bcb-4e2f-8552-5fe182a278db",
   "metadata": {},
   "source": [
    "We'll start by creating our reference (ground-truth) dataset. We have a list of user queries and the ideal source to answer the query [`datasets/eval-dataset-v1.jsonl`](https://github.com/ray-project/llm-applications/blob/main/datasets/eval-dataset-v1.jsonl). We will our LLM app above to generate reference answer for each query/source pair using `gpt-4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fa6cab00-d68d-4745-a735-048d72d8e5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ca019cc9-3241-4453-b63d-ef7c44ba584a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(Path(ROOT_DIR, \"datasets/eval-dataset-v1.jsonl\"), \"r\") as f:\n",
    "    data = [json.loads(item) for item in list(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "716b02e5-1ab5-4d19-aaea-cc89d28de508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "for row in data:\n",
    "    row[\"source\"] = row[\"source\"].replace(\"https://docs.ray.io/en/latest/\", \"https://docs.ray.io/en/master/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3cd9e509-1fe6-42ad-886f-0e118e8dd35d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'I’m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       "  'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format'},\n",
       " {'question': 'How does autoscaling work in a Ray Serve application?',\n",
       "  'source': 'https://docs.ray.io/en/master/serve/scaling-and-resource-allocation.html#autoscaling'},\n",
       " {'question': 'how do I get the address of a ray node',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-core/miscellaneous.html#node-information'},\n",
       " {'question': 'Does Ray support NCCL?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-more-libs/ray-collective.html'},\n",
       " {'question': 'could you give me an example of using this library for data-parallel training of CNNs on Ray?',\n",
       "  'source': 'https://docs.ray.io/en/master/ray-air/computer-vision.html#training-vision-models'}]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7e1eb37d-38d2-45dd-8e57-789fc20bb9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Section per document (page) dict\n",
    "sections_per_doc = {section[\"source\"]: section[\"text\"] for section in sections}\n",
    "for section in sections:\n",
    "    page = section[\"source\"]\n",
    "    if \"#\" not in page:\n",
    "        page_sections = [key for key in sections_per_doc.keys() if key.startswith(page)]\n",
    "        combined_text = \"\\n\".join(sections_per_doc[page_section] for page_section in page_sections)\n",
    "        sections_per_doc[page] = combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1c9a74a6-501e-4024-8faf-5776b323691f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\nKey Concepts of Ray Train#\\nThere are three main concepts in the Ray Train library.\\n\\nTrainers execute distributed training.\\nConfiguration objects are used to configure training.\\nCheckpoints are returned as the result of training.\\n\\n\\n\\nTrainers#\\nTrainers are responsible for executing (distributed) training runs.\\nThe output of a Trainer run is a Result that contains\\nmetrics from the training run and the latest saved Checkpoint.\\nYou can also configured trainers with Datasets and Preprocessors for scalable data ingest and preprocessing.\\n\\n\\nDeep Learning, Tree-Based, and other Trainers#\\nThere are two categories of built-in Trainers:\\n\\n\\n\\nDeep Learning Trainers\\nRay Train supports the following deep learning trainers:\\n\\nTorchTrainer\\nTensorflowTrainer\\nHorovodTrainer\\n\\nFor these trainers, you usually define your own training function that loads the model\\nand executes single-worker training steps. Refer to the following guides for more details:\\n\\nDistributed PyTorch\\nDistributed TensorFlow\\nHorovod\\n\\n\\n\\n\\nTree-Based Trainers\\nTree-based trainers utilize gradient-based decision trees for training. The most popular libraries\\nfor this are XGBoost and LightGBM.\\n\\nXGBoostTrainer\\nLightGBMTrainer\\n\\nFor these trainers, you just pass a dataset and parameters. The training loop is configured\\nautomatically.\\n\\nDistributed XGBoost/LightGBM\\n\\n\\n\\n\\n\\n\\n\\nTrain Configuration#\\nTrainers are configured with configuration objects. There are two main configuration classes,\\nthe ScalingConfig and the RunConfig.\\nThe latter contains subconfigurations, such as the FailureConfig,\\nSyncConfig and CheckpointConfig.\\n\\n\\n\\nTrain Checkpoints#\\nCalling Trainer.fit() returns a Result object, which includes\\ninformation about the run such as the reported metrics and the saved checkpoints.\\nCheckpoints have the following purposes:\\n\\nThey can be passed to a Trainer to resume training from the given model state.\\nThey can be used with Ray Data for scalable batch prediction.\\nThey can be deployed with Ray Serve.\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections_per_doc['https://docs.ray.io/en/master/train/key-concepts.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "63f8d062-abac-4e66-bc66-ecb1789370f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 https://docs.ray.io/en/master/ray-air/computer-vision.html#training-vision-models\n",
      "5 https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html#fine-tuning-the-model-with-ray-air-a-name-train-a\n",
      "37 https://docs.ray.io/en/master/tune/api/doc/ray.air.Result.html#ray-air-result\n",
      "55 https://docs.ray.io/en/master/ray-air/api/doc/ray.air.integrations.wandb.WandbLoggerCallback.html\n",
      "65 https://docs.ray.io/en/master/cluster/kubernetes/getting-started.html#deploying-the-kuberay-operator\n",
      "172 https://docs.ray.io/en/master/cluster/kubernetes/user-guides/experimental.html#rayjobs\n"
     ]
    }
   ],
   "source": [
    "# Checking if any sources are not in our parsed sources\n",
    "for i, row in enumerate(data):\n",
    "    if row[\"source\"].startswith(\"https://docs.ray.io\"):\n",
    "        if row[\"source\"] not in sections_per_doc:\n",
    "            print(i, row[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "dec3460f-c07a-4a2f-95f7-d1ef85d1a064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Content for inference\n",
    "system_content = \"\"\"\n",
    "    Your job is {answer} a {query} using the additional {context} provided.\n",
    "    Then, you must {score} your response between 1 and 5.\n",
    "    You must return your response in a line with only the score.\n",
    "    Do not add any more details.\n",
    "    On a separate line provide your {reasoning} for the score as well.\n",
    "    Return your response following the exact format outlined below.\n",
    "    Do not add or remove anything.\n",
    "    And all of this must be in a valid JSON format.\n",
    "    \n",
    "    {\"answer\": answer,\n",
    "     \"score\": score,\n",
    "     \"reasoning\": reasoning}\n",
    "    \"\"\"\n",
    "assistant_content = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8a67f8a6-c742-4c40-b8e5-d2c5fb70fec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_from_response(response):\n",
    "    # Define regular expressions for extracting values\n",
    "    answer_pattern = r'\"answer\"\\s*:\\s*\"([^\"]*)\"'\n",
    "    score_pattern = r'\"score\"\\s*:\\s*([0-9]+)'\n",
    "    reasoning_pattern = r'\"reasoning\"\\s*:\\s*\"([^\"]*)\"'\n",
    "\n",
    "    # Extract values using regular expressions\n",
    "    answer_match = re.search(answer_pattern, response)\n",
    "    score_match = re.search(score_pattern, response)\n",
    "    reasoning_match = re.search(reasoning_pattern, response)\n",
    "\n",
    "    # Convert\n",
    "    if answer_match and score_match and reasoning_match:\n",
    "        answer = answer_match.group(1)\n",
    "        score = float(score_match.group(1))\n",
    "        reasoning = reasoning_match.group(1)\n",
    "        return answer, score, reasoning\n",
    "\n",
    "    return \"\", \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "63c2db65-36fc-48cb-8e86-18b96554f977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_references(data, sections_per_doc, llm, temperature, max_context_length, system_content, assistant_content):\n",
    "    results = []\n",
    "    for row in tqdm(data):\n",
    "        # Get context\n",
    "        query = row[\"question\"]\n",
    "        context = sections_per_doc.get(row[\"source\"], \"\")\n",
    "\n",
    "        # Generate response\n",
    "        context_length = max_context_length - len(system_content + assistant_content)\n",
    "        user_content = f\"The query is {query} and the additional context is {context}\"[:context_length]\n",
    "        response = generate_response(\n",
    "            llm=llm,\n",
    "            temperature=temperature,\n",
    "            system_content=system_content, \n",
    "            assistant_content=assistant_content, \n",
    "            user_content=user_content)\n",
    "\n",
    "        # Extract from response\n",
    "        answer, score, reasoning = extract_from_response(response=response)\n",
    "\n",
    "        # Store result\n",
    "        result = ({\n",
    "                \"question\": query,\n",
    "                \"source\": row[\"source\"],\n",
    "                \"answer\": answer,\n",
    "                \"score\": score,\n",
    "                \"reasoning\": reasoning,\n",
    "            })\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd2ab6-1cf5-4a81-a2ac-42d8fbca7417",
   "metadata": {},
   "source": [
    "### gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "73f83147-be5a-447e-9237-f1c81199d7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "25cb4b98-ddc7-462e-8d6f-92f703bfc838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/179 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = \"gpt-4\"\n",
    "max_context_length = 8192\n",
    "results = get_references(\n",
    "    data=data, sections_per_doc=sections_per_doc, \n",
    "    llm=llm, temperature=0.0, max_context_length=max_context_length, \n",
    "    system_content=system_content, assistant_content=assistant_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4447323f-6e5e-4c22-8569-0684331b85a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "If we have some errors in our results, we can rerun those samples again. We could use function calling here but we'll also experiment with generating these reference answers with OSS LLMs (ex. `Llama-2-70b`) which don't have function calling:\n",
    "\n",
    "```python\n",
    "error_indices = [i for i, row in enumerate(results) if row[\"answer\"] == \"\" and row[\"source\"].startswith(\"https://docs.ray.io\")]\n",
    "for i in tqdm(error_indices):\n",
    "    row = results[i]\n",
    "    query = row[\"question\"]\n",
    "    context = sections_per_doc.get(row[\"source\"], \"\")\n",
    "    user_content = f\"The question is {query} and the additional context is {context}\"[:max_context_length]\n",
    "    response = generate_response(\n",
    "        llm=llm, \n",
    "        system_content=system_content, \n",
    "        assistant_content=assistant_content, \n",
    "        user_content=user_content)\n",
    "    answer, score, reasoning = extract_from_response(response=response)\n",
    "    result = ({\n",
    "            \"question\": query,\n",
    "            \"source\": row[\"source\"],\n",
    "            \"answer\": answer,\n",
    "            \"score\": score,\n",
    "            \"reasoning\": reasoning,\n",
    "        })\n",
    "    results[i] = result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd71260-5b7f-4ac4-b1f8-299369b157ea",
   "metadata": {},
   "source": [
    "Rerun the above cell until no errors in extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "65111ac3-6dbc-47f9-aa3a-e238b2743fac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "references_fp = Path(ROOT_DIR, \"experiments\", \"references\", \"gpt-4.json\")\n",
    "references_fp.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c0f48d7-9d8d-4eeb-a304-fc93324ec4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(references_fp, \"w\") as fp:\n",
    "    json.dump(results, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "71f389d0-f6a2-4175-8e95-dcde55244c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read from file\n",
    "with open(references_fp, \"r\") as fp:\n",
    "    results = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f15cfeae-84a0-4679-82c3-9eb1f366477c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.527777777777778\n"
     ]
    }
   ],
   "source": [
    "# Average score gpt-4 gave itself\n",
    "print (np.mean([float(result[\"score\"]) for result in results if result[\"score\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "72d05f17-eca6-48b7-8691-61724b91566b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I’m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       " 'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format',\n",
       " 'answer': \"When you're handling Ray Data type conversions and using `map_batches`, you can configure the batch type by specifying `batch_format` in `map_batches()`. You can return either format from your function. If you're dealing with NumPy datasets, your function manipulates the specific numpy dataset. For instance, in the provided example, `increase_brightness` function increases the brightness of an image. Similarly, if you're dealing with pandas DataFrame, you can perform operations like dropping NaN values from the DataFrame using your function. It's vital that the functions are correctly formatted and the intended operation is properly performed in these functions.\",\n",
       " 'score': 5.0,\n",
       " 'reasoning': 'The provided answer is well-detailed and thorough. It explains why one might be experiencing issues with Ray Data type conversions when they do map_batches and how to overcome them. The answer provides examples for both NumPy and pandas showing how to use the `map_batches()` function in different scenarios. Hence, I believe this would be quite useful for someone having trouble with type conversions in Ray.'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98960772-bf08-4eef-b594-273e4dcddebd",
   "metadata": {},
   "source": [
    "### Llama-2-70b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76733db-4b41-4911-aae8-0d8aa01c0330",
   "metadata": {},
   "source": [
    "Let's generate reference responses with `Llama-2-70b` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "30d27ea5-128a-4a5c-ad3e-b053c6ba1efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai.api_base = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "openai.api_key = os.environ[\"ANYSCALE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8037f7a6-644c-4b8c-adcc-1d1c1ca76cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/179 [00:22<22:23,  7.64s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>llm = <span style=\"color: #808000; text-decoration-color: #808000\">\"meta-llama/Llama-2-70b-chat-hf\"</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>max_context_length = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">4096</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3 results = get_references(                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>data=data, sections_per_doc=sections_per_doc,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>llm=llm, temperature=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.0</span>, max_context_length=max_context_length,                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>system_content=system_content, assistant_content=assistant_content)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_references</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Generate response</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>context_length = max_context_length - <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(system_content + assistant_content)       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>user_content = <span style=\"color: #808000; text-decoration-color: #808000\">f\"The query is {</span>query<span style=\"color: #808000; text-decoration-color: #808000\">} and the additional context is {</span>context<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>[:    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>11 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response = generate_response(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>llm=llm,                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>temperature=temperature,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>system_content=system_content,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate_response</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>retry_count = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> retry_count &lt; max_retries:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 9 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>response = openai.ChatCompletion.create(                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>model=llm,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>temperature=temperature,                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>messages=[                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/mnt/cluster_storage/pypi/lib/python3.9/site-packages/openai/api_resources/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">chat_completion.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">25</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">create</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>25 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().create(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> TryAgain <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> timeout <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> time.time() &gt; start + timeout:                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/mnt/cluster_storage/pypi/lib/python3.9/site-packages/openai/api_resources/abstract/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">engine_api_r</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">esource.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">153</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">create</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">150 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>api_key, api_base, api_type, api_version, organization, **params               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">151 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>153 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response, _, api_key = requestor.request(                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">154 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"post\"</span>,                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">155 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>url,                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">156 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>params=params,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/mnt/cluster_storage/pypi/lib/python3.9/site-packages/openai/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">api_requestor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">288</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">request</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">285 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>request_id: Optional[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">286 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>request_timeout: Optional[Union[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">float</span>, Tuple[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">float</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">float</span>]]] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>) -&gt; Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], <span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>]:                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>288 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.request_raw(                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">289 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>method.lower(),                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">290 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>url,                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">291 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>params=params,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/mnt/cluster_storage/pypi/lib/python3.9/site-packages/openai/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">api_requestor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">596</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">request_raw</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">593 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>_thread_context.session = _make_session()                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">594 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>_thread_context.session_create_time = time.time()                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">595 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>596 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>result = _thread_context.session.request(                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">597 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>method,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">598 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>abs_url,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">599 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>headers=headers,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/site-packages/requests/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">sessions.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">589</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">request</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">586 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"allow_redirects\"</span>: allow_redirects,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">587 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>}                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">588 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>send_kwargs.update(settings)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>589 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>resp = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.send(prep, **send_kwargs)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">590 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">591 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> resp                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">592 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/site-packages/requests/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">sessions.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">703</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">send</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">700 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>start = preferred_clock()                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">701 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">702 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Send the request</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>703 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>r = adapter.send(request, **kwargs)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">704 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">705 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Total elapsed time of the request (approximately)</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">706 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>elapsed = preferred_clock() - start                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/site-packages/requests/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">adapters.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">486</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">send</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">483 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>timeout = TimeoutSauce(connect=timeout, read=timeout)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>486 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resp = conn.urlopen(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">487 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>method=request.method,                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">488 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>url=url,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>body=request.body,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">connectionpool.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">714</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">urlopen</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 711 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._prepare_proxy(conn)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 712 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 713 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Make the request on the httplib connection object.</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 714 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>httplib_response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._make_request(                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 715 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>conn,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 716 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>method,                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 717 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>url,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">connectionpool.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">466</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_make_request</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 463 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Remove the TypeError from the exception chain in</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 464 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Python 3 (including for exceptions like SystemExit).</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 465 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Otherwise it looks like a bug in the code.</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 466 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>six.raise_from(e, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 467 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> (SocketTimeout, BaseSSLError, SocketError) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 468 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._raise_timeout(err=e, url=url, timeout_value=read_timeout)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 469 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">raise_from</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">connectionpool.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">461</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_make_request</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 458 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">TypeError</span>:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 459 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Python 3</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 460 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 461 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>httplib_response = conn.getresponse()                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 462 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">BaseException</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 463 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Remove the TypeError from the exception chain in</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 464 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Python 3 (including for exceptions like SystemExit).</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/http/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1377</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">getresponse</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1374 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1375 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1376 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1377 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>response.begin()                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1378 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ConnectionError</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1379 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.close()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1380 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/http/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">320</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">begin</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 317 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 318 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># read until we get a non-100 response</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 319 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 320 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>version, status, reason = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._read_status()                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 321 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> status != CONTINUE:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 322 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 323 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># skip the header from the 100 response</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/http/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">client.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">281</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_read_status</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 278 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.will_close = _UNKNOWN      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># conn will close at end of response</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 279 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 280 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_read_status</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 281 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>line = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fp.readline(_MAXLINE + <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>), <span style=\"color: #808000; text-decoration-color: #808000\">\"iso-8859-1\"</span>)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 282 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(line) &gt; _MAXLINE:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 283 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> LineTooLong(<span style=\"color: #808000; text-decoration-color: #808000\">\"status line\"</span>)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 284 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.debuglevel &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">socket.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">704</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">readinto</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">701 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">OSError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"cannot read from timed out object\"</span>)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">702 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">703 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>704 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._sock.recv_into(b)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">705 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> timeout:                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">706 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._timeout_occurred = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">707 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ssl.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1242</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">recv_into</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1239 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1240 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │     </span><span style=\"color: #808000; text-decoration-color: #808000\">\"non-zero flags not allowed in calls to recv_into() on %s\"</span> %            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1241 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │     </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span>)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1242 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.read(nbytes, buffer)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1243 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1244 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().recv_into(buffer, nbytes, flags)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1245 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ray/anaconda3/lib/python3.9/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ssl.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1100</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">read</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1097 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Read on closed or unwrapped SSL socket.\"</span>)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1098 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1099 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> buffer <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1100 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._sslobj.read(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>, buffer)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1101 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1102 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._sslobj.read(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1103 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> SSLError <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> x:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0mllm = \u001b[33m\"\u001b[0m\u001b[33mmeta-llama/Llama-2-70b-chat-hf\u001b[0m\u001b[33m\"\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0mmax_context_length = \u001b[94m4096\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3 results = get_references(                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[2m│   \u001b[0mdata=data, sections_per_doc=sections_per_doc,                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[2m│   \u001b[0mllm=llm, temperature=\u001b[94m0.0\u001b[0m, max_context_length=max_context_length,                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m\u001b[2m│   \u001b[0msystem_content=system_content, assistant_content=assistant_content)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mget_references\u001b[0m:\u001b[94m11\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Generate response\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   │   \u001b[0mcontext_length = max_context_length - \u001b[96mlen\u001b[0m(system_content + assistant_content)       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   │   \u001b[0muser_content = \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mThe query is \u001b[0m\u001b[33m{\u001b[0mquery\u001b[33m}\u001b[0m\u001b[33m and the additional context is \u001b[0m\u001b[33m{\u001b[0mcontext\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m[:    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m11 \u001b[2m│   │   \u001b[0mresponse = generate_response(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   │   │   \u001b[0mllm=llm,                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   │   │   \u001b[0mtemperature=temperature,                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   │   │   \u001b[0msystem_content=system_content,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mgenerate_response\u001b[0m:\u001b[94m9\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0mretry_count = \u001b[94m0\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwhile\u001b[0m retry_count < max_retries:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 9 \u001b[2m│   │   │   \u001b[0mresponse = openai.ChatCompletion.create(                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodel=llm,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtemperature=temperature,                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmessages=[                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/mnt/cluster_storage/pypi/lib/python3.9/site-packages/openai/api_resources/\u001b[0m\u001b[1;33mchat_completion.py\u001b[0m:\u001b[94m25\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mcreate\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m25 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m().create(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m TryAgain \u001b[94mas\u001b[0m e:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m timeout \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m time.time() > start + timeout:                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/mnt/cluster_storage/pypi/lib/python3.9/site-packages/openai/api_resources/abstract/\u001b[0m\u001b[1;33mengine_api_r\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mesource.py\u001b[0m:\u001b[94m153\u001b[0m in \u001b[92mcreate\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   │   \u001b[0mapi_key, api_base, api_type, api_version, organization, **params               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m153 \u001b[2m│   │   \u001b[0mresponse, _, api_key = requestor.request(                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m154 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mpost\u001b[0m\u001b[33m\"\u001b[0m,                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m155 \u001b[0m\u001b[2m│   │   │   \u001b[0murl,                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   │   \u001b[0mparams=params,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/mnt/cluster_storage/pypi/lib/python3.9/site-packages/openai/\u001b[0m\u001b[1;33mapi_requestor.py\u001b[0m:\u001b[94m288\u001b[0m in \u001b[92mrequest\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m285 \u001b[0m\u001b[2m│   │   \u001b[0mrequest_id: Optional[\u001b[96mstr\u001b[0m] = \u001b[94mNone\u001b[0m,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m│   │   \u001b[0mrequest_timeout: Optional[Union[\u001b[96mfloat\u001b[0m, Tuple[\u001b[96mfloat\u001b[0m, \u001b[96mfloat\u001b[0m]]] = \u001b[94mNone\u001b[0m,               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m287 \u001b[0m\u001b[2m│   \u001b[0m) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[96mbool\u001b[0m, \u001b[96mstr\u001b[0m]:                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m288 \u001b[2m│   │   \u001b[0mresult = \u001b[96mself\u001b[0m.request_raw(                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m289 \u001b[0m\u001b[2m│   │   │   \u001b[0mmethod.lower(),                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   │   │   \u001b[0murl,                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m│   │   │   \u001b[0mparams=params,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/mnt/cluster_storage/pypi/lib/python3.9/site-packages/openai/\u001b[0m\u001b[1;33mapi_requestor.py\u001b[0m:\u001b[94m596\u001b[0m in \u001b[92mrequest_raw\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m593 \u001b[0m\u001b[2m│   │   │   \u001b[0m_thread_context.session = _make_session()                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m594 \u001b[0m\u001b[2m│   │   │   \u001b[0m_thread_context.session_create_time = time.time()                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m595 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m596 \u001b[2m│   │   │   \u001b[0mresult = _thread_context.session.request(                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m597 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmethod,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m598 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mabs_url,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m599 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mheaders=headers,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/site-packages/requests/\u001b[0m\u001b[1;33msessions.py\u001b[0m:\u001b[94m589\u001b[0m in \u001b[92mrequest\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m586 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mallow_redirects\u001b[0m\u001b[33m\"\u001b[0m: allow_redirects,                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m587 \u001b[0m\u001b[2m│   │   \u001b[0m}                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m588 \u001b[0m\u001b[2m│   │   \u001b[0msend_kwargs.update(settings)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m589 \u001b[2m│   │   \u001b[0mresp = \u001b[96mself\u001b[0m.send(prep, **send_kwargs)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m590 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m591 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m resp                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m592 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/site-packages/requests/\u001b[0m\u001b[1;33msessions.py\u001b[0m:\u001b[94m703\u001b[0m in \u001b[92msend\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m700 \u001b[0m\u001b[2m│   │   \u001b[0mstart = preferred_clock()                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m701 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m702 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Send the request\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m703 \u001b[2m│   │   \u001b[0mr = adapter.send(request, **kwargs)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m704 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m705 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Total elapsed time of the request (approximately)\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m706 \u001b[0m\u001b[2m│   │   \u001b[0melapsed = preferred_clock() - start                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/site-packages/requests/\u001b[0m\u001b[1;33madapters.py\u001b[0m:\u001b[94m486\u001b[0m in \u001b[92msend\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m483 \u001b[0m\u001b[2m│   │   │   \u001b[0mtimeout = TimeoutSauce(connect=timeout, read=timeout)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m484 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m485 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m486 \u001b[2m│   │   │   \u001b[0mresp = conn.urlopen(                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m487 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmethod=request.method,                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m488 \u001b[0m\u001b[2m│   │   │   │   \u001b[0murl=url,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m489 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbody=request.body,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/\u001b[0m\u001b[1;33mconnectionpool.py\u001b[0m:\u001b[94m714\u001b[0m in \u001b[92murlopen\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 711 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._prepare_proxy(conn)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 712 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 713 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Make the request on the httplib connection object.\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 714 \u001b[2m│   │   │   \u001b[0mhttplib_response = \u001b[96mself\u001b[0m._make_request(                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 715 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mconn,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmethod,                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 717 \u001b[0m\u001b[2m│   │   │   │   \u001b[0murl,                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/\u001b[0m\u001b[1;33mconnectionpool.py\u001b[0m:\u001b[94m466\u001b[0m in \u001b[92m_make_request\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 463 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Remove the TypeError from the exception chain in\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 464 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Python 3 (including for exceptions like SystemExit).\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 465 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Otherwise it looks like a bug in the code.\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 466 \u001b[2m│   │   │   │   │   \u001b[0msix.raise_from(e, \u001b[94mNone\u001b[0m)                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 467 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m (SocketTimeout, BaseSSLError, SocketError) \u001b[94mas\u001b[0m e:                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 468 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._raise_timeout(err=e, url=url, timeout_value=read_timeout)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 469 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mraise_from\u001b[0m:\u001b[94m3\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/site-packages/urllib3/\u001b[0m\u001b[1;33mconnectionpool.py\u001b[0m:\u001b[94m461\u001b[0m in \u001b[92m_make_request\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 458 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mTypeError\u001b[0m:                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 459 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Python 3\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 460 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 461 \u001b[2m│   │   │   │   │   \u001b[0mhttplib_response = conn.getresponse()                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 462 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mBaseException\u001b[0m \u001b[94mas\u001b[0m e:                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 463 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Remove the TypeError from the exception chain in\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 464 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Python 3 (including for exceptions like SystemExit).\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/http/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m1377\u001b[0m in \u001b[92mgetresponse\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1374 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1375 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1376 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1377 \u001b[2m│   │   │   │   \u001b[0mresponse.begin()                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1378 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mConnectionError\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1379 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.close()                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1380 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/http/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m320\u001b[0m in \u001b[92mbegin\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 317 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 318 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# read until we get a non-100 response\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 319 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 320 \u001b[2m│   │   │   \u001b[0mversion, status, reason = \u001b[96mself\u001b[0m._read_status()                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 321 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m status != CONTINUE:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 322 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 323 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# skip the header from the 100 response\u001b[0m                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/http/\u001b[0m\u001b[1;33mclient.py\u001b[0m:\u001b[94m281\u001b[0m in \u001b[92m_read_status\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 278 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.will_close = _UNKNOWN      \u001b[2m# conn will close at end of response\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 279 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 280 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_read_status\u001b[0m(\u001b[96mself\u001b[0m):                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 281 \u001b[2m│   │   \u001b[0mline = \u001b[96mstr\u001b[0m(\u001b[96mself\u001b[0m.fp.readline(_MAXLINE + \u001b[94m1\u001b[0m), \u001b[33m\"\u001b[0m\u001b[33miso-8859-1\u001b[0m\u001b[33m\"\u001b[0m)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 282 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(line) > _MAXLINE:                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 283 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m LineTooLong(\u001b[33m\"\u001b[0m\u001b[33mstatus line\u001b[0m\u001b[33m\"\u001b[0m)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 284 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.debuglevel > \u001b[94m0\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/\u001b[0m\u001b[1;33msocket.py\u001b[0m:\u001b[94m704\u001b[0m in \u001b[92mreadinto\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m701 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mOSError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mcannot read from timed out object\u001b[0m\u001b[33m\"\u001b[0m)                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m702 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m703 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m704 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._sock.recv_into(b)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m705 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m timeout:                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m706 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._timeout_occurred = \u001b[94mTrue\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m707 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/\u001b[0m\u001b[1;33mssl.py\u001b[0m:\u001b[94m1242\u001b[0m in \u001b[92mrecv_into\u001b[0m                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1239 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1240 \u001b[0m\u001b[2m│   │   │   │     \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m %            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1241 \u001b[0m\u001b[2m│   │   │   │     \u001b[0m\u001b[96mself\u001b[0m.\u001b[91m__class__\u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1242 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.read(nbytes, buffer)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1243 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1244 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m().recv_into(buffer, nbytes, flags)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1245 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ray/anaconda3/lib/python3.9/\u001b[0m\u001b[1;33mssl.py\u001b[0m:\u001b[94m1100\u001b[0m in \u001b[92mread\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1097 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mRead on closed or unwrapped SSL socket.\u001b[0m\u001b[33m\"\u001b[0m)                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1098 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1099 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m buffer \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1100 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._sslobj.read(\u001b[96mlen\u001b[0m, buffer)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1101 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1102 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._sslobj.read(\u001b[96mlen\u001b[0m)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1103 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m SSLError \u001b[94mas\u001b[0m x:                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "max_context_length = 4096\n",
    "results = get_references(\n",
    "    data=data, sections_per_doc=sections_per_doc, \n",
    "    llm=llm, temperature=0.0, max_context_length=max_context_length, \n",
    "    system_content=system_content, assistant_content=assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0fe3b56e-c20b-46aa-8c95-5f677c26d3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "references_fp = Path(ROOT_DIR, \"experiments\", \"references\", \"llama-2-70b.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8174556d-72ca-4b42-b35d-1e6479af50f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(references_fp, \"w\") as fp:\n",
    "    json.dump(results, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a0a14884-eb83-4d8c-bac2-f49010d31688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read from file\n",
    "with open(references_fp, \"r\") as fp:\n",
    "    results = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2bc4849c-0639-449d-bfe2-b6b06b9687b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.928104575163399\n"
     ]
    }
   ],
   "source": [
    "# Average score llama-2-70b gave itself\n",
    "print (np.mean([float(result[\"score\"]) for result in results if result[\"score\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7bcf49f0-4433-4a49-8f68-7d49414668cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I’m struggling a bit with Ray Data type conversions when I do map_batches. Any advice?',\n",
       " 'source': 'https://docs.ray.io/en/master/data/transforming-data.html#configuring-batch-format',\n",
       " 'answer': \"You can specify the batch format by using the batch_format argument in the map_batches function. For example, to use NumPy ndarrays, you can set batch_format='numpy'. To use pandas DataFrames, you can set batch_format='pandas'.\",\n",
       " 'score': 5.0,\n",
       " 'reasoning': 'The answer is correct and provides a clear solution to the problem. It also includes examples of how to specify the batch format for both NumPy ndarrays and pandas DataFrames.'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5772f9aa-30e1-44bb-b08b-41d014256515",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c522532-98c2-46e9-b262-1747145f34e1",
   "metadata": {},
   "source": [
    "Now that we've seen the answers, scores and reasoning for our references dataset from both `gpt-4` and `Llama-2-70b`. We can use these responses to decide on a quality evaluator for our future experiments. This evaluator will be used to score answers for different experiment configuations and so we need to be able to trust their scores, reasoning, etc. After inspecting Llama2 evaluating Llama2's answers, it is definitely not a good evaluator. For most answers the reasoning is not good, and the score is pretty random with lots of 4s. Therefore, our evaluator will be `gpt-4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "dceefac3-e86c-4f2b-96ee-99e3f026692f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVALUATOR = \"gpt-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b029edf-e018-427f-a1c9-2b4bf689d09c",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea24ed-0a47-47a6-9881-0af1ea8c5691",
   "metadata": {},
   "source": [
    "We're going to start experimenting with the various components in our LLM application such as our evaluator, context, sections, chunking size, number of chunks in our context, embedding models, OSS/closed LLMs and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b51fd3-7a86-41dd-91bb-adcb83bf7269",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3f59f-f79b-4cdb-b55f-6d1d5d245628",
   "metadata": {},
   "source": [
    "str(Path(ROOT_DIR, \"datasets\", \"eval-dataset-v1.jsonl\")Before we get started with our experiments, we're going to define some utility functions that we'll use to easily generate and evaluate responses using the different experiment configurations. We'll also define some functions to help determine our response quality score, retrieval recall score, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "0f9bdb8e-ed08-47b5-b456-1b1ff3e82c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9961f6af-06ea-4179-a4c5-bfa23479364c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = str(Path(ROOT_DIR, \"datasets\", \"eval-dataset-v1.jsonl\"))\n",
    "REFERENCE_LOC = str(Path(ROOT_DIR, \"experiments\", \"references\", \"gpt-4.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "86bfa2e5-c48d-4e9a-9a22-026a7e1d8e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mappings\n",
    "EMBEDDING_DIMENSIONS = {\n",
    "    \"thenlper/gte-base\": 768,\n",
    "    \"BAAI/bge-large-en\": 1024,\n",
    "    \"text-embedding-ada-002\": 1536\n",
    "}\n",
    "MAX_CONTEXT_LENGTHS = {\n",
    "    \"gpt-4\": 8192,\n",
    "    \"gpt-3.5-turbo\": 4096,\n",
    "    \"gpt-3.5-turbo-16k\": 16384,\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\": 4096,\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\": 4096,\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\": 4096,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "13f6f1dd-b9f4-468b-aff7-7a7e68071ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_bash(command):\n",
    "    results = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e4076f37-2798-40db-9284-c70fa0da7287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_index(embedding_model_name, chunk_size, chunk_overlap):\n",
    "    # Drop current Vector DB and prepare for new one\n",
    "    execute_bash(f'''psql \"{os.environ[\"DB_CONNECTION_STRING\"]}\" -c \"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state = 'idle in transaction';\"''')\n",
    "    execute_bash(f'psql \"{os.environ[\"DB_CONNECTION_STRING\"]}\" -c \"DROP TABLE document;\"')\n",
    "    execute_bash(f'sudo -u postgres psql -f ../migrations/vector-{EMBEDDING_DIMENSIONS[embedding_model_name]}.sql')\n",
    "    SQL_DUMP_FP = Path(EFS_DIR, \"sql_dumps\", f\"{embedding_model_name.split('/')[-1]}_{chunk_size}_{chunk_overlap}.sql\")\n",
    "    \n",
    "    # Load vector DB\n",
    "    if SQL_DUMP_FP.exists():  # Load from SQL dump\n",
    "        execute_bash(f'psql \"{os.environ[\"DB_CONNECTION_STRING\"]}\" -f {SQL_DUMP_FP}')\n",
    "    else:  # Create new index\n",
    "        # Create chunks dataset\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "        chunks = text_splitter.create_documents(\n",
    "            texts=[section[\"text\"] for section in sections], \n",
    "            metadatas=[{\"source\": section[\"source\"]} for section in sections]\n",
    "        )\n",
    "        chunks_ds = ray.data.from_items([{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks])\n",
    "\n",
    "        # Embed chunks\n",
    "        embedded_chunks = chunks_ds.map_batches(\n",
    "            EmbedChunks,\n",
    "            fn_constructor_kwargs={\"model_name\": embedding_model_name},\n",
    "            batch_size=100, \n",
    "            num_gpus=1,\n",
    "            compute=ActorPoolStrategy(size=1))\n",
    "        \n",
    "        # Index data\n",
    "        embedded_chunks.map_batches(\n",
    "            StoreResults,\n",
    "            batch_size=128,\n",
    "            num_cpus=1,\n",
    "            compute=ActorPoolStrategy(size=15),\n",
    "        ).count()\n",
    "        \n",
    "        # Save to SQL dump\n",
    "        execute_bash(f\"sudo -u postgres pg_dump -c > {SQL_DUMP_FP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1f2e38a9-0366-4204-b542-c4b6e61deed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_credentials(llm):\n",
    "    if llm.startswith(\"gpt\"):\n",
    "        openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    else:\n",
    "        openai.api_base = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "        openai.api_key = os.environ[\"ANYSCALE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "64a9d54d-146c-41a6-b13c-61513549434b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "def generate_responses(\n",
    "    experiment_name, data_path, \n",
    "    chunk_size, chunk_overlap, num_chunks,\n",
    "    embedding_model_name, \n",
    "    llm, temperature, max_context_length, \n",
    "    system_content, assistant_content=\"\"):\n",
    "    \n",
    "    # Set credentials\n",
    "    set_credentials(llm=llm)\n",
    "    \n",
    "    # Build index\n",
    "    create_index(\n",
    "        embedding_model_name=embedding_model_name,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    \n",
    "    # Query agent\n",
    "    agent = QueryAgent(\n",
    "        embedding_model_name=embedding_model_name,\n",
    "        llm=llm,\n",
    "        temperature=temperature,\n",
    "        max_context_length=max_context_length,\n",
    "        system_content=system_content,\n",
    "        assistant_content=assistant_content,\n",
    "    )\n",
    "\n",
    "    # Generate responses\n",
    "    results = []\n",
    "    with open(Path(data_path), \"r\") as f:\n",
    "        questions = [json.loads(item)[\"question\"] for item in list(f)]\n",
    "    for query in tqdm(questions):\n",
    "        result = agent(query=query, num_chunks=num_chunks)\n",
    "        results.append(result)\n",
    "\n",
    "    # Save to file\n",
    "    responses_fp = Path(ROOT_DIR, \"experiments\", \"responses\", f\"{experiment_name}.json\")\n",
    "    responses_fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    config = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"data_path\": data_path,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"num_chunks\": num_chunks,\n",
    "        \"embedding_model_name\": embedding_model_name,\n",
    "        \"llm\": llm,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_context_length\": max_context_length,\n",
    "        \"system_content\": system_content,\n",
    "        \"assistant_content\": assistant_content,\n",
    "    }\n",
    "    responses = {\n",
    "        \"config\": config,\n",
    "        \"results\": results,\n",
    "    }\n",
    "    with open(responses_fp, \"w\") as fp:\n",
    "        json.dump(responses, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "73a57c87-8ef0-4d45-9594-34ca07ff7210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_retrieval_score(references, generated):\n",
    "    matches = np.zeros(len(references))\n",
    "    for i in range(len(references)):\n",
    "        reference_source = references[i][\"source\"].split(\"#\")[0]\n",
    "        if not reference_source:\n",
    "            matches[i] = 1\n",
    "            continue\n",
    "        for source in generated[i][\"sources\"]:\n",
    "            # sections don't have to perfectly match\n",
    "            if reference_source == source.split(\"#\")[0]:\n",
    "                matches[i] = 1\n",
    "                continue\n",
    "    retrieval_score = np.mean(matches)\n",
    "    return retrieval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9943ec93-cdb8-4f17-889c-7a1d98cde143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_responses(\n",
    "    experiment_name, reference_loc, response_loc,\n",
    "    evaluator, temperature, max_context_length,\n",
    "    system_content, assistant_content=\"\"):\n",
    "    \n",
    "    # Set credentials\n",
    "    set_credentials(llm=evaluator)\n",
    "    \n",
    "    # Load answers\n",
    "    with open(Path(reference_loc), \"r\") as f:\n",
    "        references = [item for item in json.load(f)]\n",
    "    with open(Path(response_loc), \"r\") as f:\n",
    "        generated = [item for item in json.load(f)[\"results\"]]\n",
    "    assert len(references) == len(generated)\n",
    "\n",
    "    # Quality score\n",
    "    results = []\n",
    "    context_length = max_context_length - len(system_content + assistant_content)\n",
    "    for ref, gen in tqdm(zip(references, generated), total=len(references)):\n",
    "        assert ref[\"question\"] == gen[\"question\"]\n",
    "        user_content = str(\n",
    "            {\n",
    "                \"question\": gen[\"question\"],\n",
    "                \"generated_answer\": gen[\"answer\"],\n",
    "                \"reference_answer\": ref[\"answer\"],\n",
    "            }\n",
    "        )[:context_length]\n",
    "\n",
    "        # Generate response\n",
    "        response = generate_response(\n",
    "            llm=evaluator,\n",
    "            temperature=temperature,\n",
    "            system_content=system_content,\n",
    "            assistant_content=assistant_content,\n",
    "            user_content=user_content,\n",
    "        )\n",
    "\n",
    "        # Extract from response\n",
    "        score, reasoning = response.split(\"\\n\", 1)\n",
    "\n",
    "        # Store result\n",
    "        result = {\n",
    "            \"question\": gen[\"question\"],\n",
    "            \"generated_answer\": gen[\"answer\"],\n",
    "            \"reference_answer\": ref[\"answer\"],\n",
    "            \"score\": float(score),\n",
    "            \"reasoning\": reasoning.lstrip(\"\\n\"),\n",
    "            \"sources\": gen[\"sources\"],\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Save to file\n",
    "    evaluator_name = evaluator.split(\"/\")[-1].lower()\n",
    "    evaluation_fp = Path(ROOT_DIR, \"experiments\", \"evaluations\", f\"{experiment_name}_{evaluator_name}.json\")\n",
    "    evaluation_fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    config = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"reference_loc\": reference_loc,\n",
    "        \"response_loc\": response_loc,\n",
    "        \"evaluator\": evaluator,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_context_length\": max_context_length,\n",
    "        \"system_content\": system_content,\n",
    "        \"assistant_content\": assistant_content,\n",
    "    }\n",
    "    evaluation = {\n",
    "        \"config\": config,\n",
    "        \"retrieval_score\": get_retrieval_score(references, generated),\n",
    "        \"quality_score\": np.mean([item[\"score\"] for item in results if (item[\"score\"] and item[\"reference_answer\"])]),\n",
    "        \"results\": results,\n",
    "    }\n",
    "    with open(evaluation_fp, \"w\") as fp:\n",
    "        json.dump(evaluation, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "66a76e95-9c1b-488f-bd9f-750a176d3d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    experiment_name, data_path,\n",
    "    chunk_size, chunk_overlap, num_chunks,\n",
    "    embedding_model_name, llm,\n",
    "    reference_loc, evaluator):\n",
    "    \"\"\"Generate responses and evaluate them.\"\"\"\n",
    "    \n",
    "    # Generate responses\n",
    "    generate_responses(\n",
    "        experiment_name=experiment_name, \n",
    "        data_path=data_path, \n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap, \n",
    "        num_chunks=num_chunks,\n",
    "        embedding_model_name=embedding_model_name, \n",
    "        llm=llm, \n",
    "        temperature=0.0, \n",
    "        max_context_length=MAX_CONTEXT_LENGTHS[llm], \n",
    "        system_content=\"Answer the {query} using the additional {context} provided.\")\n",
    "\n",
    "    # Evaluate responses\n",
    "    evaluation_system_content = \"\"\"\n",
    "        Your job is to rate the quality of our generated answer {generated_answer}\n",
    "        given a query {query} and a reference answer {reference_answer}.\n",
    "        Your score has to be between 1 and 5.\n",
    "        You must return your response in a line with only the score.\n",
    "        Do not return answers in any other format.\n",
    "        On a separate line provide your reasoning for the score as well.\n",
    "        \"\"\"\n",
    "    evaluate_responses(\n",
    "        experiment_name=experiment_name,\n",
    "        reference_loc=reference_loc, \n",
    "        response_loc=str(Path(ROOT_DIR, \"experiments\", \"responses\", f\"{experiment_name}.json\")),\n",
    "        evaluator=EVALUATOR, \n",
    "        temperature=0.0, \n",
    "        max_context_length=MAX_CONTEXT_LENGTHS[EVALUATOR],\n",
    "        system_content=evaluation_system_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "169201fd-aa78-4459-a55f-04c97f1938aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_experiment(experiment_name, evaluator=EVALUATOR):\n",
    "    eval_fp = Path(ROOT_DIR, \"experiments\", \"evaluations\", f\"{experiment_name}_{evaluator}.json\")\n",
    "    with open(eval_fp, \"r\") as fp:\n",
    "        d = json.load(fp)\n",
    "    print (experiment_name)\n",
    "    print (\"  retrieval score:\", d[\"retrieval_score\"])\n",
    "    print (\"  quality score:\", d[\"quality_score\"])\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31ebee-f839-4aaa-b328-72cee088c830",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f56c4-2629-4ff7-b57c-f319170937de",
   "metadata": {},
   "source": [
    "We're first going to test if the additonal context we provide is helpful at all. This is to validate that the RAG system is indeed worth the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282271b0-8b4a-45fe-ae23-2c674b291c13",
   "metadata": {},
   "source": [
    "#### Without context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cefda6-0ec7-40a2-afc0-b8af2bdd3332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_chunks = 0\n",
    "experiment_name = \"without-context\"\n",
    "run_experiment(\n",
    "    experiment_name=experiment_name, \n",
    "    data_path=DATA_PATH,\n",
    "    chunk_size=100, \n",
    "    chunk_overlap=50, \n",
    "    num_chunks=num_chunks,\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    reference_loc=REFERENCE_LOC,\n",
    "    evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f506ea99-7397-44b3-88a1-0d2a2df1ac21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without-context\n",
      "  retrieval score: 0.0\n",
      "  quality score: 2.631284916201117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc2fd9-68b5-4d26-8bab-f4b37ce06674",
   "metadata": {},
   "source": [
    "#### With context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819c26a-9e05-484b-8e57-edfd26a84d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks = 5\n",
    "experiment_name = \"with-context\"\n",
    "run_experiment(\n",
    "    experiment_name=experiment_name, \n",
    "    data_path=DATA_PATH,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50, \n",
    "    num_chunks=num_chunks,\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    reference_loc=REFERENCE_LOC,\n",
    "    evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327cf28-5114-4e93-b769-014108dd743a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with-context\n",
      "  retrieval score: 0.4301675977653631\n",
      "  quality score: 3.2458100558659218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01f761-d3a7-4783-9fa7-57f62679b548",
   "metadata": {},
   "source": [
    "As we can see, **using context (RAG)** does indeed help in the quality of our answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b334355-ab35-4cf3-85b5-601f62b4e7e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a75e19-8cd0-4040-8be6-cb18facf0366",
   "metadata": {},
   "source": [
    "#### Without sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e0c2584-2249-463b-9437-23b38c3bc9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa643e-72f0-4286-b519-4f0e7f6c9399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = ReadTheDocsLoader(f\"{EFS_DIR}/docs.ray.io/en/master/\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "docs = loader.load()\n",
    "for doc in docs:  # clean\n",
    "    doc.metadata[\"source\"] = doc.metadata[\"source\"].replace(str(EFS_DIR)+\"/\", \"https://\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "074a8227-7b0e-49cb-888e-2ed8d7aaed63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ddd3467b-cd77-4f6d-bba4-5955f2aa8e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chunks\n",
    "chunks = text_splitter.create_documents(\n",
    "    texts=[doc.page_content for doc in docs], \n",
    "    metadatas=[doc.metadata for doc in docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b213d197-111b-4388-989a-8931ab8df945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Environments#\\nRLlib works with several different types of environments, including Farama-Foundation Gymnasium, user-defined, multi-agent, and also batched environments.\\nTip\\nNot all environments work with all algorithms. Check out the algorithm overview for more information.\\nConfiguring Environments#', 'source': '/efs/shared_storage/goku/docs.ray.io/en/master/rllib-env.html'}\n"
     ]
    }
   ],
   "source": [
    "# Ray dataset\n",
    "chunks_ds = ray.data.from_items([{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks])\n",
    "chunks_ds.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dd77bfc5-0088-47d5-80be-8f031c1aa4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed chunks\n",
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "embedded_chunks = chunks_ds.map_batches(\n",
    "    EmbedChunks,\n",
    "    fn_constructor_kwargs={\"model_name\": embedding_model_name},\n",
    "    batch_size=100, \n",
    "    num_gpus=1,\n",
    "    compute=ActorPoolStrategy(size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d16aa979-6b7f-4dbf-acc4-fad1246c0448",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "DROP TABLE\n",
      "CREATE TABLE\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Drop current vector DB (if any)\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"DROP TABLE document;\"\n",
    "sudo -u postgres psql -f ../migrations/vector-768.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "330a21a3-5e25-4c18-aedb-38ae0cb2b806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 16:44:15,944\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(EmbedChunks)] -> ActorPoolMapOperator[MapBatches(StoreResults)]\n",
      "2023-08-24 16:44:15,945\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-24 16:44:15,946\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-08-24 16:44:15,972\tINFO actor_pool_map_operator.py:117 -- MapBatches(EmbedChunks): Waiting for 2 pool actors to start...\n",
      "2023-08-24 16:44:37,256\tINFO actor_pool_map_operator.py:117 -- MapBatches(StoreResults): Waiting for 28 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 16:51:19,313\tWARNING actor_pool_map_operator.py:267 -- To ensure full parallelization across an actor pool of size 28, the specified batch size should be at most 0. Your configured batch size for this operator was 128.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index data\n",
    "embedded_chunks.map_batches(\n",
    "    StoreResults,\n",
    "    batch_size=128,\n",
    "    num_cpus=1,\n",
    "    compute=ActorPoolStrategy(size=28),\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6bf5d4da-5232-4296-969c-385bd5ee57c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " count \n",
      "-------\n",
      " 49220\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check number of rows\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"SELECT count(*) FROM document;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f4e2847d-63be-465a-93b6-41bb19ec1e09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Save index\n",
    "export SQL_DUMP_FP=\"/efs/shared_storage/goku/sql_dumps/without-sections.sql\"\n",
    "sudo -u postgres pg_dump -c > $SQL_DUMP_FP  # save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c8cdb-6031-4ba6-a519-448f6b2c5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"without-sections\"\n",
    "run_experiment(\n",
    "    experiment_name=experiment_name, \n",
    "    data_path=DATA_PATH,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50, \n",
    "    num_chunks=5,\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    reference_loc=REFERENCE_LOC,\n",
    "    evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4867666-66ac-4c13-9bf3-72acc6d88236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without-sections\n",
      "  retrieval score: 0.0\n",
      "  quality score: 3.2849162011173183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6934303-7829-472c-aa14-495b8a2ee9ad",
   "metadata": {},
   "source": [
    "#### With sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "dd164683-7b3e-47f4-b19f-9c95e1765295",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "DROP TABLE\n",
      "CREATE TABLE\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Drop current vector DB (if any)\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"DROP TABLE document;\"\n",
    "sudo -u postgres psql -f ../migrations/vector-768.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f3c60-5caa-47c9-8364-bed3ecc431d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Switch VectorDB\n",
    "export SQL_DUMP_FP=\"/efs/shared_storage/goku/sql_dumps/with-sections.sql\"\n",
    "psql \"$DB_CONNECTION_STRING\" -f $SQL_DUMP_FP # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "723ea973-8050-4b67-8180-2d58c6dbbcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " count \n",
      "-------\n",
      " 57835\n",
      "(1 row)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check number of rows\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"SELECT count(*) FROM document;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecee626-612c-436f-8b89-a20cf6c73c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"with-sections\"\n",
    "run_experiment(\n",
    "    experiment_name=experiment_name, \n",
    "    data_path=DATA_PATH,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50, \n",
    "    num_chunks=5,\n",
    "    embedding_model_name=\"thenlper/gte-base\",\n",
    "    llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    reference_loc=REFERENCE_LOC,\n",
    "    evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d0170a9-6128-4481-aefb-63ad1999f1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with-sections\n",
      "  retrieval score: 0.4301675977653631\n",
      "  quality score: 3.2541899441340782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441b904-519c-4767-8925-5289e018359c",
   "metadata": {},
   "source": [
    "Looks like isolating sections wasn't really helpful but it doesn't hurt either. We could test with different models (ex. `gpt-3.5-turbo`) to further validate this claim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc3a24-007d-4add-b0dd-5832351c6d63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64b691b8-b2ea-4f77-857a-09e50053699c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_sizes = [100, 300, 600, 900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c054d1-9e4d-4688-b4b9-cf5a39b382c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for chunk_size in chunk_sizes:\n",
    "    experiment_name = f\"chunk-size-{chunk_size}\"\n",
    "    run_experiment(\n",
    "        experiment_name=experiment_name, \n",
    "        data_path=DATA_PATH,\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=50, \n",
    "        num_chunks=5,\n",
    "        embedding_model_name=\"thenlper/gte-base\",\n",
    "        llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        reference_loc=REFERENCE_LOC,\n",
    "        evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc0e486a-de51-4cf0-8852-00d6ac8b532c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk-size-100\n",
      "  retrieval score: 0.39106145251396646\n",
      "  quality score: 2.877094972067039\n",
      "\n",
      "chunk-size-300\n",
      "  retrieval score: 0.4301675977653631\n",
      "  quality score: 3.2653631284916202\n",
      "\n",
      "chunk-size-600\n",
      "  retrieval score: 0.547486033519553\n",
      "  quality score: 3.5335195530726256\n",
      "\n",
      "chunk-size-900\n",
      "  retrieval score: 0.547486033519553\n",
      "  quality score: 3.4860335195530725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk_size in chunk_sizes:\n",
    "    experiment_name = f\"chunk-size-{chunk_size}\"\n",
    "    print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680569cb-d1b3-464e-b771-1f5dd3d0cc66",
   "metadata": {},
   "source": [
    "Seem that a larger chunk size does help but it tapers off around the 600 characters mark (too much context might be too noisy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316e232-a250-4880-8518-a2ba7a4e5835",
   "metadata": {},
   "source": [
    "**Note**: If we were to use larger chunk sizes (ours is based on characters), keep in mind that [most](https://huggingface.co/spaces/mteb/leaderboard) open source embedding models have a maximum sequence length of 512 sub-word tokens. This means that if our chunk contains more than 512 sub-word tokens, the embedding wouldn't account for it anyway (unless we finetune our embedding model to have longer sequence lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04df6ec4-7edf-4a27-93ae-7ee2b3ff7241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80dd05-ced6-49b4-a193-c52fcebc118e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Number of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fbb854-b016-4c42-97bc-56c7eebfa3dd",
   "metadata": {},
   "source": [
    "**Note**: Keep in mind that the `chunk_size` you chose multiplied by the `num_chunks` below fits inside the LLM's context length. We're experimenting with the chunk size and number of chunks as if they were indepdent variables but they area heavily related. Especially since all of our LLMs have a finite maximum context length. So ideally, we would tune for a combination if `chunk_size` * `num_chunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03926725-4dac-43ad-880b-80825d3b958c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_chunks_list = [1, 3, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514d9c7-e16d-44c4-88b7-72a49b5c4197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for num_chunks in num_chunks_list:\n",
    "    experiment_name = f\"num-chunks-{num_chunks}\"\n",
    "    run_experiment(\n",
    "        experiment_name=experiment_name, \n",
    "        data_path=DATA_PATH,\n",
    "        chunk_size=CHUNK_SIZE, \n",
    "        chunk_overlap=CHUNK_OVERLAP, \n",
    "        num_chunks=num_chunks,\n",
    "        embedding_model_name=\"thenlper/gte-base\",\n",
    "        llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        reference_loc=REFERENCE_LOC,\n",
    "        evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "76f5a117-819e-4d06-b9e9-d196a592e123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num-chunks-1\n",
      "  retrieval score: 0.2737430167597765\n",
      "  quality score: 3.2458100558659218\n",
      "\n",
      "num-chunks-3\n",
      "  retrieval score: 0.48044692737430167\n",
      "  quality score: 3.363128491620112\n",
      "\n",
      "num-chunks-5\n",
      "  retrieval score: 0.547486033519553\n",
      "  quality score: 3.53072625698324\n",
      "\n",
      "num-chunks-6\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.5810055865921786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num_chunks in num_chunks_list:\n",
    "    experiment_name=f\"num-chunks-{num_chunks}\"\n",
    "    print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee986a80-8659-4344-bb22-71bb62b32946",
   "metadata": {},
   "source": [
    "Increasing our number of chunks improves our retrieval and quality scores. We had to stop testing at 6 chunks since our `chunk_size` is 600 tokens and `Llama-2-70b`'s maximum context length is 4096 tokens (we also have to account for the system, assistant and user content to our LLM). This is a major reason to invest in extending context size via RoPE scaling (rotary position embeddings), etc. But it also seems that the benefit of increasing the number of chunks is starting to taper off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbe4828f-9639-4957-898e-27dd0ce3ee32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CHUNKS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04dff3-5323-419f-a290-849c96899292",
   "metadata": {},
   "source": [
    "### Embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10c471-22b5-479c-bbbd-59ff3835d7b9",
   "metadata": {},
   "source": [
    "So far, we've used [`thenlper/gte-base`](https://huggingface.co/thenlper/gte-base) as our embedding model because it's a relatively small (0.22 GB) and performant option. But now, let's explore other popular options such the current leader on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard), [`BAAI/bge-large-en`](https://huggingface.co/BAAI/bge-large-en) (1.34 GB), and OpenAI's [`text-embedding-ada-002`](https://openai.com/blog/new-and-improved-embedding-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "198ec597-8aaf-4c45-a275-2094211eebb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model_names = [\"thenlper/gte-base\", \"BAAI/bge-large-en\", \"text-embedding-ada-002\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913f50c-ef13-487d-beeb-77ee38f91067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for embedding_model_name in embedding_model_names:\n",
    "    experiment_name = f\"{embedding_model_name.split('/')[-1]}\"\n",
    "    run_experiment(\n",
    "        experiment_name=experiment_name, \n",
    "        data_path=DATA_PATH,\n",
    "        chunk_size=CHUNK_SIZE, \n",
    "        chunk_overlap=CHUNK_OVERLAP, \n",
    "        num_chunks=NUM_CHUNKS,\n",
    "        embedding_model_name=embedding_model_name,\n",
    "        llm=\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "        reference_loc=REFERENCE_LOC,\n",
    "        evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c429ae5-f5a5-4a2e-893a-7add9d11337f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gte-base\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.589385474860335\n",
      "\n",
      "bge-large-en\n",
      "  retrieval score: 0.35195530726256985\n",
      "  quality score: 3.1145251396648046\n",
      "\n",
      "text-embedding-ada-002\n",
      "  retrieval score: 0.5586592178770949\n",
      "  quality score: 3.4329608938547485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for embedding_model_name in embedding_model_names:\n",
    "    experiment_name = f\"{embedding_model_name.split('/')[-1]}\"\n",
    "    print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd24b35-1db3-4326-ab0c-c4b484fb5aea",
   "metadata": {},
   "source": [
    "This is an interesting outcome because the #1 (`BAAI/bge-large-en`) on the current leaderboard isn't necessarily the best for our specific task. Using the smaller `thenlper/gte-base` produced the best retrieval and quality scores in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "845ad771-65e1-44cf-813f-3aa167c07e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21b32f-bacb-4703-b16c-d4a7014779dc",
   "metadata": {},
   "source": [
    "### OSS vs. closed LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a91c87cb-ba0d-4044-9616-b2cbad239587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llms = [\"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo-16k\",\n",
    "        \"gpt-4\",\n",
    "        \"meta-llama/Llama-2-7b-chat-hf\", \n",
    "        \"meta-llama/Llama-2-13b-chat-hf\", \n",
    "        \"meta-llama/Llama-2-70b-chat-hf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e8114-23ba-402e-a03d-594089e9b4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for llm in llms:\n",
    "    experiment_name = f\"{llm.split('/')[-1].lower()}\"\n",
    "    run_experiment(\n",
    "        experiment_name=experiment_name, \n",
    "        data_path=DATA_PATH,\n",
    "        chunk_size=CHUNK_SIZE, \n",
    "        chunk_overlap=CHUNK_OVERLAP, \n",
    "        num_chunks=NUM_CHUNKS,\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        llm=llm,\n",
    "        reference_loc=REFERENCE_LOC,\n",
    "        evaluator=EVALUATOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c7e6276e-3b92-4d2e-8438-050386c6c83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.664804469273743\n",
      "\n",
      "gpt-3.5-turbo-16k\n",
      "  retrieval score: 0.664804469273743\n",
      "  quality score: 3.7960893854748603\n",
      "\n",
      "gpt-4\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.9189944134078214\n",
      "\n",
      "llama-2-7b-chat-hf\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 2.810055865921788\n",
      "\n",
      "llama-2-13b-chat-hf\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.3715083798882683\n",
      "\n",
      "llama-2-70b-chat-hf\n",
      "  retrieval score: 0.5977653631284916\n",
      "  quality score: 3.5139664804469275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for llm in llms:\n",
    "    experiment_name = f\"{llm.split('/')[-1].lower()}\"\n",
    "    print_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1cd68-f77a-4f13-9454-3add1fc65158",
   "metadata": {},
   "source": [
    "**Note**: Some of our LLMs have much larger context lengths, ex. `gpt-4` is 8192 and `gpt-3.5-turbo-16k` is 16384. We could increase the number of chunks that we use for these since we saw that increasing `num_chunks` continued to improve the retrieval and quality scores. However, we will keep this value fixed for now since the performance started to taper off anyway and so we can compare these performances under the exact same configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275cb71-6876-404a-bdbe-f79347162696",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"llama-2-70b-chat-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916ea05-4b99-4024-9211-ed35bb8ac9dc",
   "metadata": {},
   "source": [
    "## Cost comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c145a5-e97e-4811-ba12-c1a37d71c171",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note**: Our `Llama-2` models are priced at $1/M tokens with [Anyscale Endpoints](https://endpoints.anyscale.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca18129b-62db-49af-9be3-2444d0af5751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pricing details\n",
    "pricing = {\n",
    "    \"gpt-3.5-turbo\": {\n",
    "        \"prompt\": 2e-6,\n",
    "        \"sampled\": 2e-6\n",
    "    },\n",
    "    \"gpt-4\": {\n",
    "        \"prompt\": 3e-5,\n",
    "        \"sampled\": 6e-5\n",
    "    },\n",
    "    \"llama-2-7b-chat-hf\": {\n",
    "        \"prompt\": 1e-6,\n",
    "        \"sampled\": 1e-6\n",
    "    },\n",
    "    \"llama-2-13b-chat-hf\": {\n",
    "        \"prompt\": 1e-6,\n",
    "        \"sampled\": 1e-6\n",
    "    },\n",
    "    \"llama-2-70b-chat-hf\": {\n",
    "        \"prompt\": 1e-6,\n",
    "        \"sampled\": 1e-6\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7f020340-9357-4a48-976b-cdac2e467351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cost_analysis(llm):\n",
    "    experiment_name = f\"{llm.split('/')[-1].lower()}\"\n",
    "    eval_fp = Path(ROOT_DIR, \"experiments\", \"evaluations\", f\"{experiment_name}_{EVALUATOR}.json\")\n",
    "    with open(eval_fp, \"r\") as fp:\n",
    "        d = json.load(fp)\n",
    "    num_samples = len(d[\"results\"])\n",
    "    prompt_size, sampled_size = 0, 0\n",
    "    for result in d[\"results\"]:\n",
    "        prompt_size += len(result[\"question\"]) + (CHUNK_SIZE * NUM_CHUNKS)\n",
    "        sampled_size += len(result[\"generated_answer\"])\n",
    "    total_cost = pricing[experiment_name][\"prompt\"] * prompt_size + pricing[experiment_name][\"sampled\"] * sampled_size\n",
    "    avg_cost = total_cost / num_samples\n",
    "    \n",
    "    print (llm)\n",
    "    print (f\"  avg prompt size: {int(prompt_size/num_samples)}\")\n",
    "    print (f\"  avg sampled size: {int(sampled_size/num_samples)}\")\n",
    "    print (f\"  total cost: ${total_cost:.2f}\")\n",
    "    print (f\"  avg cost: ${avg_cost:.2f}\")\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee95574b-b355-4d5a-979b-467657bbd959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n",
      "  avg prompt size: 3667\n",
      "  avg sampled size: 1117\n",
      "  total cost: $1.71\n",
      "  avg cost: $0.01\n",
      "\n",
      "gpt-4\n",
      "  avg prompt size: 3667\n",
      "  avg sampled size: 828\n",
      "  total cost: $28.59\n",
      "  avg cost: $0.16\n",
      "\n",
      "meta-llama/Llama-2-7b-chat-hf\n",
      "  avg prompt size: 3667\n",
      "  avg sampled size: 2509\n",
      "  total cost: $1.11\n",
      "  avg cost: $0.01\n",
      "\n",
      "meta-llama/Llama-2-13b-chat-hf\n",
      "  avg prompt size: 3667\n",
      "  avg sampled size: 1663\n",
      "  total cost: $0.95\n",
      "  avg cost: $0.01\n",
      "\n",
      "meta-llama/Llama-2-70b-chat-hf\n",
      "  avg prompt size: 3667\n",
      "  avg sampled size: 1573\n",
      "  total cost: $0.94\n",
      "  avg cost: $0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for llm in llms:\n",
    "    cost_analysis(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b599c-5d7e-4b5c-b05c-700830b8861d",
   "metadata": {},
   "source": [
    "**Note**: OpenAI's GPT models have [rate limits](https://platform.openai.com/docs/guides/rate-limits) to be aware of when we want to use the LLMs at scale for an application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2b5b9-8fe0-44a3-8910-8ce21056dc57",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150189f9-e9c0-4af5-b426-74c847ab22b2",
   "metadata": {},
   "source": [
    "- routing\n",
    "- additional data sources\n",
    "- longer context lengths\n",
    "- fine-tune embedding model\n",
    "- fine-tune base LLM (gpt-3.5 and OSS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
